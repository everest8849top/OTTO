{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Generates candidates.\n",
    "\n",
    "**TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=False, use_memory_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.covisitation import compute_covisitation_matrix\n",
    "from data.candidates import (\n",
    "    load_parquets,\n",
    "    create_candidates,\n",
    "    explode,\n",
    "    matrix_to_candids_dict,\n",
    ")\n",
    "\n",
    "from utils.metrics import get_coverage\n",
    "from utils.chris import suggest_clicks, suggest_buys, read_file_to_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    REGEX = \"../output/val_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    REGEX = \"../output/test_parquet/*\"\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cache = {}\n",
    "# for f in tqdm(files):\n",
    "#     data_cache[f] = read_file_to_cache(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_CT = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FOLDER = \"../output/matrices/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUFFIX = \"c-orders-v3\"  # 50\n",
    "SUFFIX = \"c-orders-v4\"  # 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_candids_dict(matrix):\n",
    "    matrix = matrix.sort_values([\"aid_x\", \"wgt\"], ascending=[True, False])\n",
    "\n",
    "    candids = matrix[[\"aid_x\", \"aid_y\"]].groupby(\"aid_x\").agg(list)\n",
    "\n",
    "    try:\n",
    "        candids = candids.to_pandas()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    candids[\"aid_y\"] = candids[\"aid_y\"].apply(lambda x: x.tolist())\n",
    "    candids_dict = candids.to_dict()[\"aid_y\"]\n",
    "\n",
    "    return candids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquets(regex):\n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob(regex)):\n",
    "        chunk = cudf.read_parquet(chunk_file)\n",
    "        chunk[\"d\"] = cudf.to_datetime(chunk.ts * 1e6).dt.day.astype(\"int8\")\n",
    "        chunk.ts = (chunk.ts / 1000).astype(\"int32\")\n",
    "        chunk[\"type\"] = chunk[\"type\"].map(TYPE_LABELS).astype(\"int8\")\n",
    "\n",
    "        dfs.append(chunk)\n",
    "\n",
    "    return (\n",
    "        cudf.concat(dfs).sort_values([\"session\", \"ts\"], ignore_index=True).to_pandas()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt = cudf.read_parquet(\"../output/val_labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_clicks = gt[gt[\"type\"] == \"clicks\"].explode(\"ground_truth\")\n",
    "# gt_clicks = gt_clicks.drop(\"type\", axis=1).rename(\n",
    "#     columns={\"ground_truth\": \"candidates\"}\n",
    "# )\n",
    "# gt_clicks[\"gt_clicks\"] = 1\n",
    "# gt_clicks[\"gt_carts\"] = 0\n",
    "# gt_clicks[\"gt_orders\"] = 0\n",
    "\n",
    "# gt_carts = gt[gt[\"type\"] == \"carts\"].explode(\"ground_truth\")\n",
    "# gt_carts = gt_carts.drop(\"type\", axis=1).rename(columns={\"ground_truth\": \"candidates\"})\n",
    "# gt_carts[\"gt_clicks\"] = 0\n",
    "# gt_carts[\"gt_carts\"] = 1\n",
    "# gt_carts[\"gt_orders\"] = 0\n",
    "\n",
    "# gt_orders = gt[gt[\"type\"] == \"orders\"].explode(\"ground_truth\")\n",
    "# gt_orders = gt_orders.drop(\"type\", axis=1).rename(\n",
    "#     columns={\"ground_truth\": \"candidates\"}\n",
    "# )\n",
    "# gt_orders[\"gt_clicks\"] = 0\n",
    "# gt_orders[\"gt_carts\"] = 0\n",
    "# gt_orders[\"gt_orders\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates_gt = cudf.concat([gt_clicks, gt_carts, gt_orders], ignore_index=True)\n",
    "\n",
    "# candidates_gt = (\n",
    "#     candidates_gt.groupby([\"session\", \"candidates\"])\n",
    "#     .max()\n",
    "#     .reset_index()\n",
    "#     .sort_values([\"session\", \"candidates\"])\n",
    "# )\n",
    "\n",
    "# candidates_gt.to_parquet(f\"../output/candidates/candidates_gt.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = load_parquets(REGEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popular Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clicks = (\n",
    "    df_val.loc[df_val[\"type\"] == 0, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")\n",
    "top_carts = (\n",
    "    df_val.loc[df_val[\"type\"] == 1, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")\n",
    "top_orders = (\n",
    "    df_val.loc[df_val[\"type\"] == 2, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_buy2buy = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-90_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_buy2buy2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-99_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_orders = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-95_{MODE}.pqt\")\n",
    ")\n",
    "top_20_carts = top_20_orders\n",
    "\n",
    "top_20_test = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-116_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_test2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-115_{MODE}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-93_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20b = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-217_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20c = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-220_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20d = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-226_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20e = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-232_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20f = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-235_{MODE}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_buy = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-239_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_new = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-700_{MODE}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_new2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-701_{MODE}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_orders(df):\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    mx = df.d.max()\n",
    "    aids2 = df.loc[df.d == mx].aid.tolist()\n",
    "    unique_aids4 = list(dict.fromkeys(aids2[::-1]))\n",
    "\n",
    "    mx = df.ts.max()\n",
    "    aids2 = df.loc[df.ts >= mx - 60 * 60 / 2].aid.tolist()\n",
    "    unique_aids5 = list(dict.fromkeys(aids2[::-1]))  # recent 1 hour\n",
    "\n",
    "    df2 = df.drop_duplicates(\"d\")\n",
    "    aids2 = df2.aid.tolist()\n",
    "    unique_aids2 = list(dict.fromkeys(aids2[::-1]))  # first of each session\n",
    "\n",
    "    df2 = df.sort_values(\"ts\", ascending=False).drop_duplicates(\"d\")\n",
    "    aids2 = df2.aid.tolist()\n",
    "    unique_aids3 = list(dict.fromkeys(aids2))  # last of each session\n",
    "\n",
    "    df = df.loc[df[\"type\"].isin([1, 2])]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        for aid in unique_aids2:\n",
    "            aids_temp[aid] += 0.5\n",
    "        for aid in unique_aids3:\n",
    "            aids_temp[aid] += 0.5\n",
    "\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_buy2buy[aid][:40]\n",
    "                    for aid in unique_buys\n",
    "                    if aid in top_20_buy2buy\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_buy2buy2[aid][:40]\n",
    "                    for aid in unique_buys\n",
    "                    if aid in top_20_buy2buy2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.1\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.1\n",
    "\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_test[aid][:40] for aid in unique_aids if aid in top_20_test]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][:20] for aid in unique_aids[:1] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids5):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids6 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20d[aid][:20] for aid in unique_buys[:1] if aid in top_20d]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids6):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "\n",
    "        aids7 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:5] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids7):\n",
    "            aids_temp[aid] += 0.25\n",
    "            if i % 5 == 0:\n",
    "                aids_temp[aid] += 0.25\n",
    "        aids7 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:5] for aid in unique_aids2 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids7):\n",
    "            aids_temp[aid] += 0.125\n",
    "            if i % 5 == 0:\n",
    "                aids_temp[aid] += 0.125\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT)]\n",
    "\n",
    "        if len(result) < 1:\n",
    "            result += top_orders[: 20 - len(result)]\n",
    "\n",
    "        return result[:ITEM_CT]\n",
    "\n",
    "    weights = [2, 2] + [1] * 8  # + [0]*30\n",
    "    weights2 = [2, 2] + [1] * 53  # + [0]*25\n",
    "    weights3 = [2, 2] + [1] * 18  # + [0]*70\n",
    "    weights4 = [2, 2] + [1] * 38  # + [0]*70\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    aids_temp = Counter()\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_orders[aid][:10] for aid in unique_aids if aid in top_20_orders]\n",
    "        )\n",
    "    )\n",
    "    w2 = weights * int(len(aids2) // 10)\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_buy2buy[aid][:10] for aid in unique_buys if aid in top_20_buy2buy]\n",
    "        )\n",
    "    )\n",
    "    w3 = weights * int(len(aids3) // 10)\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test[aid][:10] for aid in unique_aids if aid in top_20_test]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights * int(len(aids4) // 10)\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_buy2buy2[aid][:10]\n",
    "                for aid in unique_buys\n",
    "                if aid in top_20_buy2buy2\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights * int(len(aids5) // 10)\n",
    "    for i, (aid, w) in enumerate(zip(aids2, w2)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // 10)) / ln\n",
    "        aids_temp[aid] += w * m\n",
    "    for i, (aid, w) in enumerate(zip(aids3, w3)):\n",
    "        aids_temp[aid] += w / 2\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // 10)) / ln\n",
    "        aids_temp[aid] += w * m\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        aids_temp[aid] += w / 2\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20c[aid][:55] for aid in unique_aids[:1] if aid in top_20c]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights2 * int(len(aids5) // 55)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += w\n",
    "\n",
    "    # NEW\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:20]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 20)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_new[aid][:20] for aid in unique_aids[-1:] if aid in top_20_new]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 20)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20d[aid][:20] for aid in unique_buys[:1] if aid in top_20d]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights3 * int(len(aids5) // 20)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += w\n",
    "\n",
    "    ln2 = len(unique_aids5)\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_buy[aid][:20] for aid in unique_aids5 if aid in top_20_buy]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights3 * int(len(aids5) // 20)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += 2 * w / ln2\n",
    "\n",
    "    aids4 = list(\n",
    "        itertools.chain(*[top_20f[aid][:5] for aid in unique_aids4 if aid in top_20f])\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // 5\n",
    "        aids_temp[aid] += 1 / 2 - w * 0.05\n",
    "        if i % 5 == 0:\n",
    "            aids_temp[aid] += 1 / 2 - w * 0.05\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:55] for aid in unique_aids3 if aid in top_20e])\n",
    "    )\n",
    "    w5 = weights2 * int(len(aids5) // 55)\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        w2 = i // 55\n",
    "        aids_temp[aid] += w - w2 * 0.1\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:10] for aid in unique_aids2 if aid in top_20e])\n",
    "    )\n",
    "    w5 = weights * int(len(aids5) // 10)\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        w2 = i // 10\n",
    "        aids_temp[aid] += w / 2.0 - w2 * 0.05\n",
    "\n",
    "    sorted_aids = [k for k, v in aids_temp.most_common(ITEM_CT) if k not in unique_aids]\n",
    "\n",
    "    result = unique_aids + sorted_aids[: ITEM_CT - len(unique_aids)]\n",
    "\n",
    "    if len(result) < 1:\n",
    "        result += top_orders[: 20 - len(result)]\n",
    "\n",
    "    return result[:ITEM_CT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[\"chunk\"] = df_val[\"session\"] // 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk size 353331: 100%|██████████| 19/19 [26:21<00:00, 83.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 59s, sys: 6min 3s, total: 12min 2s\n",
      "Wall time: 26min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preds = []\n",
    "pbar = tqdm(df_val.groupby(\"chunk\"), total=len(df_val[\"chunk\"].unique()))\n",
    "\n",
    "for _, dfg in pbar:\n",
    "    pbar.set_description(f\"Chunk size {len(dfg)}\")\n",
    "    pred_df_orders = dfg.groupby([\"session\"]).parallel_apply(\n",
    "        lambda x: suggest_orders(x)\n",
    "    )\n",
    "    preds.append(pred_df_orders)\n",
    "    \n",
    "pred_df_orders = pd.concat(preds)\n",
    "\n",
    "del preds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 24s, sys: 7.81 s, total: 2min 32s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if MODE != \"test\":\n",
    "    try:\n",
    "        clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "    except:\n",
    "        clicks_pred_df = pd.DataFrame(pred_df_orders.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "\n",
    "    orders_pred_df = pd.DataFrame(pred_df_orders.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "    carts_pred_df = pd.DataFrame(pred_df_orders.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()\n",
    "\n",
    "    pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "    pred_df.columns = [\"session_type\", \"labels_l\"]\n",
    "    pred_df[\"labels\"] = pred_df[\"labels_l\"].apply(lambda x: \" \".join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- clicks \t- Found 1.11M GTs with 127.05M candidates (pos_prop=0.87%)\t-  Highest reachable Recall : 0.6317\n",
      "- carts \t- Found 287.71K GTs with 127.05M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.4995\n",
      "- orders \t- Found 220.07K GTs with 127.05M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : 0.7026\n",
      "\n",
      "-> CV : 0.6346\n"
     ]
    }
   ],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(\"../output/val_labels.parquet\")\n",
    "\n",
    "    recs = []\n",
    "    df_pred = pred_df[[\"session_type\", \"labels_l\"]].copy()\n",
    "    df_pred.columns = [\"session_type\", \"candidates\"]\n",
    "    df_pred[\"session\"] = (\n",
    "        df_pred[\"session_type\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "    )\n",
    "    df_pred[\"type\"] = df_pred[\"session_type\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    df_pred = df_pred.merge(gt, on=[\"session\", \"type\"], how=\"left\")\n",
    "\n",
    "    for col in CLASSES:\n",
    "        df_pred_c = df_pred[df_pred[\"type\"] == col]\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df_pred_c[\"candidates\"].values, df_pred_c[\"ground_truth\"].values\n",
    "        )\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "\n",
    "        recs.append(n_found / n_gts)\n",
    "\n",
    "    cv = np.average(recs, weights=WEIGHTS)\n",
    "    print(f\"\\n-> CV : {cv:.4f}\")\n",
    "\n",
    "    del clicks_pred_df, orders_pred_df, carts_pred_df, pred_df, df_pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 20\n",
    " - clicks \t- Found 908.67K GTs with 36.03M candidates (pos_prop=2.52%)\t-  Highest reachable Recall : 0.5177\n",
    " - carts \t- Found 245.23K GTs with 36.03M candidates (pos_prop=0.68%)\t-  Highest reachable Recall : 0.4257\n",
    " - orders \t- Found 206.18K GTs with 36.03M candidates (pos_prop=0.57%)\t-  Highest reachable Recall : 0.6582\n",
    "- 50\n",
    " - clicks \t- Found 1.07M GTs with 87.44M candidates (pos_prop=1.22%)\t-  Highest reachable Recall : 0.6076\n",
    " - carts \t- Found 275.95K GTs with 87.44M candidates (pos_prop=0.32%)\t-  Highest reachable Recall : 0.4790\n",
    " - orders \t- Found 216.38K GTs with 87.44M candidates (pos_prop=0.25%)\t-  Highest reachable Recall : 0.6908\n",
    "- 75\n",
    " - clicks \t- Found 1.11M GTs with 127.24M candidates (pos_prop=0.87%)\t-  Highest reachable Recall : 0.6317\n",
    " - carts \t- Found 287.71K GTs with 127.24M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.4995\n",
    " - orders \t- Found 220.08K GTs with 127.24M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : 0.7026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candids = pd.DataFrame(pred_df_orders).reset_index()\n",
    "df_candids.columns = [\"session\", \"candidates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(f\"../output/{MODE}_labels.parquet\")\n",
    "    gt[\"ground_truth\"] = gt[\"ground_truth\"].apply(lambda x: x.tolist())\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df_candids.columns:\n",
    "            df_candids = df_candids.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, test=False):\n",
    "    if \"aid\" in df.columns:\n",
    "        df.drop([\"aid\", \"type\"], axis=1, inplace=True)\n",
    "\n",
    "    df = cudf.from_pandas(df)\n",
    "    df = df.explode(\"candidates\")\n",
    "    df = df.drop_duplicates(keep=\"first\", subset=[\"session\", \"candidates\"])\n",
    "\n",
    "    df[\"candidates\"] = df[\"candidates\"].astype(\"uint32\")\n",
    "    df[\"session\"] = df[\"session\"].astype(\"uint32\")\n",
    "\n",
    "    df = df.sort_values([\"session\", \"candidates\"]).reset_index(drop=True)\n",
    "\n",
    "    if not test:\n",
    "        for col in [\"gt_clicks\", \"gt_carts\", \"gt_orders\"]:\n",
    "            df_tgt = (\n",
    "                df[[\"session\", \"candidates\", col]].explode(col).reset_index(drop=True)\n",
    "            ).fillna(-1)\n",
    "            df_tgt[col] = df_tgt[col].astype(\"int64\") == df_tgt[\"candidates\"].astype(\n",
    "                \"int64\"\n",
    "            )\n",
    "\n",
    "            assert not df_tgt.isna().any().max()\n",
    "\n",
    "            df_tgt = df_tgt.groupby([\"session\", \"candidates\"]).max().reset_index()\n",
    "            df_tgt = df_tgt.sort_values([\"session\", \"candidates\"]).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            assert not df_tgt.isna().any().max()\n",
    "\n",
    "            df[col] = df_tgt[col].astype(\"uint8\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../output/candidates/candidates_c-orders-v4_val.parquet\n"
     ]
    }
   ],
   "source": [
    "df_candids = explode(df_candids, test=(MODE == \"test\"))\n",
    "\n",
    "df_candids.to_parquet(\n",
    "    f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False\n",
    ")\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theo's version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"test\"\n",
    "SUFFIX = \"v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MATRIX = 20\n",
    "MAX_COOC = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    PARQUET_FILES = \"../output/val_parquet/*\"\n",
    "elif MODE == \"val_c\":  # needs to be recomputed, matrices as well\n",
    "    PARQUET_FILES = \"../output/val_c_parquet/*\"\n",
    "elif MODE == \"train\":\n",
    "    PARQUET_FILES = \"../output/train_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    PARQUET_FILES = \"../output/test_parquet/*\"\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_parquets(PARQUET_FILES)\n",
    "df = df.sort_values([\"session\", \"ts\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time  # TODO\n",
    "# df = create_candidates(df, clicks_candids, type_weighted_candids, max_cooc=MAX_COOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clicks_candids, type_weighted_candids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_candid = df[\"candidates\"].apply(len)\n",
    "sns.histplot(np.clip(n_candid, 0, 150))\n",
    "\n",
    "plt.title(f\"Proportion of sessions with <20 candidates : {(n_candid < 20).mean() :.3f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "\n",
    "    recalls = []\n",
    "    gt = pd.read_parquet(f\"../output/{MODE}_labels.parquet\")\n",
    "    # gt = pd.read_parquet(\"../input/chris/test_labels.parquet\")\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df.columns:\n",
    "            df = df.merge(gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\").rename(\n",
    "                columns={\"ground_truth\": f\"gt_{col}\"}\n",
    "            )\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df[\"candidates\"].values, df[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"{col}\\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.3f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    cv = np.average(recalls, weights=WEIGHTS)\n",
    "    print(f\"-> Highest reachable CV : {cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode & saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = explode(df, test=(MODE==\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\n",
    "    f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False\n",
    ")\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
