{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Trains XGBoost models.\n",
    "\n",
    "**TODO**:\n",
    "- better neg sampling technique ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import numba\n",
    "import xgboost\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numerize.numerize import numerize\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.metrics import get_coverage, evaluate\n",
    "from utils.plot import plot_importances\n",
    "from utils.load import *\n",
    "from utils.logger import *\n",
    "\n",
    "from inference.xgb import xgb_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2023-01-24/27/\"  #  \"../logs/2023-01-26/3/\"  # \"../logs/2023-01-26/2/\"\n",
    "VERSION = \"cv7-tv5.11\"  # 0.6692 /  0.6716 /  0.6684 / 0.6693 -> 0.6696\n",
    "# VERSION = \"cv8-tv5.11\"  # 0.6695 /  0.6710 /  0.6686 / 0.6694 -> 0.6696\n",
    "# VERSION = \"cv9-tv5.11\"  # 0.6690 /  0.6717 /  0.6683 / 0.6691 -> 0.6695\n",
    "\n",
    "REGEX = f\"../output/features/fts_val_{VERSION}/*\"\n",
    "TEST_REGEX = None  # f\"../output/features/fts_test_{VERSION}/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_inference(REGEX, TEST_REGEX, EXP_FOLDER, debug=False, save=True)  # CV 0.6696  //  0.6692 - 0.6716 - 0.6684 - 0.6693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_imp = pd.read_csv(\"../logs/2023-01-24/22/ft_imp.csv\")\n",
    "# list(ft_imp.sort_values('importance')[\"index\"])[:200]\n",
    "# plot_importances(ft_imp.set_index(\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX = \"../output/val_parquet/*\"\n",
    "\n",
    "sessions = load_sessions(REGEX)\n",
    "sessions_g = sessions[[\"session\", \"aid\"]].groupby('session').agg(\"count\").reset_index()\n",
    "sessions_len_1 = sessions_g[sessions_g['aid'] == 1]\n",
    "\n",
    "sessions_len_more = sessions_g[sessions_g['aid'] > 1]\n",
    "# sessions_g = sessions_g.merge(sessions_g2, on=\"session\", suffixes=('', '_'), how=\"left\")\n",
    "# sessions_len_1 = sessions_g.dropna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "- neg sampling could use candidates from lower versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # ORDERS - 0.6680\n",
    "    [\"../logs/2023-01-20/1/\", \"../logs/2023-01-20/9/\", \"../logs/2023-01-20/13/\", \"../logs/2023-01-20/12/\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # Carts - 0.4404\n",
    "    [\"../logs/2023-01-20/17/\", \"../logs/2023-01-20/29/\", \"../logs/2023-01-20/28/\", \"../logs/2023-01-20/27/\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # Clicks - 0.5593\n",
    "    [\"../logs/2023-01-20/26/\", \"../logs/2023-01-20/23/\", \"../logs/2023-01-20/30/\", \"../logs/2023-01-20/32/\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "    \"../logs/2023-01-24/29/\",  # CARTS - Pos ratio 0.2 - Extra_prop 0                CV 0.4420\n",
    "    [\"../logs/2023-01-25/0/\", \"../logs/2023-01-25/4/\",  \"../logs/2023-01-25/6/\", \"../logs/2023-01-25/7/\"],  # CARTS Pos ratio 0.5 - Extra_prop 1.0    CV 0.4424\n",
    "    [\"../logs/2023-01-26/3/\", \"../logs/2023-01-26/2/\"],                                                     # CARTS Pos ratio 0.5 - Extra_prop 2.0    CV 0.4426\n",
    "]\n",
    "\n",
    "WEIGHTS = [1, 1, 2]  # Carts 0.4428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [ # Orders 0.6698\n",
    "    \"../logs/2023-01-24/28/\",  # ORDER - Pos ratio 0.2 - Extra_prop 0.5              CV 0.6694\n",
    "    \"../logs/2023-01-24/27/\",  # ORDER - Pos ratio 0.5 - Extra_prop 1                CV 0.6696\n",
    "    \"../logs/2023-01-24/17/\",  # ORDER - Pos ratio 0.2 - Extra_prop 0.5  rm low imp  CV 0.6694\n",
    "    \"../logs/2023-01-25/11/\",  # Pos ratio 0.5 - Extra_prop 2.0                       CV 0.6697\n",
    "]\n",
    "\n",
    "WEIGHTS = [0.25, 2, 0.25, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "#     \"../logs/2023-01-24/29/\",  # CARTS - Pos ratio 0.2 - Extra_prop 0                CV 0.4420\n",
    "    \"../logs/2023-01-24/28/\",  # ORDER - Pos ratio 0.2 - Extra_prop 0.5              CV 0.6694\n",
    "    \"../logs/2023-01-24/27/\",  # ORDER - Pos ratio 0.5 - Extra_prop 1                CV 0.6696\n",
    "#     \"../logs/2023-01-24/22/\",  # ORDER - Pos ratio 0.2 - Extra_prop 0                CV 0.6691\n",
    "    \"../logs/2023-01-24/17/\",  # ORDER - Pos ratio 0.2 - Extra_prop 0.5  rm low imp  CV 0.6694\n",
    "#     [\"../logs/2023-01-25/0/\", \"../logs/2023-01-25/4/\",  \"../logs/2023-01-25/6/\", \"../logs/2023-01-25/7/\"],  # CARTS Pos ratio 0.5 - Extra_prop 1.0    CV 0.4424\n",
    "#     [\"../logs/2023-01-26/3/\", \"../logs/2023-01-26/2/\"],                                                     # CARTS Pos ratio 0.5 - Extra_prop 2.0    CV 0.4426\n",
    "    \n",
    "    \"../logs/2023-01-25/11/\",  # Pos ratio 0.5 - Extra_prop 2.0                       CV 0.6697\n",
    "]\n",
    "\n",
    "WEIGHTS = [1] * len(EXP_FOLDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t === Exp ../logs/2023-01-24/28/\t Target gt_orders ===\n",
      "\n",
      "cv7-tv5.11 - Pos ratio 0.2 - Extra_prop 0.5\n",
      "\n",
      "\t === Exp ../logs/2023-01-24/27/\t Target gt_orders ===\n",
      "\n",
      "cv7-tv5.11 - Pos ratio 0.5 - Extra_prop 2\n",
      "\n",
      "\t === Exp ../logs/2023-01-24/17/\t Target gt_orders ===\n",
      "\n",
      "cv7-tv5.11 - Pos ratio 0.2 - Extra_prop 0.5\n",
      "\n",
      "\t === Exp ../logs/2023-01-25/11/\t Target gt_orders ===\n",
      "\n",
      "cv7-tv5.11 - Pos ratio 0.5 - Extra_prop 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val_ = None\n",
    "# dfs_val_list = []\n",
    "for exp_folders, w in zip(EXP_FOLDERS, WEIGHTS):\n",
    "    \n",
    "    if not isinstance(exp_folders, list):\n",
    "        exp_folders = [exp_folders]\n",
    "\n",
    "    dfs_val = []\n",
    "    for exp_folder in exp_folders:\n",
    "        config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "        print(f'\\t === Exp {exp_folder}\\t Target {config.target} ===\\n')\n",
    "        TARGET = config.target\n",
    "        try:\n",
    "            print(f\"{config.version} - Pos ratio {config.pos_ratio} - Extra_prop {config.extra_prop if config.use_extra else 0}\\n\")\n",
    "        except:\n",
    "            print(f\"{config.version} - Pos ratio {config.pos_ratio} - Extra_prop 0\\n\")\n",
    "\n",
    "        for fold in range(config.k):\n",
    "#             if fold == 3:\n",
    "#                 continue\n",
    "            try:\n",
    "                df_val = cudf.read_parquet(exp_folder + f\"df_val_{fold}.parquet\")\n",
    "#                 df_val = df_val.merge(sessions_len_1, on=\"session\", how=\"left\").dropna(0)\n",
    "#                 df_val = df_val.merge(sessions_len_more, on=\"session\", how=\"left\").dropna(0)\n",
    "\n",
    "#                 df_val['pred'] = df_val.groupby('session')['pred'].rank()\n",
    "#                 df_val['pred'] = (df_val['pred'] - df_val['pred'].min()) / (df_val['pred'].max() - df_val['pred'].min())\n",
    "                df_val['pred'] *= w\n",
    "\n",
    "                dfs_val.append(df_val)\n",
    "#                 print(f'-> Retrieved fold {fold}', end=\"\")\n",
    "#                 evaluate(df_val, TARGET)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            \n",
    "#             break\n",
    "\n",
    "    dfs_val = cudf.concat(dfs_val, ignore_index=True)\n",
    "#     dfs_val_list.append(dfs_val.sort_values(['session', 'candidates'], ignore_index=True))\n",
    "\n",
    "#     print('\\n ===> CV :')\n",
    "#     evaluate(dfs_val, TARGET)    \n",
    "\n",
    "    if df_val_ is None: \n",
    "        df_val_ = dfs_val.copy()\n",
    "    else:\n",
    "        df_val_ = df_val_.set_index(['session', 'candidates']).add(\n",
    "            dfs_val.set_index(['session', 'candidates']), fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "    del dfs_val\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> gt_orders  -  Recall : 0.6698\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6698090920699783"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(df_val_, TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import LogisticRegression\n",
    "\n",
    "def kfold(df, config):\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    folds = cudf.read_csv(config.folds_file)\n",
    "    if \"fold\" not in df.columns:\n",
    "        df = df.merge(folds, how=\"left\", on=\"session\")\n",
    "\n",
    "    preds = []\n",
    "    for fold in range(config.k):\n",
    "        df_train = df[df['fold'] != fold].reset_index(drop=True)\n",
    "        df_val = df[df['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "        model = LogisticRegression(C=0.1)\n",
    "        cols = ['pred_0', 'rank_0', 'pred_1', 'rank_1']\n",
    "\n",
    "        model.fit(df_train[cols], df_train[TARGET])\n",
    "        df_val['pred'] = model.predict_proba(df_val[cols])[1]\n",
    "\n",
    "#         evaluate(df_val, TARGET)\n",
    "        preds.append(df_val)\n",
    "#         return\n",
    "        \n",
    "    preds = cudf.concat(preds, ignore_index=True)\n",
    "    evaluate(preds, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_vals = dfs_val_list[0].merge(dfs_val_list[1], on=['session', 'candidates'], how=\"left\", suffixes=(\"_0\", \"_1\"))\n",
    "# dfs_vals = dfs_vals.dropna(0)\n",
    "\n",
    "# gt = cudf.read_parquet(\"../output/candidates/candidates_cv3-tv5_val.parquet\")\n",
    "# # gt = cudf.read_parquet(\"../output/candidates/candidates_cv7-tv5_val.parquet\")\n",
    "# dfs_vals = dfs_vals.merge(gt, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_vals['pred'] = dfs_vals['pred_1']\n",
    "# evaluate(dfs_vals, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold(dfs_vals, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import WEIGHTS\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CV : 0.5907\n"
     ]
    }
   ],
   "source": [
    "cv = np.average([0.5593, 0.4428, 0.6698], weights=WEIGHTS)  # LB 0.601+\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CV : 0.5903\n"
     ]
    }
   ],
   "source": [
    "cv = np.average([0.5593, 0.4424, 0.6694], weights=WEIGHTS)  # LB 0.601+\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CV : 0.5900\n"
     ]
    }
   ],
   "source": [
    "cv = np.average([0.5593, 0.4420, 0.6691], weights=WEIGHTS)  # LB 0.601\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.5593, 0.4404, 0.6680], weights=WEIGHTS)  # LB 0.600\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.554, 0.4404, 0.6680], weights=WEIGHTS)  # LB 0.599  - High\n",
    "print(f\"-> CV : {cv:.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.554, 0.4392, 0.6673], weights=WEIGHTS)  # LB 0.599  - Low \n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.554, 0.4382, 0.6668], weights=WEIGHTS)  # LB 0.598  - Mid\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = None\n",
    "for exp_folders in EXP_FOLDERS:\n",
    "    dfs_test = []\n",
    "\n",
    "    if not isinstance(exp_folders, list):\n",
    "        exp_folders = [exp_folders]\n",
    "    \n",
    "    for exp_folder in exp_folders:\n",
    "        config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "        print(f'\\t === Exp {exp_folder}\\t Target {config.target} ===\\n')\n",
    "        TARGET = config.target\n",
    "\n",
    "        for fold in range(config.k):\n",
    "            try:\n",
    "                dfs_test.append(cudf.read_parquet(exp_folder + f\"df_test_{fold}.parquet\"))\n",
    "                print(f'-> Retrieved fold {fold}\\n')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    dfs_test = cudf.concat(dfs_test, ignore_index=True).groupby(['session', 'candidates']).mean().reset_index()\n",
    "\n",
    "    print(f'- Retrieved {len(dfs_test)} test candidates.\\n')\n",
    "\n",
    "    if df_test is None:\n",
    "        df_test = dfs_test\n",
    "    else:\n",
    "        df_test = df_test.set_index(['session', 'candidates']).add(\n",
    "            dfs_test.set_index(['session', 'candidates']), fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "    del dfs_test\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df_test[['session', 'candidates', 'pred']].copy()\n",
    "\n",
    "preds = preds.sort_values(['session', 'pred'], ascending=[True, False])\n",
    "preds = preds[['session', 'candidates', 'pred']].groupby('session').agg(list).reset_index()\n",
    "\n",
    "preds = preds.to_pandas()\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill less than 20 candidates. This should be useless in the future\n",
    "dfs = load_sessions(f\"../output/test_parquet/*\")\n",
    "\n",
    "if config.target == \"gt_carts\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 1, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "elif config.target == \"gt_orders\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 2, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "else:\n",
    "    top = dfs.loc[dfs[\"type\"] == 0, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: list(x) + top[: 20 - len(x)])\n",
    "\n",
    "del dfs\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_VERSION = \"cv7-tv5.11\"\n",
    "MODEL_VERSION = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder_2 = LOG_PATH + f\"{FT_VERSION}.{MODEL_VERSION}/\"\n",
    "\n",
    "os.makedirs(log_folder_2, exist_ok=True)\n",
    "save_config(config, log_folder_2 + 'config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = preds[['session', 'candidates']].copy()\n",
    "assert len(sub) == 1671803\n",
    "\n",
    "sub['candidates'] = sub['candidates'].parallel_apply(lambda x: \" \".join(map(str, x)))\n",
    "sub['session'] =  sub['session'].astype(str) + \"_\" + TARGET[3:]\n",
    "sub.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "sub.to_csv(log_folder_2 + f'sub_{TARGET}.csv', index=False)\n",
    "print(f\"-> Saved sub to {log_folder_2 + f'sub_{TARGET}.csv'}\\n\")\n",
    "\n",
    "display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([os.path.exists(log_folder_2 + f'sub_gt_{c}.csv') for c in CLASSES]):\n",
    "    sub_final = cudf.concat([\n",
    "        cudf.read_csv(log_folder_2 + f'sub_gt_{c}.csv') for c in CLASSES\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    assert len(sub_final) == 5015409\n",
    "    sub_final.to_csv(log_folder_2 + f\"submission.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n-> Saved final sub to {log_folder_2 + f'submission.csv'}\\n\")\n",
    "\n",
    "    display(sub_final.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions submit -c otto-recommender-system -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
