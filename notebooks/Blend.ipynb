{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Trains XGBoost models.\n",
    "\n",
    "**TODO**:\n",
    "- better neg sampling technique ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import numba\n",
    "import xgboost\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numerize.numerize import numerize\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.metrics import get_coverage, evaluate\n",
    "from utils.plot import plot_importances\n",
    "from utils.load import *\n",
    "from utils.logger import *\n",
    "\n",
    "from inference.xgb import xgb_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = \"../logs/2023-01-20/9/\"\n",
    "VERSION = \"cv3-tv5.10\"\n",
    "TARGET = \"gt_orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET != \"gt_clicks\":\n",
    "    REGEX = f\"../output/features/fts_val_{VERSION}_{TARGET}/*\"\n",
    "else:\n",
    "    REGEX = f\"../output/features/fts_val_{VERSION}/*\"\n",
    "\n",
    "TEST_REGEX = f\"../output/features/fts_test_{VERSION}/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_inference(REGEX, TEST_REGEX, EXP_FOLDER, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_imp = pd.read_csv(\"../logs/2023-01-20/1/ft_imp.csv\")\n",
    "# list(ft_imp.sort_values('importance')[\"index\"])[:100]\n",
    "# plot_importances(ft_imp.set_index(\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "- neg sampling could use candidates from lower versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # CARTS - 0.4392\n",
    "#     \"../logs/2023-01-17/5/\",   # 0.4389\n",
    "#     \"../logs/2023-01-17/7/\",   # 0.4390\n",
    "    \"../logs/2023-01-17/8/\",   # 0.4391  (0.4405 fold 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # ORDERS - 0.6673\n",
    "    \"../logs/2023-01-17/4/\",    # 0.6674    (0.6672 fold 0)\n",
    "#     \"../logs/2023-01-17/6/\",    # 0.6672\n",
    "#     \"../logs/2023-01-17/9/\",    # 0.6670\n",
    "#     \"../logs/2023-01-17/11/\",     # 0.6669  -  Rank\n",
    "]  # Rank avg blend ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # ORDERS - 0.6680\n",
    "    [\"../logs/2023-01-20/1/\", \"../logs/2023-01-20/9/\", \"../logs/2023-01-20/13/\", \"../logs/2023-01-20/12/\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # Carts - 0.4404\n",
    "    [\"../logs/2023-01-20/17/\", \"../logs/2023-01-20/29/\", \"../logs/2023-01-20/28/\", \"../logs/2023-01-20/27/\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # Clicks - 0.5593\n",
    "    [\"../logs/2023-01-20/26/\", \"../logs/2023-01-20/23/\", \"../logs/2023-01-20/30/\", \"../logs/2023-01-20/32/\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # ORDERS - 0.6680\n",
    "    [\"../logs/2023-01-20/1/\"], #\"../logs/2023-01-20/9/\", \"../logs/2023-01-20/13/\", \"../logs/2023-01-20/12/\"],\n",
    "    [\"../logs/2023-01-21/41/\"],\n",
    "    [\"../logs/2023-01-21/45/\"],\n",
    "]\n",
    "\n",
    "WEIGHTS = [1, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t === Exp ../logs/2023-01-20/1/\t Target gt_orders ===\n",
      "\n",
      "-> Retrieved fold 0\n",
      "-> gt_orders  -  Recall : 0.6676\n",
      "\n",
      "\t === Exp ../logs/2023-01-21/41/\t Target gt_orders ===\n",
      "\n",
      "-> Retrieved fold 0\n",
      "-> gt_orders  -  Recall : 0.6666\n",
      "\n",
      "\t === Exp ../logs/2023-01-21/45/\t Target gt_orders ===\n",
      "\n",
      "-> Retrieved fold 0\n",
      "-> gt_orders  -  Recall : 0.6668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val_ = None\n",
    "for exp_folders, w in zip(EXP_FOLDERS, WEIGHTS):\n",
    "    \n",
    "    if not isinstance(exp_folders, list):\n",
    "        exp_folders = [exp_folders]\n",
    "    \n",
    "    dfs_val = []\n",
    "    for exp_folder in exp_folders:\n",
    "        config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "        print(f'\\t === Exp {exp_folder}\\t Target {config.target} ===\\n')\n",
    "        TARGET = config.target\n",
    "\n",
    "        for fold in range(config.k):\n",
    "            try:\n",
    "                df_val = cudf.read_parquet(exp_folder + f\"df_val_{fold}.parquet\")\n",
    "                df_val['pred'] = df_val.groupby('session')['pred'].rank()\n",
    "#                 df_val['pred'] = (df_val['pred'] - df_val['pred'].min()) / (df_val['pred'].max() - df_val['pred'].min())\n",
    "                df_val['pred'] *= w\n",
    "                dfs_val.append(df_val)\n",
    "                print(f'-> Retrieved fold {fold}', end=\"\")\n",
    "\n",
    "                evaluate(df_val, TARGET)\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "        \n",
    "#     print('\\n ===> CV :')\n",
    "    dfs_val = cudf.concat(dfs_val, ignore_index=True)\n",
    "#     evaluate(dfs_val, TARGET)    \n",
    "\n",
    "    if df_val_ is None: \n",
    "        df_val_ = dfs_val.copy()\n",
    "    else:\n",
    "        df_val_ = df_val_.set_index(['session', 'candidates']).add(\n",
    "            dfs_val.set_index(['session', 'candidates']), fill_value=0\n",
    "        ).reset_index()\n",
    "        \n",
    "    del dfs_val\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> gt_orders  -  Recall : 0.6677\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6677094284468833"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(df_val_, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> gt_orders  -  Recall : 0.6676\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.667606861714403"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(df_val_, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.5593, 0.4404, 0.6680], weights=WEIGHTS)  # LB 0.599  - High\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.554, 0.4404, 0.6680], weights=WEIGHTS)  # LB 0.599  - High\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.554, 0.4392, 0.6673], weights=WEIGHTS)  # LB 0.599  - Low \n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = np.average([0.554, 0.4382, 0.6668], weights=WEIGHTS)  # LB 0.598  - Mid\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = None\n",
    "for exp_folders in EXP_FOLDERS:\n",
    "    dfs_test = []\n",
    "\n",
    "    if not isinstance(exp_folders, list):\n",
    "        exp_folders = [exp_folders]\n",
    "    \n",
    "    for exp_folder in exp_folders:\n",
    "        config = Config(json.load(open(exp_folder + \"config.json\", \"r\")))\n",
    "        print(f'\\t === Exp {exp_folder}\\t Target {config.target} ===\\n')\n",
    "        TARGET = config.target\n",
    "\n",
    "        for fold in range(config.k):\n",
    "            try:\n",
    "                dfs_test.append(cudf.read_parquet(exp_folder + f\"df_test_{fold}.parquet\"))\n",
    "                print(f'-> Retrieved fold {fold}\\n')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    dfs_test = cudf.concat(dfs_test, ignore_index=True).groupby(['session', 'candidates']).mean().reset_index()\n",
    "\n",
    "    print(f'- Retrieved {len(dfs_test)} test candidates.\\n')\n",
    "\n",
    "    if df_test is None:\n",
    "        df_test = dfs_test\n",
    "    else:\n",
    "        df_test = df_test.set_index(['session', 'candidates']).add(\n",
    "            dfs_test.set_index(['session', 'candidates']), fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "    del dfs_test\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df_test[['session', 'candidates', 'pred']].copy()\n",
    "\n",
    "preds = preds.sort_values(['session', 'pred'], ascending=[True, False])\n",
    "preds = preds[['session', 'candidates', 'pred']].groupby('session').agg(list).reset_index()\n",
    "\n",
    "preds = preds.to_pandas()\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill less than 20 candidates. This should be useless in the future\n",
    "dfs = load_sessions(f\"../output/test_parquet/*\")\n",
    "\n",
    "if config.target == \"gt_carts\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 1, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "elif config.target == \"gt_orders\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 2, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "else:\n",
    "    top = dfs.loc[dfs[\"type\"] == 0, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: list(x) + top[: 20 - len(x)])\n",
    "\n",
    "del dfs\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_VERSION = \"cv3-tv5.10\"\n",
    "MODEL_VERSION = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder_2 = LOG_PATH + f\"{FT_VERSION}.{MODEL_VERSION}/\"\n",
    "\n",
    "os.makedirs(log_folder_2, exist_ok=True)\n",
    "save_config(config, log_folder_2 + 'config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = preds[['session', 'candidates']].copy()\n",
    "assert len(sub) == 1671803\n",
    "\n",
    "sub['candidates'] = sub['candidates'].parallel_apply(lambda x: \" \".join(map(str, x)))\n",
    "sub['session'] =  sub['session'].astype(str) + \"_\" + TARGET[3:]\n",
    "sub.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "sub.to_csv(log_folder_2 + f'sub_{TARGET}.csv', index=False)\n",
    "print(f\"-> Saved sub to {log_folder_2 + f'sub_{TARGET}.csv'}\\n\")\n",
    "\n",
    "display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([os.path.exists(log_folder_2 + f'sub_gt_{c}.csv') for c in CLASSES]):\n",
    "    sub_final = cudf.concat([\n",
    "        cudf.read_csv(log_folder_2 + f'sub_gt_{c}.csv') for c in CLASSES\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    assert len(sub_final) == 5015409\n",
    "    sub_final.to_csv(log_folder_2 + f\"submission.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n-> Saved final sub to {log_folder_2 + f'submission.csv'}\\n\")\n",
    "\n",
    "    display(sub_final.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions submit -c otto-recommender-system -f submission.csv -m \"Message\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
