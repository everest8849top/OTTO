{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Trains XGBoost models.\n",
    "\n",
    "**TODO**:\n",
    "- better neg sampling technique ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import numba\n",
    "import xgboost\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.dataset import FeaturesDataset\n",
    "from model_zoo.mlp import define_model\n",
    "\n",
    "from utils.load import *\n",
    "from utils.metrics import get_coverage\n",
    "from utils.logger import save_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v3.5\"\n",
    "# VERSION = \"v2.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "- neg sampling could use candidates from lower versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_RATIO = 0.5\n",
    "TARGET = \"gt_*\"   # \"gt_clicks\", \"gt_carts\", \"gt_orders\", \"gt_*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.23s/it]\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"val\":\n",
    "    df_train = load_parquets_cudf_chunks(\n",
    "        f\"../output/features/fts_train_{VERSION}/*\",\n",
    "        pos_ratio=POS_RATIO,\n",
    "        target=TARGET,\n",
    "        n_chunks=5,\n",
    "    )\n",
    "    val_regex = f\"../output/features/fts_val_{VERSION}/*\"\n",
    "else:  # Test\n",
    "    df_train = load_parquets_cudf_chunks(\n",
    "        f\"../output/features/fts_val_{VERSION}/*\",\n",
    "        pos_ratio=POS_RATIO,\n",
    "        target=TARGET,\n",
    "        n_chunks=5,\n",
    "    )\n",
    "    val_regex = f\"../output/features/fts_test_{VERSION}/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FeaturesDataset(df_train, [\"gt_carts\"], df_train.columns[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_val = pd.read_csv(f'../output/fts_train_{VERSION}.csv', nrows=10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = define_model(\"res\")\n",
    "# x = torch.rand(5, 50)\n",
    "# model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "import cudf\n",
    "from numerize.numerize import numerize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from training.train import fit\n",
    "from inference.predict import predict\n",
    "from model_zoo.mlp import define_model\n",
    "from data.dataset import FeaturesDataset\n",
    "from utils.load import load_parquets_cudf\n",
    "from utils.torch import seed_everything, count_parameters\n",
    "\n",
    "\n",
    "def train(df_train, val_regex, config, log_folder=None):\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    print(f\"\\n-------------   Training {config.model.upper()} Model   -------------\\n\")\n",
    "    \n",
    "    train_dataset = FeaturesDataset(df_train, config.target, config.features)\n",
    "\n",
    "    if config.mode != \"test\":\n",
    "        df_val = load_parquets_cudf(val_regex, max_n=1).to_pandas()\n",
    "        val_dataset = FeaturesDataset(df_val, config.target, config.features)\n",
    "    \n",
    "    model = define_model(\n",
    "        name=config.model,\n",
    "        nb_ft=config.nb_ft,\n",
    "        d=config.d,\n",
    "        p=config.p,\n",
    "        num_layers=config.num_layers,\n",
    "        num_classes=config.num_classes\n",
    "    ).cuda()\n",
    "    model.zero_grad()\n",
    "        \n",
    "    print(f\"    -> {numerize(len(df_train))} training candidates\")\n",
    "    print(f\"    -> {numerize(len(df_val))} validation candidates subset\")\n",
    "    print(f\"    -> {numerize(count_parameters(model))} trainable parameters\\n\")\n",
    "    \n",
    "    pred_val = fit(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        config.data_config,\n",
    "        config.loss_config,\n",
    "        config.optimizer_config,\n",
    "        epochs=config.epochs,\n",
    "        verbose_eval=config.verbose_eval,\n",
    "        use_fp16=config.use_fp16,\n",
    "        run=None,\n",
    "    )\n",
    "\n",
    "    val_candids = sum([len(cudf.read_parquet(f, columns=['gt_orders'])) for f in glob.glob(val_regex)])\n",
    "    print(f\"\\n    -> Inferring {numerize(val_candids)} candidates\\n\")\n",
    "\n",
    "    cols = ['session', 'candidates', 'gt_clicks', 'gt_carts', 'gt_orders', 'pred_clicks', 'pred_carts', 'pred_orders']\n",
    "\n",
    "    dfs = []\n",
    "    for path in tqdm(glob.glob(val_regex)):\n",
    "        dfg = cudf.read_parquet(path)\n",
    "        dataset =  FeaturesDataset(dfg.to_pandas(), None, config.features)\n",
    "        preds = predict(model, dataset, config.loss_config, batch_size=config.data_config[\"val_bs\"])\n",
    "        \n",
    "        for i, tgt in enumerate(config.target):\n",
    "            dfg[\"pred\" + tgt[2:]] = preds[:, i]\n",
    "        dfs.append(dfg[[c for c in cols if c in dfg.columns]])\n",
    "#         break\n",
    "\n",
    "    results = cudf.concat(dfs, ignore_index=True).sort_values(['session', 'candidates'])\n",
    "\n",
    "    if config.mode == \"test\":\n",
    "        return results\n",
    "\n",
    "    # Score\n",
    "    print()\n",
    "    for i, tgt in enumerate(config.target):\n",
    "        auc = cuml.metrics.roc_auc_score(results[tgt].astype('int32'), results[\"pred\" + tgt[2:]].values)\n",
    "        print(f'-> {tgt} - AUC : {auc:.4f}')\n",
    "\n",
    "    if log_folder is None:\n",
    "        return results\n",
    "\n",
    "    # Save stuff\n",
    "    # TODO\n",
    "#     results.to_csv(log_folder + \"df_val.csv\", index=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 100\n",
    "    version = VERSION\n",
    "    mode = MODE\n",
    "\n",
    "    features = [\n",
    "        'logspace_w', 'linspace_w', 'linspace_w_t163', 'logspace_w_t163', 'linspace_w_t191', 'logspace_w_t191',\n",
    "\n",
    "        'matrix_123_temporal_20_mean', 'matrix_123_temporal_20_sum', 'matrix_123_temporal_20_max',\n",
    "        'matrix_123_temporal_20_logspace_mean', 'matrix_123_temporal_20_logspace_sum', 'matrix_123_temporal_20_logspace_max',\n",
    "        'matrix_123_temporal_20_linspace_mean', 'matrix_123_temporal_20_linspace_sum', 'matrix_123_temporal_20_linspace_max',\n",
    "        'matrix_123_type136_20_mean', 'matrix_123_type136_20_sum', 'matrix_123_type136_20_max',\n",
    "        'matrix_123_type136_20_logspace_mean', 'matrix_123_type136_20_logspace_sum', 'matrix_123_type136_20_logspace_max',\n",
    "        'matrix_123_type136_20_linspace_mean', 'matrix_123_type136_20_linspace_sum', 'matrix_123_type136_20_linspace_max',\n",
    "        'matrix_12__20_mean', 'matrix_12__20_sum', 'matrix_12__20_max',\n",
    "        'matrix_12__20_logspace_mean', 'matrix_12__20_logspace_sum', 'matrix_12__20_logspace_max',\n",
    "        'matrix_12__20_linspace_mean', 'matrix_12__20_linspace_sum', 'matrix_12__20_linspace_max',\n",
    "        'matrix_123_type0.590.5_20_mean', 'matrix_123_type0.590.5_20_sum', 'matrix_123_type0.590.5_20_max',\n",
    "        'matrix_123_type0.590.5_20_logspace_mean', 'matrix_123_type0.590.5_20_logspace_sum', 'matrix_123_type0.590.5_20_logspace_max',\n",
    "        'matrix_123_type0.590.5_20_linspace_mean', 'matrix_123_type0.590.5_20_linspace_sum', 'matrix_123_type0.590.5_20_linspace_max',\n",
    "        \n",
    "        'clicks_popularity_w', 'carts_popularity_w', 'orders_popularity_w',\n",
    "        'view_popularity_log_w', 'view_popularity_lin_w', \n",
    "    \n",
    "        'clicks_popularity', 'carts_popularity', 'orders_popularity',\n",
    "        'view_popularity_log', 'view_popularity_lin',\n",
    "        \n",
    "        'clicks_popularity_old', 'carts_popularity_old', 'orders_popularity_old',\n",
    "        'view_popularity_log_old', 'view_popularity_lin_old',\n",
    "\n",
    "        'candidate_clicks_before', 'candidate_carts_before', 'candidate_orders_before', 'candidate_*_before',\n",
    "        'n_views', 'n_clicks', 'n_carts', 'n_orders',\n",
    "    ]\n",
    "\n",
    "    target = [TARGET] if TARGET != \"gt_*\" else [\"gt_clicks\", \"gt_carts\", \"gt_orders\"]\n",
    "    pos_ratio = POS_RATIO\n",
    "    \n",
    "    # Model\n",
    "    model = \"mlp\"\n",
    "    nb_ft = len(features)\n",
    "    d = 512\n",
    "    p = 0.1\n",
    "    num_layers = 3\n",
    "    num_classes = len(target)\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"name\": \"bce\",\n",
    "        \"smoothing\": 0.,\n",
    "        \"activation\": \"sigmoid\",\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"batch_size\": 2 ** 17,\n",
    "        \"val_bs\": 2 ** 17,\n",
    "        \"use_balanced_sampler\": False,  # TODO\n",
    "        \"use_weighted_sampler\": False,  # TODO\n",
    "        \"sampler_weights\": [1, 3],  # TODO\n",
    "    }\n",
    "\n",
    "    optimizer_config = {\n",
    "        \"name\": \"Adam\",\n",
    "        \"lr\": 3e-3,\n",
    "        \"warmup_prop\": 0.,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "    }\n",
    "\n",
    "    epochs = 50  # 70\n",
    "\n",
    "    use_fp16 = True\n",
    "\n",
    "    verbose = 1\n",
    "    verbose_eval = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE = False\n",
    "TRAIN = True\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------   Training MLP Model   -------------\n",
      "\n",
      "    -> 4.88M training candidates\n",
      "    -> 9.82M validation candidates subset\n",
      "    -> 208.52K trainable parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_folder = None\n",
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f'Logging results to {log_folder}')\n",
    "    save_config(Config, log_folder + 'config')\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "df_val = train(df_train, val_regex, Config, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB ES AUC : 0.96893\n",
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_sessions(f\"../output/{MODE}_parquet/*\")\n",
    "preds = df_val[['session']].drop_duplicates(keep=\"first\").sort_values('session', ignore_index=True).to_pandas()\n",
    "\n",
    "for idx, c in enumerate(CLASSES):\n",
    "    if \"gt_\" + c not in Config.target:\n",
    "        continue\n",
    "            \n",
    "    preds_c = df_val.sort_values(['session', f'pred_{c}'], ascending=[True, False])\n",
    "    preds_c = preds_c[['session', 'candidates', f'pred_{c}']].groupby('session').agg(list).reset_index()\n",
    "\n",
    "    preds_c = preds_c.to_pandas()\n",
    "    preds_c['candidates'] = preds_c['candidates'].apply(lambda x: x[:20])\n",
    "    \n",
    "    # Fill less than 20 candidates. This should be useless in the future\n",
    "    top = dfs.loc[dfs[\"type\"] == idx, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "    preds_c['candidates'] = preds_c['candidates'].apply(lambda x: list(x) + top[:20 - len(x)])\n",
    "    \n",
    "    preds_c = preds_c.sort_values('session')\n",
    "    preds[f\"candidates_{c}\"] = preds_c[\"candidates\"].values\n",
    "    preds[f'pred_{c}'] = preds_c[f'pred_{c}'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dfs, preds_c\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval\n",
    "- 0.6576 5 leaves\n",
    "- 0.6577 4 leaves\n",
    "- 0.6575 5 leaves mcw0.01\n",
    "- 0.6576 5 leaves mcw0.0001\n",
    "- 0.6576 6 leaves mcw0.0001\n",
    "- 0.6576 6 leaves mcw0.001\n",
    "- 0.657. 6 leaves mcw0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- clicks\t-  Found 32.62K GTs\t-  Recall : 0.3348\n",
      "- carts\t-  Found 5.53K GTs\t-  Recall : 0.1938\n",
      "- orders\t-  Found 3.22K GTs\t-  Recall : 0.2094\n"
     ]
    }
   ],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(\"../output/val_labels.parquet\")\n",
    "\n",
    "    recalls = []\n",
    "    for col in CLASSES:\n",
    "        if \"gt_\" + col not in Config.target:\n",
    "            continue\n",
    "\n",
    "        if f\"gt_{col}\" not in preds.columns:\n",
    "            preds = preds.merge(gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\").rename(\n",
    "                columns={\"ground_truth\": f\"gt_{col}\"}\n",
    "            )\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            preds[f\"candidates_{col}\"].values, preds[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t-  Found {numerize(n_found)} GTs\\t-  Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)\n",
    "        \n",
    "        \n",
    "    cv = np.average(recalls, weights=WEIGHTS)\n",
    "    # cv = np.average([0.5059, 0.4139, 0.6540], weights=WEIGHTS)\n",
    "    print(f\"\\n-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- orders\t-  Found 205.66K GTs\t-  Recall : 0.6566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> CV : 0.5734\n"
     ]
    }
   ],
   "source": [
    "cv = np.average([0.5270, 0.4203, 0.6577], weights=WEIGHTS)\n",
    "# cv = np.average([0.5059, 0.4139, 0.6540], weights=WEIGHTS)\n",
    "print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"test\":\n",
    "    log_folder = LOG_PATH + f\"{VERSION}.0/\"\n",
    "    os.makedirs(log_folder, exist_ok=True)\n",
    "    save_config(Config, log_folder + 'config')\n",
    "\n",
    "    sub = preds[['session', 'candidates']].copy()\n",
    "    assert len(sub) == 1671803\n",
    "\n",
    "    sub['candidates'] = sub['candidates'].parallel_apply(lambda x: \" \".join(map(str, x)))\n",
    "    sub['session'] =  sub['session'].astype(str) + \"_\" + TARGET[3:]\n",
    "    sub.columns = [\"session_type\", \"labels\"]\n",
    "    \n",
    "    sub.to_csv(log_folder + f'sub_{TARGET}.csv', index=False)\n",
    "    print(f\"-> Saved sub to {log_folder + f'sub_{TARGET}.csv'}\\n\")\n",
    "\n",
    "    display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cudf.read_csv(\"../input/sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"test\":\n",
    "    if all([os.path.exists(log_folder + f'sub_gt_{c}.csv') for c in CLASSES]):\n",
    "        \n",
    "        sub_final = cudf.concat([\n",
    "            cudf.read_csv(log_folder + f'sub_gt_{c}.csv') for c in CLASSES\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        assert len(sub_final) == 5015409\n",
    "        sub_final.to_csv(log_folder + f\"submission_{cv:.4f}.csv\", index=False)\n",
    "        \n",
    "        print(f\"-> Saved final sub to {log_folder + f'submission_{cv:.4f}.csv'}\\n\")\n",
    "        \n",
    "        display(sub_final.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
