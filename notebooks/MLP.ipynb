{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Trains XGBoost models.\n",
    "\n",
    "**TODO**:\n",
    "- Merlin loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import numba\n",
    "import xgboost\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.dataset import FeaturesDataset\n",
    "from model_zoo.mlp import define_model\n",
    "from training.mlp import train\n",
    "\n",
    "from utils.load import *\n",
    "from utils.metrics import get_coverage\n",
    "from utils.logger import save_config, prepare_log_folder, create_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v3.5\"\n",
    "# VERSION = \"v2.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "- neg sampling could use candidates from lower versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_RATIO = 0.1\n",
    "TARGET = \"gt_*\"   # \"gt_clicks\", \"gt_carts\", \"gt_orders\", \"gt_*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'../output/tmp/df_{MODE}_{POS_RATIO}_{TARGET}.parquet'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    if MODE == \"val\":\n",
    "        df_train = load_parquets_cudf_chunks(\n",
    "            f\"../output/features/fts_train_{VERSION}/*\",\n",
    "            pos_ratio=POS_RATIO,\n",
    "            target=TARGET,\n",
    "            n_chunks=5,\n",
    "        )\n",
    "    \n",
    "    else:  # Test\n",
    "        df_train = load_parquets_cudf_chunks(\n",
    "            f\"../output/features/fts_val_{VERSION}/*\",\n",
    "            pos_ratio=POS_RATIO,\n",
    "            target=TARGET,\n",
    "            n_chunks=5,\n",
    "        )\n",
    "        val_regex = f\"../output/features/fts_test_{VERSION}/*\"\n",
    "        \n",
    "    print(f'-> Saving to {path}')\n",
    "    df_train.to_pandas().to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    val_regex = f\"../output/features/fts_val_{VERSION}/*\"\n",
    "else:\n",
    "    val_regex = f\"../output/features/fts_test_{VERSION}/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = glob.glob(val_regex)\n",
    "train_files = [path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.io import Dataset\n",
    "from merlin.loader.torch import Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(train_files, engine=\"parquet\")\n",
    "loader = Loader(\n",
    "    dataset,\n",
    "    batch_size=2**16,\n",
    "    shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     worker_init_fn=worker_init_fn,\n",
    "#     persistent_workers=True,\n",
    "#     num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# batch = next(iter(loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = cudf.read_parquet(path, columns=['session', \"candidates\"] + [TARGET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# x = torch.cat([batch[k] for k in batch.keys()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = FeaturesDataset(df_train, [\"gt_carts\"], df_train.columns[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_val = pd.read_csv(f'../output/fts_train_{VERSION}.csv', nrows=10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = define_model(\"res\")\n",
    "# x = torch.rand(5, 50)\n",
    "# model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 100\n",
    "    version = VERSION\n",
    "    mode = MODE\n",
    "    pos_ratio = POS_RATIO\n",
    "\n",
    "    features = [\n",
    "        'logspace_w', 'linspace_w', 'linspace_w_t163', 'logspace_w_t163', 'linspace_w_t191', 'logspace_w_t191',\n",
    "\n",
    "        'matrix_123_temporal_20_mean', 'matrix_123_temporal_20_sum', 'matrix_123_temporal_20_max',\n",
    "        'matrix_123_temporal_20_logspace_mean', 'matrix_123_temporal_20_logspace_sum', 'matrix_123_temporal_20_logspace_max',\n",
    "        'matrix_123_temporal_20_linspace_mean', 'matrix_123_temporal_20_linspace_sum', 'matrix_123_temporal_20_linspace_max',\n",
    "        'matrix_123_type136_20_mean', 'matrix_123_type136_20_sum', 'matrix_123_type136_20_max',\n",
    "        'matrix_123_type136_20_logspace_mean', 'matrix_123_type136_20_logspace_sum', 'matrix_123_type136_20_logspace_max',\n",
    "        'matrix_123_type136_20_linspace_mean', 'matrix_123_type136_20_linspace_sum', 'matrix_123_type136_20_linspace_max',\n",
    "        'matrix_12__20_mean', 'matrix_12__20_sum', 'matrix_12__20_max',\n",
    "        'matrix_12__20_logspace_mean', 'matrix_12__20_logspace_sum', 'matrix_12__20_logspace_max',\n",
    "        'matrix_12__20_linspace_mean', 'matrix_12__20_linspace_sum', 'matrix_12__20_linspace_max',\n",
    "        'matrix_123_type0.590.5_20_mean', 'matrix_123_type0.590.5_20_sum', 'matrix_123_type0.590.5_20_max',\n",
    "        'matrix_123_type0.590.5_20_logspace_mean', 'matrix_123_type0.590.5_20_logspace_sum', 'matrix_123_type0.590.5_20_logspace_max',\n",
    "        'matrix_123_type0.590.5_20_linspace_mean', 'matrix_123_type0.590.5_20_linspace_sum', 'matrix_123_type0.590.5_20_linspace_max',\n",
    "        \n",
    "        'clicks_popularity_w', 'carts_popularity_w', 'orders_popularity_w',\n",
    "        'view_popularity_log_w', 'view_popularity_lin_w', \n",
    "    \n",
    "        'clicks_popularity', 'carts_popularity', 'orders_popularity',\n",
    "        'view_popularity_log', 'view_popularity_lin',\n",
    "        \n",
    "        'clicks_popularity_old', 'carts_popularity_old', 'orders_popularity_old',\n",
    "        'view_popularity_log_old', 'view_popularity_lin_old',\n",
    "\n",
    "        'candidate_clicks_before', 'candidate_carts_before', 'candidate_orders_before', 'candidate_*_before',\n",
    "        'n_views', 'n_clicks', 'n_carts', 'n_orders',\n",
    "    ]\n",
    "\n",
    "    target = [TARGET] if TARGET != \"gt_*\" else [\"gt_clicks\", \"gt_carts\", \"gt_orders\"]\n",
    "    pos_ratio = POS_RATIO\n",
    "    \n",
    "    # Model\n",
    "    model = \"mlp\"\n",
    "    nb_ft = len(features)\n",
    "    d = 768\n",
    "    p = 0.1\n",
    "    num_layers = 3\n",
    "    num_classes = len(target)\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"name\": \"bce\",\n",
    "        \"smoothing\": 0.,\n",
    "        \"activation\": \"sigmoid\",\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"target\": target,\n",
    "        \"features\": features,\n",
    "        \"batch_size\": 2 ** 17,\n",
    "        \"val_bs\": 2 ** 17,\n",
    "    }\n",
    "\n",
    "    optimizer_config = {\n",
    "        \"name\": \"Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"warmup_prop\": 0.,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "    }\n",
    "\n",
    "    epochs = 50  # 70\n",
    "\n",
    "    use_fp16 = True\n",
    "\n",
    "    verbose = 1\n",
    "    verbose_eval = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "df_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_folder = None\n",
    "# if not DEBUG:\n",
    "#     log_folder = prepare_log_folder(LOG_PATH)\n",
    "#     print(f'Logging results to {log_folder}')\n",
    "#     save_config(Config, log_folder + 'config')\n",
    "#     create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "# df_val = train(train_files, val_files, Config, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGB ES AUC carts : 0.96893\n",
    "- Best CV :  0.5696\n",
    "- CV to beat : 0.5735\n",
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "    \"../logs/2023-01-08/13/\",\n",
    "    \"../logs/2023-01-08/16/\",\n",
    "    \"../logs/2023-01-08/17/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = None\n",
    "\n",
    "for exp_folder in tqdm(EXP_FOLDERS):\n",
    "    df_val_ = cudf.read_parquet(exp_folder + \"results.parquet\")\n",
    "    df_val_ = df_val_.sort_values(['session', 'candidates'])\n",
    "    if df_val is None:\n",
    "        df_val = df_val_.copy()\n",
    "    else:\n",
    "#         assert df_val\n",
    "        for c in df_val.columns[2:]:\n",
    "            df_val[c] += df_val_[c]\n",
    "\n",
    "for c in df_val.columns[2:]:\n",
    "    df_val[c] /= len(EXP_FOLDERS)\n",
    "\n",
    "df_val[['session', 'candidates']] = df_val[['session', 'candidates']].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_sessions(f\"../output/{MODE}_parquet/*\")\n",
    "preds = df_val[['session']].drop_duplicates(keep=\"first\").sort_values('session', ignore_index=True).to_pandas()\n",
    "\n",
    "for idx, c in enumerate(CLASSES):\n",
    "    if \"gt_\" + c not in Config.target:\n",
    "        continue\n",
    "            \n",
    "    preds_c = df_val.sort_values(['session', f'pred_{c}'], ascending=[True, False])\n",
    "    preds_c = preds_c[['session', 'candidates', f'pred_{c}']].groupby('session').agg(list).reset_index()\n",
    "\n",
    "    preds_c = preds_c.to_pandas()\n",
    "    preds_c['candidates'] = preds_c['candidates'].apply(lambda x: x[:20])\n",
    "    \n",
    "    # Fill less than 20 candidates. This should be useless in the future\n",
    "    top = dfs.loc[dfs[\"type\"] == idx, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "    preds_c['candidates'] = preds_c['candidates'].apply(lambda x: list(x) + top[:20 - len(x)])\n",
    "    \n",
    "    preds_c = preds_c.sort_values('session')\n",
    "    preds[f\"candidates_{c}\"] = preds_c[\"candidates\"].values\n",
    "    preds[f'pred_{c}'] = preds_c[f'pred_{c}'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfs, preds_c\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(\"../output/val_labels.parquet\")\n",
    "\n",
    "    recalls = []\n",
    "    for col in CLASSES:\n",
    "        if \"gt_\" + col not in Config.target:\n",
    "            continue\n",
    "\n",
    "        if f\"gt_{col}\" not in preds.columns:\n",
    "            preds = preds.merge(gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\").rename(\n",
    "                columns={\"ground_truth\": f\"gt_{col}\"}\n",
    "            )\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            preds[f\"candidates_{col}\"].values, preds[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t-  Found {numerize(n_found)} GTs\\t-  Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)\n",
    "        \n",
    "    cv = np.average(recalls, weights=WEIGHTS)\n",
    "    # cv = np.average([0.5059, 0.4139, 0.6540], weights=WEIGHTS)\n",
    "    print(f\"\\n-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clicks \t-  Found 924.24K GTs\t-  Recall : 0.5265\n",
    "- carts \t-  Found 241.15K GTs\t-  Recall : 0.4186\n",
    "- orders \t-  Found 205.97K GTs\t-  Recall : 0.6575\n",
    "\n",
    "-> CV : 0.5728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"test\":\n",
    "    sub = preds[['session', 'candidates']].copy()\n",
    "    assert len(sub) == 1671803\n",
    "\n",
    "    sub['candidates'] = sub['candidates'].parallel_apply(lambda x: \" \".join(map(str, x)))\n",
    "    sub['session'] =  sub['session'].astype(str) + \"_\" + TARGET[3:]\n",
    "    sub.columns = [\"session_type\", \"labels\"]\n",
    "    \n",
    "    sub.to_csv(log_folder + f'sub_{TARGET}.csv', index=False)\n",
    "    print(f\"-> Saved sub to {log_folder + f'sub_{TARGET}.csv'}\\n\")\n",
    "\n",
    "    display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"test\":\n",
    "    if all([os.path.exists(log_folder + f'sub_gt_{c}.csv') for c in CLASSES]):\n",
    "        \n",
    "        sub_final = cudf.concat([\n",
    "            cudf.read_csv(log_folder + f'sub_gt_{c}.csv') for c in CLASSES\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        assert len(sub_final) == 5015409\n",
    "        sub_final.to_csv(log_folder + f\"submission_{cv:.4f}.csv\", index=False)\n",
    "        \n",
    "        print(f\"-> Saved final sub to {log_folder + f'submission_{cv:.4f}.csv'}\\n\")\n",
    "        \n",
    "        display(sub_final.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
