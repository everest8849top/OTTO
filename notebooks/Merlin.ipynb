{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/kaggle/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers4rec[pytorch,nvtabular]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvtabular as nvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.dataset import OttoDataset\n",
    "# from data.preparation import prepare_data\n",
    "# # from training.main import k_fold\n",
    "# from models import OttoTransformer\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.logger import prepare_log_folder, save_config, create_logger\n",
    "\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in tqdm(glob.glob(\"../output/train_*.parquet\")):\n",
    "    df = pd.read_parquet(path)\n",
    "    df['target'] = df['type'].apply(lambda x: [CLASSES.index(c) + 1 for c in x])\n",
    "    df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nvt.Dataset([\"../output/train_0.parquet\"], engine=\"parquet\")\n",
    "\n",
    "CONTINUOUS_COLUMNS = ['ts']\n",
    "CATEGORICAL_COLUMNS = ['aid']\n",
    "LABEL_COLUMNS = ['target']\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TorchAsyncItr(\n",
    "   dataset,\n",
    "   cats=CATEGORICAL_COLUMNS,\n",
    "   conts=CONTINUOUS_COLUMNS,\n",
    "   labels=LABEL_COLUMNS,\n",
    "   batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DLDataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=None,\n",
    "#    collate_fn=collate_fn,\n",
    "   pin_memory=False,\n",
    "   num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200000 [01:39<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_loader):\n",
    "    break\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aid': [tensor([1517085, 1563459, 1309446,   16246, 1781822, 1152674, 1649869,  461689,\n",
       "            305831,  461689,  362233, 1649869, 1649869,  984597, 1649869,  803544,\n",
       "           1110941, 1190046, 1760685,  631008,  461689, 1190046, 1650637,  313546,\n",
       "           1650637,  979517,  351157, 1062149, 1157384, 1841388, 1469630,  305831,\n",
       "           1110548, 1110548,  305831, 1650114, 1604396, 1009750, 1800933,  495779,\n",
       "            394655,  495779,  789245,  789245,  366890,  361317, 1700164, 1755597,\n",
       "            789245,  784978, 1171505,  784978, 1700164,  784978, 1521766, 1725503,\n",
       "            528847, 1816325,  984597, 1072782,  173702, 1072782, 1407538, 1629651,\n",
       "           1768568, 1318324, 1840418, 1813509, 1813509,  667924, 1226444,  709550,\n",
       "            709417, 1225559, 1048044, 1052813, 1225559,  240346, 1582117, 1707783,\n",
       "           1624436, 1157411,  358305, 1202970,  832192, 1498443,  723931, 1436439,\n",
       "           1693461, 1206554, 1110741,  346352, 1802050,  154930,  964169,  964169,\n",
       "            823637,  964169, 1411683,  964169, 1167722,  964169, 1619737,  964169,\n",
       "           1840615,  512756,  946219, 1090479, 1164387, 1308544,  719622, 1750538,\n",
       "           1443747, 1750538,  337364, 1653945, 1222638, 1622987,  608383, 1460239,\n",
       "           1436439,  321397,  828625, 1624436, 1157411, 1537907, 1070142,  959208,\n",
       "            275288, 1318324, 1072782, 1072782,  173702, 1428075,  892659, 1127565,\n",
       "           1072782,   97836,  384343,  218130,  294248,  166547,  504365,  102416,\n",
       "             30373,  724999], device='cuda:0'),\n",
       "   tensor([[0]], device='cuda:0')],\n",
       "  'ts': [tensor([1.6593e+12, 1.6593e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12,\n",
       "           1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12,\n",
       "           1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12,\n",
       "           1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6594e+12, 1.6595e+12, 1.6595e+12,\n",
       "           1.6595e+12, 1.6595e+12, 1.6595e+12, 1.6595e+12, 1.6595e+12, 1.6595e+12,\n",
       "           1.6595e+12, 1.6596e+12, 1.6596e+12, 1.6596e+12, 1.6596e+12, 1.6596e+12,\n",
       "           1.6596e+12, 1.6596e+12, 1.6596e+12, 1.6596e+12, 1.6596e+12, 1.6596e+12,\n",
       "           1.6597e+12, 1.6597e+12, 1.6597e+12, 1.6597e+12, 1.6597e+12, 1.6597e+12,\n",
       "           1.6597e+12, 1.6597e+12, 1.6597e+12, 1.6597e+12, 1.6597e+12, 1.6597e+12,\n",
       "           1.6597e+12, 1.6598e+12, 1.6598e+12, 1.6598e+12, 1.6598e+12, 1.6598e+12,\n",
       "           1.6598e+12, 1.6598e+12, 1.6599e+12, 1.6600e+12, 1.6600e+12, 1.6600e+12,\n",
       "           1.6600e+12, 1.6600e+12, 1.6600e+12, 1.6600e+12, 1.6600e+12, 1.6601e+12,\n",
       "           1.6601e+12, 1.6601e+12, 1.6601e+12, 1.6601e+12, 1.6602e+12, 1.6602e+12,\n",
       "           1.6602e+12, 1.6604e+12, 1.6604e+12, 1.6604e+12, 1.6604e+12, 1.6605e+12,\n",
       "           1.6605e+12, 1.6605e+12, 1.6605e+12, 1.6605e+12, 1.6605e+12, 1.6605e+12,\n",
       "           1.6605e+12, 1.6605e+12, 1.6605e+12, 1.6605e+12, 1.6605e+12, 1.6606e+12,\n",
       "           1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12,\n",
       "           1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12,\n",
       "           1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12, 1.6606e+12,\n",
       "           1.6607e+12, 1.6607e+12, 1.6607e+12, 1.6607e+12, 1.6607e+12, 1.6607e+12,\n",
       "           1.6607e+12, 1.6607e+12, 1.6607e+12, 1.6607e+12, 1.6607e+12, 1.6607e+12,\n",
       "           1.6607e+12, 1.6608e+12, 1.6608e+12, 1.6608e+12, 1.6608e+12, 1.6608e+12,\n",
       "           1.6608e+12, 1.6608e+12, 1.6609e+12, 1.6609e+12, 1.6609e+12, 1.6609e+12,\n",
       "           1.6610e+12, 1.6610e+12, 1.6610e+12, 1.6610e+12, 1.6610e+12, 1.6610e+12,\n",
       "           1.6611e+12, 1.6611e+12], device='cuda:0'),\n",
       "   tensor([[0]], device='cuda:0')]},\n",
       " [None,\n",
       "  {'target': [tensor([0., 0., 0., 0., 0., 0., 1., 1., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0.], device='cuda:0'),\n",
       "    tensor([[0]], device='cuda:0')]}]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers4rec import torch as tr\n",
    "from merlin_standard_lib import Schema\n",
    "from transformers4rec.torch.ranking_metric import RecallAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'session_id', 'value_count': {'min': '2', 'max': '20'}, 'type': 'INT', 'int_domain': {'name': 'session_id', 'max': '1855610', 'is_categorical': True}, 'annotation': {'tag': ['item_id', 'list', 'categorical', 'item']}}, {'name': 'target', 'value_count': {'min': '2', 'max': '20'}, 'type': 'INT', 'int_domain': {'name': 'target', 'max': '2', 'is_categorical': True}, 'annotation': {'tag': ['list', 'categorical', 'item']}}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA_PATH = \"../output/schema.pb\"\n",
    "\n",
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=20,\n",
    "#         continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"mlm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularSequenceFeatures(\n",
       "  (to_merge): ModuleDict(\n",
       "    (categorical_module): SequenceEmbeddingFeatures(\n",
       "      (filter_features): FilterFeatures()\n",
       "      (embedding_tables): ModuleDict(\n",
       "        (session_id): Embedding(1855611, 64, padding_idx=0)\n",
       "        (target): Embedding(3, 64, padding_idx=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_aggregation): ConcatFeatures()\n",
       "  (projection_module): SequentialBlock(\n",
       "    (0): DenseBlock(\n",
       "      (0): Linear(in_features=128, out_features=100, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (_masking): MaskedLanguageModeling()\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=20\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = tr.SequentialBlock(\n",
    "    inputs,\n",
    "    tr.MLPBlock([64]),\n",
    "    tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Defines the evaluation top-N metrics and the cut-offs\n",
    "metrics = [\n",
    "    RecallAt(top_ks=[20], labels_onehot=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([-1, 20, 100])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.output_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a head related to next item prediction task \n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, hf_format=True, metrics=metrics),\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the end-to-end Model class \n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = {\n",
    "    \"microsoft/deberta-v3-base\": 32,\n",
    "    \"microsoft/deberta-v3-large\": 32,\n",
    "}\n",
    "\n",
    "LRS = {\n",
    "    \"microsoft/deberta-v3-base\": 3e-5,\n",
    "    \"microsoft/deberta-v3-large\": 3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # General\n",
    "    seed = 2222\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # Splits\n",
    "    k = 4\n",
    "    random_state = 2222\n",
    "    selected_folds = [0, 1, 2, 3]\n",
    "    folds_file = \"/workspace/folds_kgd_4.csv\"\n",
    "\n",
    "    # Architecture\n",
    "    name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "    pretrained_weights = None \n",
    "\n",
    "    no_dropout = False\n",
    "    use_conv = False\n",
    "    use_lstm = False\n",
    "    nb_layers = 1\n",
    "    nb_ft = 128\n",
    "    conv_kernel = 5\n",
    "    drop_p = 0 if no_dropout else 0.1\n",
    "    multi_sample_dropout = False\n",
    "\n",
    "    num_classes = 3\n",
    "    n_ids = N_IDS\n",
    "\n",
    "    # Texts\n",
    "    max_len_train = 410\n",
    "    max_len = 410\n",
    "\n",
    "#     extra_data_path = OUT_PATH + \"pl_case5/\"\n",
    "    extra_data_path = None  # OUT_PATH + \"pl_6/df_pl.csv\"\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"name\": \"bce\",  # ce, bce\n",
    "        \"smoothing\": 0,  # 0.01\n",
    "        \"activation\": \"sigmoid\",  # \"sigmoid\", \"softmax\"\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"batch_size\": BATCH_SIZES[name],\n",
    "        \"val_bs\": BATCH_SIZES[name] * 2,\n",
    "        \"use_len_sampler\": True,\n",
    "        \"pad_token\": 1 if \"roberta\" in name else 0,\n",
    "    }\n",
    "\n",
    "    optimizer_config = {\n",
    "        \"name\": \"AdamW\",\n",
    "        \"lr\": 5e-5,\n",
    "        \"lr_transfo\": LRS[name],\n",
    "        \"lr_decay\": 0.99,\n",
    "        \"warmup_prop\": 0.1,\n",
    "        \"weight_decay\": 1,\n",
    "        \"betas\": (0.5, 0.99),\n",
    "        \"max_grad_norm\": 1.,\n",
    "        # AWP\n",
    "        \"use_awp\": False,\n",
    "        \"awp_start_step\": 1000,\n",
    "        \"awp_lr\": 1,\n",
    "        \"awp_eps\": 5e-5 if \"xlarge\" in name else 1e-3,\n",
    "        \"awp_period\": 3,\n",
    "        # SWA\n",
    "        \"use_swa\": False,\n",
    "        \"swa_start\": 9400,\n",
    "        \"swa_freq\": 500,\n",
    "    }\n",
    "\n",
    "    gradient_checkpointing = False\n",
    "    acc_steps = 1\n",
    "    epochs = 1\n",
    "\n",
    "    use_fp16 = True\n",
    "\n",
    "    verbose = 1\n",
    "    verbose_eval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "log_folder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f\"Logging results to {log_folder}\")\n",
    "    save_config(Config, log_folder + \"config.json\")\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "pred_val, pred_test = k_fold(\n",
    "    Config,\n",
    "    df,\n",
    "    df_test=df_test,\n",
    "    log_folder=log_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
