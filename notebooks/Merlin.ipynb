{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/kaggle/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nvtabular as nvt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.ops import *\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin_standard_lib import Schema\n",
    "from transformers4rec import torch as tr\n",
    "from transformers4rec.torch.ranking_metric import RecallAt\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "from trainer import Trainer\n",
    "from transformers4rec.config.trainer import T4RecTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.dataset import OttoDataset\n",
    "# from data.preparation import prepare_data\n",
    "# # from training.main import k_fold\n",
    "# from models import OttoTransformer\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.logger import prepare_log_folder, save_config, create_logger\n",
    "\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for path in tqdm(glob.glob(\"../output/train_*.parquet\")):\n",
    "# for path in tqdm(glob.glob(\"../output/val.parquet\")):\n",
    "#     df = pd.read_parquet(path)\n",
    "#     print(df['aid'].apply(len).max())\n",
    "# #     df['target'] = df['type'].apply(lambda x: [CLASSES.index(c) + 1 for c in x])\n",
    "# #     df.to_parquet(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nvt.Dataset([\"../input/parquets/train_0.parquet\"], engine=\"parquet\")\n",
    "\n",
    "CONTINUOUS_COLUMNS = ['ts']\n",
    "CATEGORICAL_COLUMNS = ['aid']\n",
    "LABEL_COLUMNS = ['target']\n",
    "\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TorchAsyncItr(\n",
    "   dataset,\n",
    "   cats=CATEGORICAL_COLUMNS,\n",
    "   conts=CONTINUOUS_COLUMNS,\n",
    "   labels=LABEL_COLUMNS,\n",
    "   batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DLDataLoader(\n",
    "   train_dataset,\n",
    "   batch_size=None,\n",
    "#    collate_fn=collate_fn,\n",
    "   pin_memory=False,\n",
    "   num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in tqdm(train_loader):\n",
    "#     batch\n",
    "#     break\n",
    "#     continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt = ['target'] >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    "# aid = ['aid'] >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    "# ts = ['ts'] >> nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    "\n",
    "\n",
    "# # Truncate\n",
    "# aid_truncated = aid >> nvt.ops.ListSlice(0, 20) >> nvt.ops.Rename(postfix = '_trim') >> TagAsItemID()\n",
    "# tgt_truncated = tgt >> nvt.ops.ListSlice(0, 20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "# ts_truncated = ts >> nvt.ops.ListSlice(0, 20) >> nvt.ops.Rename(postfix = '_trim')\n",
    "\n",
    "# # Select\n",
    "# selected_features = (\n",
    "#     aid_truncated +\n",
    "#     tgt_truncated +\n",
    "#     ts_truncated\n",
    "# )\n",
    "\n",
    "# workflow = nvt.Workflow(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = nvt.Dataset(df, cpu=False)\n",
    "# workflow.fit(dataset)\n",
    "# sessions_ds = workflow.transform(dataset)\n",
    "# sessions_gdf = sessions_ds.to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = nvt.Dataset(glob.glob(\"../output/train_*.parquet\")[:1], engine=\"parquet\", cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dataset = workflow.fit_transform(dataset)\n",
    "\n",
    "# new_dataset.to_parquet(\"../output/worflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_PATH = \"../output/schema.pb\"\n",
    "# SCHEMA_PATH = \"../output/worflow/schema.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'aid', 'value_count': {'min': '2', 'max': '512'}, 'type': 'INT', 'int_domain': {'name': 'aid', 'max': '1855610', 'is_categorical': True}, 'annotation': {'tag': ['item_id', 'list', 'categorical', 'item']}}, {'name': 'target', 'value_count': {'min': '2', 'max': '512'}, 'type': 'INT', 'int_domain': {'name': 'target', 'min': '1', 'max': '3', 'is_categorical': True}, 'annotation': {'tag': ['list', 'categorical', 'item']}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = Schema().from_proto_text(SCHEMA_PATH)\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tr.TabularSequenceFeatures.from_schema(\n",
    "        schema,\n",
    "        max_sequence_length=500,\n",
    "#         continuous_projection=64,\n",
    "        d_output=100,\n",
    "        masking=\"mlm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularSequenceFeatures(\n",
       "  (to_merge): ModuleDict(\n",
       "    (categorical_module): SequenceEmbeddingFeatures(\n",
       "      (filter_features): FilterFeatures()\n",
       "      (embedding_tables): ModuleDict(\n",
       "        (aid): Embedding(1855611, 64, padding_idx=0)\n",
       "        (target): Embedding(4, 64, padding_idx=0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_aggregation): ConcatFeatures()\n",
       "  (projection_module): SequentialBlock(\n",
       "    (0): DenseBlock(\n",
       "      (0): Linear(in_features=128, out_features=100, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (_masking): MaskedLanguageModeling()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define XLNetConfig class and set default parameters for HF XLNet config  \n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=64, n_head=4, n_layer=2, total_seq_length=500\n",
    ")\n",
    "# Define the model block including: inputs, masking, projection and transformer block.\n",
    "body = tr.SequentialBlock(\n",
    "    inputs,\n",
    "    tr.MLPBlock([64]),\n",
    "    tr.TransformerBlock(transformer_config, masking=inputs.masking)\n",
    ")\n",
    "\n",
    "# Defines the evaluation top-N metrics and the cut-offs\n",
    "metrics = [\n",
    "    RecallAt(top_ks=[20], labels_onehot=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a head related to next item prediction task \n",
    "head = tr.Head(\n",
    "    body,\n",
    "    tr.NextItemPredictionTask(weight_tying=True, hf_format=True, metrics=metrics),\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the end-to-end Model class \n",
    "model = tr.Model(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging results to /workspace/logs/2022-11-09/3/\n",
      "\n",
      "['../input/parquets/train_0.parquet', '../input/parquets/train_1.parquet', '../input/parquets/train_10.parquet', '../input/parquets/train_11.parquet', '../input/parquets/train_12.parquet', '../input/parquets/train_13.parquet', '../input/parquets/train_14.parquet', '../input/parquets/train_15.parquet', '../input/parquets/train_16.parquet', '../input/parquets/train_17.parquet', '../input/parquets/train_18.parquet', '../input/parquets/train_19.parquet', '../input/parquets/train_2.parquet', '../input/parquets/train_20.parquet', '../input/parquets/train_21.parquet', '../input/parquets/train_22.parquet', '../input/parquets/train_23.parquet', '../input/parquets/train_24.parquet', '../input/parquets/train_25.parquet', '../input/parquets/train_26.parquet', '../input/parquets/train_27.parquet', '../input/parquets/train_28.parquet', '../input/parquets/train_29.parquet', '../input/parquets/train_3.parquet', '../input/parquets/train_30.parquet', '../input/parquets/train_31.parquet', '../input/parquets/train_32.parquet', '../input/parquets/train_33.parquet', '../input/parquets/train_34.parquet', '../input/parquets/train_35.parquet', '../input/parquets/train_36.parquet', '../input/parquets/train_37.parquet', '../input/parquets/train_38.parquet', '../input/parquets/train_39.parquet', '../input/parquets/train_4.parquet', '../input/parquets/train_40.parquet', '../input/parquets/train_41.parquet', '../input/parquets/train_42.parquet', '../input/parquets/train_43.parquet', '../input/parquets/train_44.parquet', '../input/parquets/train_45.parquet', '../input/parquets/train_46.parquet', '../input/parquets/train_47.parquet', '../input/parquets/train_48.parquet', '../input/parquets/train_49.parquet', '../input/parquets/train_5.parquet', '../input/parquets/train_50.parquet', '../input/parquets/train_51.parquet', '../input/parquets/train_52.parquet', '../input/parquets/train_53.parquet', '../input/parquets/train_54.parquet', '../input/parquets/train_55.parquet', '../input/parquets/train_6.parquet', '../input/parquets/train_7.parquet', '../input/parquets/train_8.parquet', '../input/parquets/train_9.parquet']\n",
      "['../input/parquets/val.parquet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 11098496\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 86707\n",
      "Saving model checkpoint to /workspace/logs/2022-11-09/3/checkpoint-10000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "log_folder = \"/workspace/logs/\"\n",
    "\n",
    "if TRAIN:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f'Logging results to {log_folder}\\n')\n",
    "    create_logger(log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for training \n",
    "train_args = T4RecTrainingArguments(\n",
    "    data_loader_engine='nvtabular', \n",
    "    dataloader_drop_last=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=128, \n",
    "    per_device_eval_batch_size=128,\n",
    "    output_dir=log_folder, \n",
    "    learning_rate=0.0005,\n",
    "    lr_scheduler_type='cosine', \n",
    "    learning_rate_num_cosine_cycles_by_epoch=1,\n",
    "    num_train_epochs=1,\n",
    "    max_sequence_length=500, \n",
    "    report_to=[],\n",
    "    logging_steps=1000,\n",
    "    save_steps=10000,\n",
    "    no_cuda=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True,\n",
    ")\n",
    "trainer.reset_lr_scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = sorted(glob.glob(\"../input/parquets/train_*.parquet\"))\n",
    "eval_paths = sorted(glob.glob(\"../input/parquets/val.parquet\"))\n",
    "\n",
    "# train_paths = ['../input/parquets/train_0.parquet']\n",
    "# eval_paths = ['../input/parquets/train_0.parquet']\n",
    "\n",
    "trainer.train_dataset_or_path = train_paths\n",
    "trainer.eval_dataset_or_path = eval_paths\n",
    "\n",
    "print(train_paths)\n",
    "print(eval_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11245' max='86707' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11245/86707 2:41:23 < 18:03:17, 1.16 it/s, Epoch 0.13/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>12.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>10.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>10.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>9.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>8.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>8.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>8.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>7.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>7.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>7.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>7.056200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    trainer.train()\n",
    "    trainer._save_model_and_checkpoint(save_model_class=True)\n",
    "else:\n",
    "#     trainer.load_model_trainer_states_from_checkpoint('/workspace/logs/2022-11-08/6/checkpoint-86707')\n",
    "    trainer.load_model_trainer_states_from_checkpoint('/workspace/logs/2022-11-09/0/checkpoint-15620')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = trainer.evaluate(eval_dataset=eval_paths, metric_key_prefix='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(train_metrics.keys()):\n",
    "    print(\" %s = %s\" % (key, str(train_metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
