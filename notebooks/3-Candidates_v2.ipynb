{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Generates candidates.\n",
    "\n",
    "**TODO**: New chris candids ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pandarallel.initialize(nb_workers=16, progress_bar=False, use_memory_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from utils.metrics import get_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    REGEX = \"../output/val_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    REGEX = \"../output/test_parquet/*\"\n",
    "elif MODE == \"extra\":\n",
    "    REGEX = \"../output/val_trimmed_parquet/*\"\n",
    "    GT_FILE = \"../output/val_labels_trimmed.parquet\"\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_CT = 75\n",
    "ITEM_CT2 = 50\n",
    "ITEM_CT3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FOLDER = \"../output/matrices/\"\n",
    "# MATRIX_FOLDER = \"../output/matrices_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKS = False\n",
    "MULTIPLIER = 1\n",
    "\n",
    "if CLICKS:\n",
    "    SUFFIX = \"c-clicks-v3\"  # 50\n",
    "else:\n",
    "#     SUFFIX = \"c-orders-v7\"  # 50\n",
    "    SUFFIX = \"c-orders-v8\"  # 75\n",
    "#     SUFFIX = \"c-orders-v9\"  # 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.candidates_chris import (\n",
    "    load_parquets,\n",
    "    df_parallelize_run,\n",
    "    explode,\n",
    "    matrix_to_candids_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make valid lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_FOLDER = f\"../output/lists/{MODE}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = load_parquets(REGEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"extra\":  # Remove useless sessions for speed-up\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "    kept_sessions = gt[gt[\"type\"] != \"clicks\"].drop(\"ground_truth\", axis=1)\n",
    "    kept_sessions = kept_sessions.drop_duplicates(subset=\"session\", keep=\"first\")\n",
    "\n",
    "    prev_len = len(df_val)\n",
    "    df_val = (\n",
    "        df_val.merge(kept_sessions, on=\"session\", how=\"left\", suffixes=(\"\", \"_x\"))\n",
    "        .dropna(0)\n",
    "        .drop(\"type_x\", axis=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    factor = prev_len / len(df_val)\n",
    "    print(f\"Reduced dataset size by {factor:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MN = df_val.ts.min()\n",
    "# # print(MN)  # should be 1661119200 for valid\n",
    "# df_val[\"day\"] = (df_val.ts - MN) // (60 * 60 * 24)\n",
    "\n",
    "# df_val = df_val.sort_values([\"session\", \"ts\"])\n",
    "# df_val[\"x\"] = df_val.groupby([\"session\", \"d\"]).d.cumcount()\n",
    "# df_val[\"a\"] = 0\n",
    "# df_val.loc[df_val.x == 0, \"a\"] = 1\n",
    "# df_val = df_val.sort_values([\"session\", \"ts\"], ascending=[True, False])\n",
    "# df_val[\"x\"] = df_val.groupby([\"session\", \"d\"]).d.cumcount()\n",
    "# df_val[\"b\"] = 0\n",
    "# df_val.loc[df_val.x == 0, \"b\"] = 1\n",
    "# df_val = df_val.sort_values([\"session\", \"ts\"])\n",
    "# df_val = df_val.drop(\"x\", axis=1)\n",
    "# df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"../output/lists/\", exist_ok=True)\n",
    "# os.makedirs(LIST_FOLDER, exist_ok=True)\n",
    "\n",
    "# print(f\"-> Saving to {LIST_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIECES = 10\n",
    "\n",
    "# temp = [group for name, group in df_val.sort_values([\"session\", \"ts\"]).groupby([\"session\"])]\n",
    "\n",
    "# pbar = tqdm(range(PIECES))\n",
    "# for PART in pbar:\n",
    "#     pbar.set_description(\n",
    "#         f\"PART {PART + 1} - {PART * len(temp)//PIECES} -> {(PART+1) * len(temp) // PIECES}\"\n",
    "#     )\n",
    "\n",
    "#     mylist = [\n",
    "#         [\n",
    "#             h.session.iloc[0],\n",
    "#             h.aid.to_list(),\n",
    "#             h.type.to_list(),\n",
    "#             h.ts.to_list(),\n",
    "#             h.d.to_list(),\n",
    "#             h.a.to_list(),\n",
    "#             h.b.to_list(),\n",
    "#             h.day.to_list(),\n",
    "#         ]\n",
    "#         for h in temp[PART * len(temp) // PIECES : (PART + 1) * len(temp) // PIECES]\n",
    "#     ]\n",
    "\n",
    "#     with open(LIST_FOLDER + f\"group_{PART}.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(mylist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popular Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clicks = (\n",
    "    df_val.loc[df_val[\"type\"] == 0, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")\n",
    "top_carts = (\n",
    "    df_val.loc[df_val[\"type\"] == 1, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")\n",
    "top_orders = (\n",
    "    df_val.loc[df_val[\"type\"] == 2, \"aid\"].value_counts().index.values[:100].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_ = \"val\" if MODE == \"extra\" else MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_buy2buy = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-90_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_buy2buy2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-99_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_orders = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_cpu-95_{MODE_}.pqt\")\n",
    ")\n",
    "top_20_carts = top_20_orders\n",
    "\n",
    "top_20_test = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-116_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_test2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-115_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-93_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20b = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-217_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20c = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-220_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20d = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-226_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20e = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-232_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20f = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-235_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_buy = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-239_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_new = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-700_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_20_new2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-701_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_day = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-155_{MODE_}.pqt\")\n",
    ")\n",
    "\n",
    "top_40_day2 = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_gpu-157_{MODE_}.pqt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "\n",
    "def suggest_aids(df):\n",
    "    # aids=df.aid.tolist()\n",
    "    # types = df.type.tolist()\n",
    "    session = df[0]\n",
    "    aids = df[1]\n",
    "    types = df[2]\n",
    "    ds = df[4]\n",
    "    ds2 = df[6]\n",
    "\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # df2 = df.sort_values('ts',ascending=False).drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids3 = list(dict.fromkeys(aids2[::-1] )) #last of each session\n",
    "    unique_aids3 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds2[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # mx = df.d.max()\n",
    "    # aids2 = df.loc[df.d==mx].aid.tolist()\n",
    "    # unique_aids4 = list(dict.fromkeys(aids2[::-1] ))\n",
    "    mx = np.max(ds)\n",
    "    unique_aids4 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds[i] == mx][::-1])\n",
    "    )\n",
    "\n",
    "    # df = df.loc[ df['type'].isin([1,2]) ]\n",
    "    # unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    unique_buys = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1])\n",
    "    )\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    if len(unique_aids) >= 15:\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][:20] for aid in unique_aids[:2] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:15] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.3\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_test2[aid][:20]\n",
    "                    for aid in unique_aids[:2]\n",
    "                    if aid in top_20_test2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT2) if k not in unique_aids]\n",
    "\n",
    "        if len(result) < 1:\n",
    "            result += top_clicks[:1]\n",
    "        return session, result[:ITEM_CT2]\n",
    "    #         return session, (result + top_clicks[: ITEM_CT2 - len(result)])[:ITEM_CT2]\n",
    "\n",
    "    aids_temp = Counter()\n",
    "\n",
    "    weights3 = [2, 2] + [1] * 28\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:30]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 30)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    aids2 = list(\n",
    "        itertools.chain(*[top_20[aid][:20] for aid in unique_aids if aid in top_20])\n",
    "    )\n",
    "    for i, aid in enumerate(aids2):\n",
    "        m = 0.1 + 0.9 * (ln - (i // (20))) / ln\n",
    "        aids_temp[aid] += m\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += m\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20b[aid][:20] for aid in unique_aids[:2] if aid in top_20b]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test2[aid][:20] for aid in unique_aids[:2] if aid in top_20_test2]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    aids4 = list(\n",
    "        itertools.chain(*[top_20f[aid][:10] for aid in unique_aids4 if aid in top_20f])\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // (10)\n",
    "        aids_temp[aid] += 1 - w * 0.1\n",
    "        if i % (10) == 0:\n",
    "            aids_temp[aid] += 1 - w * 0.1\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:20] for aid in unique_aids3 if aid in top_20e])\n",
    "    )\n",
    "    for i, aid in enumerate(aids5):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(1) if k not in unique_aids]\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(*[top_20c[aid][:10] for aid in top_aids2 if aid in top_20c])\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (10) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(ITEM_CT2) if k not in unique_aids]\n",
    "    result = top_aids2\n",
    "\n",
    "    if len(result) < 1:\n",
    "        result += top_clicks[:1]\n",
    "    return session, result[:ITEM_CT2]\n",
    "\n",
    "\n",
    "#     return session, (result + top_clicks[: ITEM_CT2 - len(result)])[:ITEM_CT2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df):\n",
    "    # aids=df.aid.tolist()\n",
    "    # types = df.type.tolist()\n",
    "\n",
    "    session = df[0]\n",
    "    aids = df[1]\n",
    "    types = df[2]\n",
    "    tss = df[3]\n",
    "    ds = df[4]\n",
    "    ds2 = df[6]\n",
    "    # days = df[7]\n",
    "\n",
    "    top_day = top_40_day2\n",
    "    click_aids = click_df[session][:ITEM_CT3]\n",
    "\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # df2 = df.sort_values('ts',ascending=False).drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids3 = list(dict.fromkeys(aids2[::-1] )) #last of each session\n",
    "    unique_aids3 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds2[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # mx = df.d.max()\n",
    "    # aids2 = df.loc[df.d==mx].aid.tolist()\n",
    "    # unique_aids4 = list(dict.fromkeys(aids2[::-1] ))\n",
    "    mx = np.max(ds)\n",
    "    unique_aids4 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds[i] == mx][::-1])\n",
    "    )\n",
    "\n",
    "    # aids2 = df.loc[df.ts >= mx - 60*60*24].aid.tolist()\n",
    "    # unique_aids6 = list(dict.fromkeys(aids2[::-1] )) #recent 1 day\n",
    "    mx = np.max(tss)\n",
    "    unique_aids6 = list(\n",
    "        dict.fromkeys(\n",
    "            [f for i, f in enumerate(aids) if tss[i] >= mx - 60 * 60 * 24][::-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # df = df.loc[ df['type'].isin([1,2]) ]\n",
    "    # unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    unique_buys = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1])\n",
    "    )\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    if len(unique_aids) >= 15:\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][: 20 * 2] for aid in unique_aids[:2] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][: 15 * 2] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.3\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_test2[aid][: 20 * 2]\n",
    "                    for aid in unique_aids[:2]\n",
    "                    if aid in top_20_test2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.6\n",
    "\n",
    "        # aids3 = list(itertools.chain(*[top_20[aid][:10] for aid in click_aids[:5] if aid in top_20]))\n",
    "        # for i,aid in enumerate(aids3):\n",
    "        #    aids_temp[aid] += 0.3\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT)]\n",
    "        return session, (result + top_clicks[: ITEM_CT - len(result)])[:ITEM_CT]\n",
    "        # return sorted_aids\n",
    "\n",
    "    aids_temp = Counter()\n",
    "\n",
    "    # NEW\n",
    "    MM = 4\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[top_day[aid][: 10 * MM] for aid in unique_aids6 if aid in top_day]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids2):\n",
    "        aids_temp[aid] += 1\n",
    "\n",
    "    # NEW NEW\n",
    "    # ln0 = len(click_aids)\n",
    "    aids2 = list(\n",
    "        itertools.chain(*[top_20[aid][:20] for aid in click_aids if aid in top_20])\n",
    "    )\n",
    "    for i, aid in enumerate(aids2):\n",
    "        aids_temp[aid] += 0.5\n",
    "        # if i%20==0: aids_temp[aid] += 0.5\n",
    "\n",
    "    weights3 = [2, 2] + [1] * 28\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:30]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 30)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    # aids2 = list(itertools.chain(*[top_20[aid][:20*2] for aid in unique_aids if aid in top_20]))\n",
    "    # for i,aid in enumerate(aids2):\n",
    "    #    m = 0.1 + 0.9*(ln-(i//(20*2)))/ln\n",
    "    #    aids_temp[aid] += m\n",
    "    #    if i%(20*2)==0: aids_temp[aid] += m\n",
    "\n",
    "    # FROM GIBA\n",
    "    for i, a in enumerate(unique_aids):\n",
    "        w0 = np.max(\n",
    "            [1 - (0.35 * i), 0.001]\n",
    "        )  # Weight aid order starting from the last one.\n",
    "        if a in top_20:\n",
    "            for j, aj in enumerate(top_20[a]):\n",
    "                w1 = np.max(\n",
    "                    [1 - (0.005 * j), 0.01]\n",
    "                )  # Weight the candidate aid from the dict\n",
    "                aids_temp[aj] += w0 * w1\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20b[aid][: 20 * 2] for aid in unique_aids[:2] if aid in top_20b]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_test2[aid][: 20 * 2]\n",
    "                for aid in unique_aids[:2]\n",
    "                if aid in top_20_test2\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "\n",
    "    # TRY GIBA HERE\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20f[aid][: 10 * 2] for aid in unique_aids4 if aid in top_20f]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // (10 * 2)\n",
    "        aids_temp[aid] += 1 - w * 0.1\n",
    "        if i % (10 * 2) == 0:\n",
    "            aids_temp[aid] += 1 - w * 0.1\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20e[aid][: 20 * 2] for aid in unique_aids3 if aid in top_20e]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids5):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (20 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(1) if k not in unique_aids]\n",
    "\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20c[aid][: 10 * 2] for aid in top_aids2 if aid in top_20c]\n",
    "        )\n",
    "    )\n",
    "    for i, aid in enumerate(aids3):\n",
    "        aids_temp[aid] += 1\n",
    "        if i % (10 * 2) == 0:\n",
    "            aids_temp[aid] += 1\n",
    "    top_aids2 = [k for k, v in aids_temp.most_common(ITEM_CT) if k not in unique_aids]\n",
    "\n",
    "    result = unique_aids + top_aids2[: ITEM_CT - len(unique_aids)]\n",
    "    return session, (result + top_clicks[: ITEM_CT - len(result)])[:ITEM_CT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_orders(df):\n",
    "    session = df[0]\n",
    "    aids = df[1]\n",
    "    types = df[2]\n",
    "    tss = df[3]\n",
    "    ds = df[4]\n",
    "    ds1 = df[5]\n",
    "    ds2 = df[6]\n",
    "    days = df[7]\n",
    "\n",
    "    # top_day = top_40_day[ df.day.values[0] ]\n",
    "    top_day = top_40_day\n",
    "    # click_aids = click_df[df.session.values[0]][:ITEM_CT2]\n",
    "    click_aids = click_df[session][:ITEM_CT3]\n",
    "\n",
    "    # aids = df.aid.tolist()\n",
    "    # types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "\n",
    "    # mx = df.d.max()\n",
    "    # aids2 = df.loc[df.d==mx].aid.tolist()\n",
    "    # unique_aids4 = list(dict.fromkeys(aids2[::-1] )) # last session\n",
    "    mx = np.max(ds)\n",
    "    unique_aids4 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds[i] == mx][::-1])\n",
    "    )\n",
    "\n",
    "    # mx = df.ts.max()\n",
    "    # aids2 = df.loc[df.ts >= mx - 60*60/2].aid.tolist()\n",
    "    # unique_aids5 = list(dict.fromkeys(aids2[::-1] )) #recent 1 hour\n",
    "    mx = np.max(tss)\n",
    "    unique_aids5 = list(\n",
    "        dict.fromkeys(\n",
    "            [f for i, f in enumerate(aids) if tss[i] >= mx - 60 * 60 / 2][::-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # aids2 = df.loc[df.ts >= mx - 60*60*24].aid.tolist()\n",
    "    # unique_aids6 = list(dict.fromkeys(aids2[::-1] )) #recent 1 day\n",
    "    unique_aids6 = list(\n",
    "        dict.fromkeys(\n",
    "            [f for i, f in enumerate(aids) if tss[i] >= mx - 60 * 60 * 24][::-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # df2 = df.drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids2 = list(dict.fromkeys(aids2[::-1] )) #first of each session\n",
    "    unique_aids2 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds1[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # df2 = df.sort_values('ts',ascending=False).drop_duplicates('d')\n",
    "    # aids2 = df2.aid.tolist()\n",
    "    # unique_aids3 = list(dict.fromkeys(aids2 )) #last of each session\n",
    "    unique_aids3 = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if ds2[i] == 1][::-1])\n",
    "    )\n",
    "\n",
    "    # df = df.loc[ df['type'].isin([1,2]) ]\n",
    "    # unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    unique_buys = list(\n",
    "        dict.fromkeys([f for i, f in enumerate(aids) if types[i] in [1, 2]][::-1])\n",
    "    )\n",
    "\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        for aid in unique_aids2:\n",
    "            aids_temp[aid] += 0.5\n",
    "        for aid in unique_aids3:\n",
    "            aids_temp[aid] += 0.5\n",
    "\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_buy2buy[aid][:40]\n",
    "                    for aid in unique_buys\n",
    "                    if aid in top_20_buy2buy\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids3 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_buy2buy2[aid][:40]\n",
    "                    for aid in unique_buys\n",
    "                    if aid in top_20_buy2buy2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids3):\n",
    "            aids_temp[aid] += 0.1\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.1\n",
    "\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_test[aid][:40] for aid in unique_aids if aid in top_20_test]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20c[aid][:20] for aid in unique_aids[:1] if aid in top_20c]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids5):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids6 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20d[aid][:20] for aid in unique_buys[:1] if aid in top_20d]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids6):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "\n",
    "        aids7 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:5] for aid in unique_aids3 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids7):\n",
    "            aids_temp[aid] += 0.25\n",
    "            if i % 5 == 0:\n",
    "                aids_temp[aid] += 0.25\n",
    "        aids7 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20b[aid][:5] for aid in unique_aids2 if aid in top_20b]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids7):\n",
    "            aids_temp[aid] += 0.125\n",
    "            if i % 5 == 0:\n",
    "                aids_temp[aid] += 0.125\n",
    "\n",
    "        # NEW STUFF\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_day[aid][:40] for aid in unique_aids6 if aid in top_day]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 40 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_test[aid][:20] for aid in click_aids if aid in top_20_test]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        aids4 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_buy[aid][:20] for aid in click_aids if aid in top_20_buy]\n",
    "            )\n",
    "        )\n",
    "        for i, aid in enumerate(aids4):\n",
    "            aids_temp[aid] += 0.05\n",
    "            if i % 20 == 0:\n",
    "                aids_temp[aid] += 0.05\n",
    "        for aid in click_aids:\n",
    "            aids_temp[aid] += 0.05\n",
    "\n",
    "        result = [k for k, v in aids_temp.most_common(ITEM_CT)]\n",
    "\n",
    "        if MODE != \"test\":\n",
    "            if len(result) < 1:\n",
    "                result += top_orders[:1]\n",
    "            return session, result[:ITEM_CT]\n",
    "        else:\n",
    "            return session, (result + top_orders[: ITEM_CT - len(result)])[:ITEM_CT]\n",
    "\n",
    "    #         return session, (result + top_orders[: ITEM_CT - len(result)])[:ITEM_CT]\n",
    "\n",
    "    weights = [2, 2] + [1] * 8  # + [0]*30\n",
    "    weights2 = [2, 2] + [1] * 53  # + [0]*25\n",
    "    weights3 = [2, 2] + [1] * 18  # + [0]*70\n",
    "    weights4 = [2, 2] + [1] * 38  # + [0]*70\n",
    "    weights5 = [2, 2] + [1] * 28  # + [0]*70\n",
    "\n",
    "    ln = len(unique_aids)\n",
    "\n",
    "    MM = 3\n",
    "    aids_temp = Counter()\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_orders[aid][: 10 * MM]\n",
    "                for aid in unique_aids\n",
    "                if aid in top_20_orders\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w2 = weights5 * int(len(aids2) // (10 * MM))\n",
    "    aids3 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_buy2buy[aid][: 10 * MM]\n",
    "                for aid in unique_buys\n",
    "                if aid in top_20_buy2buy\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w3 = weights5 * int(len(aids3) // (10 * MM))\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test[aid][: 10 * MM] for aid in unique_aids if aid in top_20_test]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights5 * int(len(aids4) // (10 * MM))\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[\n",
    "                top_20_buy2buy2[aid][: 10 * MM]\n",
    "                for aid in unique_buys\n",
    "                if aid in top_20_buy2buy2\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights5 * int(len(aids5) // (10 * MM))\n",
    "    for i, (aid, w) in enumerate(zip(aids2, w2)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // (10 * MM))) / ln\n",
    "        aids_temp[aid] += w * m\n",
    "    for i, (aid, w) in enumerate(zip(aids3, w3)):\n",
    "        aids_temp[aid] += w / 2\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // (10 * MM))) / ln\n",
    "        aids_temp[aid] += w * m\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        aids_temp[aid] += w / 2\n",
    "\n",
    "    # NEW\n",
    "    MM = 4\n",
    "    aids2 = list(\n",
    "        itertools.chain(\n",
    "            *[top_day[aid][: 10 * MM] for aid in unique_aids6 if aid in top_day]\n",
    "        )\n",
    "    )\n",
    "    w2 = weights4 * int(len(aids2) // (10 * MM))\n",
    "    for i, (aid, w) in enumerate(zip(aids2, w2)):\n",
    "        m = 0.25 + 0.75 * (ln - (i // (10 * MM))) / ln\n",
    "        aids_temp[aid] += 1  # w*m\n",
    "\n",
    "    # NEW\n",
    "    ln0 = len(click_aids)\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_test[aid][:20] for aid in click_aids if aid in top_20_test]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights3 * int(len(aids4) // (20))\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln0 - (i // (20))) / ln0\n",
    "        aids_temp[aid] += w * m\n",
    "    aids4 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_buy[aid][:20] for aid in click_aids if aid in top_20_buy]\n",
    "        )\n",
    "    )\n",
    "    w4 = weights3 * int(len(aids4) // (20))\n",
    "    for i, (aid, w) in enumerate(zip(aids4, w4)):\n",
    "        m = 0.25 + 0.75 * (ln0 - (i // (20))) / ln0\n",
    "        aids_temp[aid] += w * m\n",
    "    for aid in click_aids:\n",
    "        aids_temp[aid] += 1\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20c[aid][:55] for aid in unique_aids[:1] if aid in top_20c]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights2 * int(len(aids5) // 55)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += w\n",
    "\n",
    "    if len(unique_aids) == 1:\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[\n",
    "                    top_20_new2[aid][:20]\n",
    "                    for aid in unique_aids[-1:]\n",
    "                    if aid in top_20_new2\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 20)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "        aids5 = list(\n",
    "            itertools.chain(\n",
    "                *[top_20_new[aid][:20] for aid in unique_aids[-1:] if aid in top_20_new]\n",
    "            )\n",
    "        )\n",
    "        w5 = weights3 * int(len(aids5) // 20)\n",
    "        for aid, w in zip(aids5, w5):\n",
    "            aids_temp[aid] += w\n",
    "\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20d[aid][:20] for aid in unique_buys[:1] if aid in top_20d]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights3 * int(len(aids5) // 20)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += w\n",
    "\n",
    "    ln2 = len(unique_aids5)\n",
    "    aids5 = list(\n",
    "        itertools.chain(\n",
    "            *[top_20_buy[aid][:20] for aid in unique_aids5 if aid in top_20_buy]\n",
    "        )\n",
    "    )\n",
    "    w5 = weights3 * int(len(aids5) // 20)\n",
    "    for aid, w in zip(aids5, w5):\n",
    "        aids_temp[aid] += 2 * w / ln2\n",
    "\n",
    "    aids4 = list(\n",
    "        itertools.chain(*[top_20f[aid][:5] for aid in unique_aids4 if aid in top_20f])\n",
    "    )\n",
    "    for i, aid in enumerate(aids4):\n",
    "        w = i // 5\n",
    "        aids_temp[aid] += 1 / 2 - w * 0.05\n",
    "        if i % 5 == 0:\n",
    "            aids_temp[aid] += 1 / 2 - w * 0.05\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:55] for aid in unique_aids3 if aid in top_20e])\n",
    "    )\n",
    "    w5 = weights2 * int(len(aids5) // 55)\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        w2 = i // 55\n",
    "        aids_temp[aid] += w - w2 * 0.1\n",
    "    aids5 = list(\n",
    "        itertools.chain(*[top_20e[aid][:10] for aid in unique_aids2 if aid in top_20e])\n",
    "    )\n",
    "    w5 = weights * int(len(aids5) // 10)\n",
    "    for i, (aid, w) in enumerate(zip(aids5, w5)):\n",
    "        w2 = i // 10\n",
    "        aids_temp[aid] += w / 2.0 - w2 * 0.05\n",
    "\n",
    "    sorted_aids = [k for k, v in aids_temp.most_common(ITEM_CT) if k not in unique_aids]\n",
    "    result = unique_aids + sorted_aids[: ITEM_CT - len(unique_aids)]\n",
    "\n",
    "    if MODE != \"test\":\n",
    "        if len(result) < 1:\n",
    "            result += top_orders[:1]\n",
    "        return session, result[:ITEM_CT]\n",
    "    else:\n",
    "        return session, (result + top_orders[: ITEM_CT - len(result)])[:ITEM_CT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "PIECES = 10\n",
    "\n",
    "valid_bysession_list = []\n",
    "for PART in range(PIECES):\n",
    "    with open(LIST_FOLDER + f\"group_{PART}.pkl\", 'rb') as f:\n",
    "        valid_bysession_list.extend(pickle.load(f))\n",
    "    \n",
    "print(len(valid_bysession_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.exists(f\"../output/candidates/clicks_fast_{MODE}.parquet\"):\n",
    "    preds = df_parallelize_run(suggest_aids, valid_bysession_list)\n",
    "    \n",
    "    pred_df = cudf.DataFrame(\n",
    "        cudf.Series([f[1] for f in preds], index=[f[0] for f in preds])\n",
    "    ).reset_index()\n",
    "    pred_df.columns = [\"session\", \"candidates\"]\n",
    "\n",
    "    pred_df.to_parquet(f\"../output/candidates/clicks_fast_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_df = (\n",
    "    cudf.read_parquet(f\"../output/candidates/clicks_fast_{MODE}.parquet\")\n",
    "    .set_index(\"session\")[\"candidates\"]\n",
    "    .to_pandas()\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = df_parallelize_run(suggest_clicks, valid_bysession_list)\n",
    "\n",
    "# pred_df = cudf.DataFrame(\n",
    "#     cudf.Series([f[1] for f in preds], index=[f[0] for f in preds])\n",
    "# ).reset_index()\n",
    "# pred_df.columns = [\"session\", \"candidates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = df_parallelize_run(suggest_orders, valid_bysession_list)\n",
    "\n",
    "pred_df = pd.Series([f[1] for f in preds], index=[f[0] for f in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(\n",
    "    pred_df.add_suffix(\"_clicks\"), columns=[\"labels\"]\n",
    ").reset_index()\n",
    "orders_pred_df = pd.DataFrame(\n",
    "    pred_df.add_suffix(\"_orders\"), columns=[\"labels\"]\n",
    ").reset_index()\n",
    "carts_pred_df = pd.DataFrame(\n",
    "    pred_df.add_suffix(\"_carts\"), columns=[\"labels\"]\n",
    ").reset_index()\n",
    "\n",
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df], ignore_index=True)\n",
    "pred_df.columns = [\"session_type\", \"labels_l\"]\n",
    "pred_df[\"labels\"] = pred_df[\"labels_l\"].apply(lambda x: \" \".join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "\n",
    "    recs = []\n",
    "    df_pred = pred_df[[\"session_type\", \"labels_l\"]].copy()\n",
    "    df_pred.columns = [\"session_type\", \"candidates\"]\n",
    "    df_pred[\"session\"] = (\n",
    "        df_pred[\"session_type\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "    )\n",
    "    df_pred[\"type\"] = df_pred[\"session_type\"].apply(lambda x: x.split(\"_\")[1])\n",
    "\n",
    "    df_pred = df_pred.merge(gt, on=[\"session\", \"type\"], how=\"left\")\n",
    "\n",
    "    for col in CLASSES:\n",
    "        df_pred_c = df_pred[df_pred[\"type\"] == col]\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df_pred_c[\"candidates\"].values, df_pred_c[\"ground_truth\"].values\n",
    "        )\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "\n",
    "        recs.append(n_found / n_gts)\n",
    "\n",
    "    cv = np.average(recs, weights=WEIGHTS)\n",
    "    print(f\"\\n-> CV : {cv:.4f}\")\n",
    "\n",
    "    del clicks_pred_df, orders_pred_df, carts_pred_df, df_pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 75\n",
    " - clicks \t- Found 1.11M GTs with 134.98M candidates (pos_prop=0.82%)\t-  Highest reachable Recall : 0.6312\n",
    " - carts \t- Found 293.95K GTs with 134.98M candidates (pos_prop=0.22%)\t-  Highest reachable Recall : 0.5103\n",
    " - orders \t- Found 222.1K GTs with 134.98M candidates (pos_prop=0.16%)\t-  Highest reachable Recall : 0.7090\n",
    "- 50\n",
    " - clicks \t- Found 1.05M GTs with 90.01M candidates (pos_prop=1.17%)\t-  Highest reachable Recall : 0.5999\n",
    " - carts \t- Found 279.92K GTs with 90.01M candidates (pos_prop=0.31%)\t-  Highest reachable Recall : 0.4859\n",
    " - orders \t- Found 217.58K GTs with 90.01M candidates (pos_prop=0.24%)\t-  Highest reachable Recall : 0.6946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candids = pred_df[[\"session_type\", \"labels_l\"]].copy()\n",
    "df_candids.columns = [\"session\", \"candidates\"]\n",
    "df_candids[\"session\"] = (\n",
    "    df_candids[\"session\"].apply(lambda x: x.split(\"_\")[0]).astype(\"int32\")\n",
    ")\n",
    "df_candids = df_candids.drop_duplicates(keep=\"first\", subset=\"session\").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "df_candids = df_candids.sort_values(\"session\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "    gt[\"ground_truth\"] = gt[\"ground_truth\"].apply(lambda x: x.tolist())\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df_candids.columns:\n",
    "            df_candids = df_candids.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candids = explode(df_candids, test=(MODE == \"test\"))\n",
    "\n",
    "df_candids.to_parquet(\n",
    "    f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False\n",
    ")\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theo's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.candidates import (\n",
    "    load_parquets,\n",
    "    create_candidates,\n",
    "    explode,\n",
    "    matrix_to_candids_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"extra\"\n",
    "SUFFIX = \"v5\"  # 6 for new matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MATRIX = 20\n",
    "MAX_COOC = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    PARQUET_FILES = \"../output/val_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    PARQUET_FILES = \"../output/test_parquet/*\"\n",
    "elif MODE == \"extra\":\n",
    "    PARQUET_FILES =  \"../output/val_trimmed_parquet/*\"\n",
    "    GT_FILE = \"../output/val_labels_trimmed.parquet\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "MATRIX_FOLDER = \"../output/matrices/\"\n",
    "# MATRIX_FOLDER = \"../output/matrices_2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_parquets(PARQUET_FILES)\n",
    "df = df.sort_values([\"session\", \"ts\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"extra\":  # Remove useless sessions for speed-up\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "    kept_sessions = gt[gt['type'] != \"clicks\"].drop('ground_truth', axis=1)\n",
    "    kept_sessions = kept_sessions.drop_duplicates(subset=\"session\", keep=\"first\")\n",
    "\n",
    "    prev_len = len(df)\n",
    "    df = df.merge(\n",
    "        kept_sessions, on=\"session\", how=\"left\", suffixes=('', '_x')\n",
    "    ).dropna(0).drop('type_x', axis=1).reset_index(drop=True)\n",
    "\n",
    "    factor = prev_len / len(df)\n",
    "    print(f'Reduced dataset size by {factor:.1f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE_ = \"val\" if MODE == \"extra\" else MODE\n",
    "    \n",
    "clicks_candids = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(MATRIX_FOLDER + f\"matrix_123_temporal_{N_MATRIX}_{MODE_}.pqt\")\n",
    ")\n",
    "type_weighted_candids = matrix_to_candids_dict(\n",
    "    cudf.read_parquet(\n",
    "        MATRIX_FOLDER + f\"matrix_123_type0.590.5_{N_MATRIX}_{MODE_}.pqt\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = create_candidates(df, clicks_candids, type_weighted_candids, max_cooc=MAX_COOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    recalls = []\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df.columns:\n",
    "            df = df.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df[\"candidates\"].values, df[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)\n",
    "\n",
    "    cv = np.average(recalls, weights=WEIGHTS)\n",
    "    print(f\"\\n-> Highest reachable CV : {cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clicks \t- Found 1.02M GTs with 89.04M candidates (pos_prop=1.14%)\t-  Highest reachable Recall : 0.5806\n",
    "- carts \t- Found 277.39K GTs with 89.04M candidates (pos_prop=0.31%)\t-  Highest reachable Recall : 0.4816\n",
    "- orders \t- Found 217.5K GTs with 89.04M candidates (pos_prop=0.24%)\t-  Highest reachable Recall : 0.6944\n",
    "\n",
    "-> Highest reachable CV : 0.619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode & saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = explode(df, test=(MODE == \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False)\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blend Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"extra\"  # \"test\"  \"extra\"\n",
    "CLICKS = False\n",
    "\n",
    "SUFFIX = \"cv9-tv5\"\n",
    "\n",
    "if CLICKS:\n",
    "    SUFFIX = \"clicks_\" + SUFFIX\n",
    "\n",
    "if MODE == \"extra\":\n",
    "    GT_FILE = \"../output/val_labels_trimmed.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLICKS:\n",
    "    chris_candids = cudf.read_parquet(\n",
    "        f\"../output/candidates/candidates_c-clicks-v3_{MODE}.parquet\"  # TODO : v7\n",
    "    )\n",
    "else:\n",
    "    chris_candids = cudf.read_parquet(\n",
    "        f\"../output/candidates/candidates_c-orders-v9_{MODE}.parquet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theo_candids = cudf.read_parquet(f\"../output/candidates/candidates_v5_{MODE}.parquet\")\n",
    "# theo_candids = cudf.read_parquet(f\"../output/candidates/candidates_v6_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candids = (\n",
    "    cudf.concat([chris_candids, theo_candids])\n",
    "    .drop_duplicates(keep=\"first\", subset=[\"session\", \"candidates\"])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candids.to_parquet(\n",
    "    f\"../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\", index=False\n",
    ")\n",
    "print(f\"Saved to ../output/candidates/candidates_{SUFFIX}_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    df = candids[[\"session\", \"candidates\"]].groupby(\"session\").agg(list)\n",
    "    df = df.reset_index().to_pandas()\n",
    "\n",
    "    GT_FILE = '../output/val_labels.parquet' if MODE == \"val\" else '../output/val_labels_trimmed.parquet'\n",
    "    gt = pd.read_parquet(GT_FILE)\n",
    "\n",
    "    recalls = []\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df.columns:\n",
    "            df = df.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df[\"candidates\"].values, df[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v9 + Theo v5**\n",
    "- clicks \t- Found 1.16M GTs with 206.7M candidates (pos_prop=0.56%)\t-  Highest reachable Recall : 0.6590\n",
    "- carts \t- Found 309.85K GTs with 206.7M candidates (pos_prop=0.15%)\t-  Highest reachable Recall : 0.5379\n",
    "- orders \t- Found 227.4K GTs with 206.7M candidates (pos_prop=0.11%)\t-  Highest reachable Recall : 0.7260"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v8 + Theo v5**\n",
    "- clicks \t- Found 1.13M GTs with 167.12M candidates (pos_prop=0.68%)\t-  Highest reachable Recall : 0.6443\n",
    "- carts \t- Found 303.47K GTs with 167.12M candidates (pos_prop=0.18%)\t-  Highest reachable Recall : 0.5268\n",
    "- orders \t- Found 225.46K GTs with 167.12M candidates (pos_prop=0.13%)\t-  Highest reachable Recall : 0.7198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v7 + Theo v5**\n",
    "- clicks \t- Found 1.09M GTs with 129.87M candidates (pos_prop=0.84%)\t-  Highest reachable Recall : 0.6224\n",
    "- carts \t- Found 295.05K GTs with 129.87M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : **0.5122**\n",
    "- orders \t- Found 222.88K GTs with 129.87M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : **0.7115**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chris v6 + Theo v6** (leaky)\n",
    "- clicks \t- Found 1.12M GTs with 130.33M candidates (pos_prop=0.86%)\t-  Highest reachable Recall : 0.6391\n",
    "- carts \t- Found 296.12K GTs with 130.33M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.5141\n",
    "- orders \t- Found 223.13K GTs with 130.33M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : 0.7123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clicks Chris v3 + Theo v5**\n",
    "- clicks \t- Found 1.12M GTs with 129M candidates (pos_prop=0.87%)\t-  Highest reachable Recall : **0.6361**\n",
    "- carts \t- Found 293.98K GTs with 129M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : **0.5104**\n",
    "- orders \t- Found 222.29K GTs with 129M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : **0.7097**\n",
    "\n",
    "**Chris v3 + Theo v5**\n",
    "- clicks\t- Found 1.11M GTs with 128.42M candidates (pos_prop=0.86%)\t-  Highest reachable Recall : 0.6298\n",
    "- carts\t- Found 292.58K GTs with 128.42M candidates (pos_prop=0.23%)\t-  Highest reachable Recall : 0.5079\n",
    "- orders\t- Found 222.05K GTs with 128.42M candidates (pos_prop=0.17%)\t-  Highest reachable Recall : 0.7089\n",
    "\n",
    "**Chris v4 + Theo v5**\n",
    "- clicks \t- Found 1.14M GTs with 161.5M candidates (pos_prop=0.70%)\t-  Highest reachable Recall : 0.6471\n",
    "- carts \t- Found 299.54K GTs with 161.5M candidates (pos_prop=0.19%)\t-  Highest reachable Recall : 0.5200\n",
    "- orders \t- Found 224.13K GTs with 161.5M candidates (pos_prop=0.14%)\t-  Highest reachable Recall : 0.7155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
