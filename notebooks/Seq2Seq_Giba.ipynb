{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Generates candidates.\n",
    "\n",
    "**TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "from merlin.io import Dataset\n",
    "from torch.optim import SparseAdam\n",
    "from merlin.loader.torch import Loader\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.load import load_sessions\n",
    "from utils.metrics import get_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    files = glob.glob(\"../output/full_train_parquet/*\") + glob.glob(\n",
    "        \"../output/val_parquet/*\"\n",
    "    )\n",
    "elif MODE == \"test\":\n",
    "    files = glob.glob(\"../output/full_train_val_parquet/*\") + glob.glob(\n",
    "        \"../output/test_parquet/*\"\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"../output/matrix_factorization/train-proc-1_{MODE}.parquet\"):\n",
    "    train = load_sessions(files)\n",
    "\n",
    "    train = train.sort_values([\"session\", \"ts\"], ascending=[True, True]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    train[f\"ts_diff\"] = (\n",
    "        train.groupby(\"session\")[\"ts\"].shift(1).fillna(0).astype(\"int32\")\n",
    "    )\n",
    "    train[f\"ts_diff\"] = train[f\"ts\"] - train[f\"ts_diff\"]\n",
    "    train[f\"ts_diff\"] = train[f\"ts_diff\"].clip(0, 24 * 60 * 60)\n",
    "\n",
    "    train.loc[train.ts_diff < 2 * 60 * 60, f\"ts_diff\"] = 0\n",
    "    train.loc[train.ts_diff >= 2 * 60 * 60, f\"ts_diff\"] = 1\n",
    "\n",
    "    train[f\"subses\"] = train.groupby(\"session\")[\"ts_diff\"].cumsum()\n",
    "    train[f\"subses\"] = (train[f\"session\"] * 128 + train[f\"subses\"]).factorize()[0]\n",
    "\n",
    "    for lag in range(11):\n",
    "        train[f\"lag{lag}\"] = (\n",
    "            train.groupby(\"subses\")[\"aid\"].shift(lag).fillna(-1).astype(\"int32\")\n",
    "        )\n",
    "\n",
    "    del train[\"ts_diff\"], train[\"aid\"]\n",
    "\n",
    "    train = train.loc[(train[\"type\"] == 0) & (train[\"lag1\"] >= 0)].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    del train[\"type\"]\n",
    "\n",
    "    train[\"hour\"] = ((train[\"ts\"] - train[\"ts\"].min()) // (8 * 60 * 60)).astype(\"int8\")\n",
    "    del train[\"ts\"], train[\"subses\"]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    train.tail()\n",
    "\n",
    "    gc.collect()\n",
    "    train.to_pandas().to_parquet(\n",
    "        f\"../output/matrix_factorization/train-proc-1_{MODE}.parquet\"\n",
    "    )\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "- TODO : Cart -> Buy / Buy -> Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_aids=1855602 + 1, n_factors=32):\n",
    "        super().__init__()\n",
    "        self.aid_emb = nn.Embedding(1855602 + 128, n_factors, sparse=False)\n",
    "        self.aid_emb.weight.data.normal_(mean=0.0, std=0.001)\n",
    "        self.head = nn.Linear(11, 1)\n",
    "\n",
    "    def forward(self, lags, targets):\n",
    "        targets = torch.repeat_interleave(targets.view(-1, 1), lags.shape[1], dim=1)\n",
    "        tgt = self.aid_emb(targets)\n",
    "        tgt = torch.nn.functional.normalize(tgt, p=2.0, dim=2, eps=1e-12)\n",
    "\n",
    "        e1 = self.aid_emb(lags)\n",
    "        e1 = torch.nn.functional.normalize(e1, p=2.0, dim=2, eps=1e-12)\n",
    "        e1 = e1 * tgt\n",
    "        e1 = e1.sum(2)\n",
    "\n",
    "        out = self.head(e1)\n",
    "        return out.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, iterator, optimizer, clip, device=\"cuda\"):\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    gc.collect()\n",
    "    with tqdm(enumerate(iterator), total=len(iterator), miniters=100) as pbar:\n",
    "        for i, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_pos = model(data, target[:, 0])\n",
    "\n",
    "            # Shuffle target to build negative samples\n",
    "            target = target[torch.randperm(target.shape[0])]\n",
    "            output_neg = model(data, target[:, 0])\n",
    "\n",
    "            outputs = torch.cat([output_pos, output_neg])\n",
    "            targets = torch.cat(\n",
    "                [torch.ones_like(output_pos), torch.zeros_like(output_neg)]\n",
    "            )\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            del data, target\n",
    "\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            cumloss = epoch_loss / (i + 1)\n",
    "            pbar.set_description(f\"Loss {cumloss:.5f}\")\n",
    "\n",
    "    gc.collect()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def valid_loop(model, iterator, device=\"cuda\"):\n",
    "    ypred = []\n",
    "    ytarget = []\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(iterator), total=len(iterator), miniters=50) as pbar:\n",
    "            for i, (data, target) in pbar:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output_pos = model(data, target[:, 0])\n",
    "\n",
    "                # Shuffle target to build negative samples\n",
    "                target = target[torch.randperm(target.shape[0])]\n",
    "                output_neg = model(data, target[:, 0])\n",
    "\n",
    "                outputs = torch.cat([output_pos, output_neg])\n",
    "                targets = torch.cat(\n",
    "                    [torch.ones_like(output_pos), torch.zeros_like(output_neg)]\n",
    "                )\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                ypred.append(outputs.cpu().numpy())\n",
    "                ytarget.append(targets.cpu().numpy())\n",
    "                del data, target\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                cumloss = epoch_loss / (i + 1)\n",
    "                pbar.set_description(f\"Loss {cumloss:.5f}\")\n",
    "\n",
    "    ypred = np.concatenate(ypred)\n",
    "    ytarget = np.concatenate(ytarget)\n",
    "    gc.collect()\n",
    "\n",
    "    auc = roc_auc_score(ytarget.flatten(), ypred.flatten())\n",
    "\n",
    "    return epoch_loss / len(iterator), auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(f\"../output/matrix_factorization/train-proc-1_{MODE}.parquet\")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    train[f'lag{i}'] = train[f'lag{i}'].clip(0, None)\n",
    "    \n",
    "train['hour'] = train['hour'].astype('int32') + 1855602 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = train.loc[(train.session % 400) == 11].copy().reset_index()\n",
    "train = train.loc[(train.session % 400) != 11].reset_index()\n",
    "\n",
    "TRAIN = train[\n",
    "    [\n",
    "        \"lag1\",\n",
    "        \"lag2\",\n",
    "        \"lag3\",\n",
    "        \"lag4\",\n",
    "        \"lag5\",\n",
    "        \"lag6\",\n",
    "        \"lag7\",\n",
    "        \"lag8\",\n",
    "        \"lag9\",\n",
    "        \"lag10\",\n",
    "        \"hour\",\n",
    "    ]\n",
    "].values.copy()\n",
    "VALID = valid[\n",
    "    [\n",
    "        \"lag1\",\n",
    "        \"lag2\",\n",
    "        \"lag3\",\n",
    "        \"lag4\",\n",
    "        \"lag5\",\n",
    "        \"lag6\",\n",
    "        \"lag7\",\n",
    "        \"lag8\",\n",
    "        \"lag9\",\n",
    "        \"lag10\",\n",
    "        \"hour\",\n",
    "    ]\n",
    "].values.copy()\n",
    "TRAIN_TARGET = train[[\"lag0\"]].values.copy()\n",
    "VALID_TARGET = valid[[\"lag0\"]].values.copy()\n",
    "del train, valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatrixFactorization(n_aids=1855602 + 1, n_factors=128).to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lags = (1855603 * torch.rand(9, 11)).long().to(\"cuda\")\n",
    "# targets = (1855603 * torch.rand(9)).long().to(\"cuda\")\n",
    "# model(lags, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BS = 32 * 32 * 32\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    torch.as_tensor(TRAIN).long(), torch.as_tensor(TRAIN_TARGET).long()\n",
    ")\n",
    "train_dl = DataLoader(\n",
    "    train_ds, BS, True, num_workers=4, drop_last=True, pin_memory=True\n",
    ")\n",
    "\n",
    "valid_ds = TensorDataset(\n",
    "    torch.as_tensor(VALID).long(), torch.as_tensor(VALID_TARGET).long()\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    valid_ds, BS, False, num_workers=4, drop_last=False, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loop(model, train_dl, optimizer, 1000.0)\n",
    "\n",
    "    valloss, auc = valid_loop(model, valid_dl)\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"../output/matrix_factorization/model_giba_{epoch}_{auc:.4f}_v10.pt\",\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS:02d} \\t loss={valloss:.3f} \\t val_auc={auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = model.aid_emb.weight\n",
    "    embeddings = torch.nn.functional.normalize(embeddings, p=2.0, dim=1, eps=1e-12)\n",
    "    embeddings = embeddings.detach().cpu().numpy()\n",
    "\n",
    "np.save(f\"../output/matrix_factorization/embed_giba_{MODE}.npy\", embeddings)\n",
    "print(\n",
    "    f\"Saved embeddings of shape {embeddings.shape} to \"\n",
    "    + f\"../output/matrix_factorization/embed_giba_{MODE}.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
