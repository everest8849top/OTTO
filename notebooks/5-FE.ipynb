{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** This is an exploration notebook for feature computation. Use the `fe_main.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import cudf\n",
    "import glob\n",
    "import numba\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "from data.fe import *\n",
    "from utils.load import load_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val\"  #  \"val\", \"test\"\n",
    "CANDIDATES_VERSION = \"c-orders-v3\"\n",
    "FEATURES_VERSION = \"8\"\n",
    "\n",
    "SUFFIX = f\"{CANDIDATES_VERSION}.{FEATURES_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_FILE = f'../output/candidates/candidates_{CANDIDATES_VERSION}_{MODE}.parquet'\n",
    "PARQUET_FILES = f\"../output/{MODE}_parquet/*\"\n",
    "\n",
    "if MODE == \"val\":\n",
    "    OLD_PARQUET_FILES = \"../output/full_train_parquet/*\"\n",
    "elif MODE == \"train\":\n",
    "    OLD_PARQUET_FILES = \"../output/other_parquet/*\"\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = cudf.read_parquet(CANDIDATE_FILE)\n",
    "pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PART = 0\n",
    "CHUNK_SIZE = 1_000_000  # PER SESSION INSTEAD ??\n",
    "\n",
    "ids = np.arange(PART * CHUNK_SIZE, min((PART + 1) * CHUNK_SIZE, len(pairs)))\n",
    "pairs = pairs.iloc[ids].reset_index(drop=True)\n",
    "print(pairs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "weights = compute_weights(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.merge(weights, how=\"left\", on=[\"session\", \"candidates\"])\n",
    "pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)\n",
    "\n",
    "for c in weights.columns[2:]:\n",
    "    pairs[c] = pairs[c].fillna(pairs[c].min() / 2).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sessions\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = compute_w2v_features(pairs, PARQUET_FILES, f'../output/matrix_factorization/word2vec_{MODE}.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_PATH = \"../output/matrix_factorization/\"\n",
    "\n",
    "EMBED_NAMES = [\n",
    "    f'embed_1-9_64_cartbuy_{MODE}',\n",
    "    f'embed_1_64_{MODE}',\n",
    "    f'embed_1-5_64_{MODE}',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embed_name in EMBED_NAMES:\n",
    "    print(f'-> Features from matrix {embed_name}')\n",
    "    name = embed_name.rsplit('_', 1)[0]\n",
    "\n",
    "    # Load embeddings\n",
    "    embed_path = os.path.join(EMBED_PATH, embed_name + \".npy\")\n",
    "    embed = np.load(embed_path)\n",
    "    embed /= np.reshape(np.sqrt(np.sum(embed * embed, axis=1)), (-1, 1))\n",
    "    embed = np.concatenate((embed, np.zeros((1, embed.shape[1])))).astype(np.float32)\n",
    "    \n",
    "    # Retrieve sessions\n",
    "    sessions = load_sessions(PARQUET_FILES)\n",
    "    if \"_cartbuy\" in embed_path:\n",
    "        sessions = sessions[sessions['type'] != 0]\n",
    "    sessions = sessions.sort_values(['session', \"ts\"], ascending=[True, False])\n",
    "    \n",
    "    # Last n events\n",
    "    df_s = sessions[['session', \"aid\"]].groupby('session').first().reset_index()\n",
    "    df_s.columns = ['session', 'last_0']\n",
    "    \n",
    "    sessions['n'] = sessions[['session', \"aid\"]].groupby('session').cumcount()\n",
    "    for n in range(5):\n",
    "        if n > 0:\n",
    "            df_s = df_s.merge(\n",
    "                sessions[['session', \"aid\"]][sessions['n'] == n], how=\"left\", on=\"session\"\n",
    "            ).rename(columns={\"aid\": f\"last_{n}\"})\n",
    "        df_s[f\"last_{n}\"] = df_s[f\"last_{n}\"].fillna(embed.shape[0] - 1).astype(\"int32\")\n",
    "    \n",
    "    pairs = pairs.merge(df_s, how=\"left\", on=\"session\")\n",
    "    for n in range(5):\n",
    "        pairs[f\"last_{n}\"] = pairs[f\"last_{n}\"].fillna(embed.shape[0] - 1).astype(\"int32\")\n",
    "        pairs[f'{name}_last_{n}'] = np.sum(embed[pairs['candidates'].to_pandas().values] * embed[pairs[f'last_{n}'].to_pandas().values], axis=1)\n",
    "        pairs[f'{name}_last_{n}'] -= (pairs[f'last_{n}'] == embed.shape[0] - 1)  # nan are set to -1\n",
    "        pairs.drop(f'last_{n}', axis=1, inplace=True)\n",
    "\n",
    "    weights_noclick = None\n",
    "    if \"_cartbuy\" in embed_path:\n",
    "        sessions = load_sessions(PARQUET_FILES)\n",
    "        weights_noclick = compute_weights(sessions, no_click=True)\n",
    "\n",
    "    sessions = sessions.sort_values(['session', \"ts\"], ascending=[True, False])\n",
    "    \n",
    "    sessions = sessions.sort_values(['session', \"aid\"]).groupby('session').agg(list).reset_index()\n",
    "    pairs = pairs.merge(sessions[[\"session\", \"aid\"]], how=\"left\", on=\"session\")\n",
    "    pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)\n",
    "    \n",
    "    fts = compute_matrix_factorization_features(\n",
    "        pairs[[\"session\", \"candidates\", \"aid\"]],\n",
    "        embed,\n",
    "        weights if weights_noclick is None else weights_noclick\n",
    "    )\n",
    "    \n",
    "    for c in fts.columns[2:]:\n",
    "        pairs[f\"{name}_{re.sub('w_', '', c)}\"] = fts[c].values\n",
    "\n",
    "    del fts, sessions, weights_noclick, df_s, embed\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()\n",
    "\n",
    "    pairs.drop('aid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = compute_popularity_features(pairs, [OLD_PARQUET_FILES, PARQUET_FILES], \"\")\n",
    "pairs = compute_popularity_features(pairs, PARQUET_FILES, \"_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = compute_popularities_new(pairs, sessions, mode=MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.groupby('gt_orders').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covisitation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FOLDER = \"../output/matrices/\"\n",
    "# MATRIX_NAMES = [f\"matrix_123_temporal_20_{MODE}\", f\"matrix_123_type136_20_{MODE}\", f\"matrix_12__20_{MODE}\", f\"matrix_123_type0.590.5_20_{MODE}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_NAMES = [\n",
    "    f\"matrix_123_temporal_20_{MODE}\",\n",
    "    f\"matrix_123_type136_20_{MODE}\",\n",
    "    f\"matrix_12__20_{MODE}\",\n",
    "    f\"matrix_123_type0.590.5_20_{MODE}\",\n",
    "    f\"matrix_cpu-90_{MODE}\",\n",
    "    f\"matrix_cpu-95_{MODE}\",\n",
    "    f\"matrix_cpu-99_{MODE}\",\n",
    "    f\"matrix_gpu-116_{MODE}\",\n",
    "    f\"matrix_gpu-115_{MODE}\",\n",
    "    f\"matrix_gpu-93_{MODE}\",\n",
    "    f\"matrix_gpu-217_{MODE}\",\n",
    "    f\"matrix_gpu-226_{MODE}\",\n",
    "    f\"matrix_gpu-232_{MODE}\",\n",
    "    f\"matrix_gpu-239_{MODE}\",\n",
    "    f\"matrix_gpu-700_{MODE}\",\n",
    "    f\"matrix_gpu-701_{MODE}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "\n",
    "sessions = sessions.sort_values(['session', \"aid\"]).groupby('session').agg(list).reset_index()\n",
    "pairs = pairs.merge(sessions[[\"session\", \"aid\"]], how=\"left\", on=\"session\")\n",
    "pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in MATRIX_NAMES:\n",
    "    print(f' -> Features from {name}')\n",
    "\n",
    "    fts = compute_coocurence_features(\n",
    "        pairs[['session', 'candidates', 'aid']],\n",
    "        os.path.join(MATRIX_FOLDER, name + \".pqt\"),\n",
    "        weights\n",
    "    )\n",
    "\n",
    "    for c in fts.columns[2:]:\n",
    "        pairs[f\"{name.rsplit('_', 1)[0]}_{re.sub('w_', '', c)}\"] = fts[c].values\n",
    "\n",
    "    del fts\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()\n",
    "    \n",
    "pairs.drop('aid', axis=1, inplace=True)\n",
    "\n",
    "del sessions, weights\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rank_feature(pairs, feature):\n",
    "    df_ft = pairs[[\"session\", \"candidates\", feature]]\n",
    "    df_ft = df_ft.sort_values(feature, ascending=False, ignore_index=True)\n",
    "    df_ft[f'{feature}_rank'] = 1\n",
    "    df_ft[f'{feature}_rank'] = df_ft[f'{feature}_rank'].astype(\"uint8\")\n",
    "    df_ft[f'{feature}_rank'] = df_ft.groupby(\"session\")[f'{feature}_rank'].cumsum()\n",
    "\n",
    "    df_ft[f'{feature}_rank'] = df_ft.groupby([\"session\", feature])[f'{feature}_rank'].cummin()  # Ties\n",
    "\n",
    "    df_ft = df_ft.drop(feature, axis=1).sort_values([\"session\", \"candidates\"], ignore_index=True)\n",
    "\n",
    "    pairs[f'{feature}_rank'] = df_ft[f'{feature}_rank'].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts_to_rank = [ft for ft in pairs.columns[5:] if not any([k in ft for k in [\"_rank\", \"_sum\", \"_max\"]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ft in tqdm(fts_to_rank):\n",
    "    add_rank_feature(pairs, ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = pairs.columns[5:]  # [ft for ft in pairs.columns[5:] if \"rank\" in ft]\n",
    "\n",
    "# corr = pairs[FEATURES].corr()\n",
    "# corr = corr.to_pandas()\n",
    "# corr = corr.values\n",
    "\n",
    "# mask = np.zeros_like(corr, dtype=bool)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "# corr[mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TH = 0.99\n",
    "\n",
    "# for i in range(len(corr)):\n",
    "#     for j in range(len(corr)):\n",
    "#         if corr[i, j] > TH:\n",
    "#             print(FEATURES[i], FEATURES[j], f'{corr[i, j] :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session features\n",
    "- Count views/clicks/carts/orders of session\n",
    "- Count views/clicks/carts/orders of each candidate\n",
    "\n",
    "TODO :\n",
    "- Distance to last view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)\n",
    "\n",
    "for i, c in enumerate(CLASSES + [\"*\"]):\n",
    "    print(f'-> Candidate {c if c != \"*\" else \"views\"} in session')\n",
    "\n",
    "    sessions = load_sessions(PARQUET_FILES)\n",
    "    \n",
    "    if c != \"*\":\n",
    "        sessions.loc[sessions[\"type\"] != i, \"aid\"] = -1\n",
    "    sessions = sessions.groupby('session').agg(list).reset_index()\n",
    "\n",
    "    pairs[f'candidate_{c}_before'] = count_actions(\n",
    "        pairs[['session', 'candidates']],\n",
    "        sessions\n",
    "    )\n",
    "\n",
    "    del sessions\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "\n",
    "n_views = sessions[['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_views\"})\n",
    "n_clicks = sessions[sessions['type'] == 0][['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_clicks\"})\n",
    "n_carts = sessions[sessions['type'] == 1][['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_carts\"})\n",
    "n_orders = sessions[sessions['type'] == 2][['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_orders\"})\n",
    "\n",
    "sessions_fts = n_views.merge(n_clicks, how=\"left\", on=\"session\").fillna(0)\n",
    "sessions_fts = sessions_fts.merge(n_carts, how=\"left\", on=\"session\").fillna(0)\n",
    "sessions_fts = sessions_fts.merge(n_orders, how=\"left\", on=\"session\").fillna(0)\n",
    "\n",
    "for c in sessions_fts.columns[1:]:\n",
    "    sessions_fts[c] = np.clip(sessions_fts[c], 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.merge(sessions_fts, on=\"session\", how=\"left\")\n",
    "pairs = pairs.sort_values(['session', 'candidates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_by_chunks(pairs, f\"../output/features/fts_{MODE}_{SUFFIX}/\", part=PART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7241b2af102f7e024509099765066b36197b195077f7bfac6e5bc041ba17c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
