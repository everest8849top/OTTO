{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from params import *\n",
    "from utils.logger import Config, upload_to_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparation import *\n",
    "from data.processing import *\n",
    "from data.post_processing import *\n",
    "\n",
    "from utils.plot import *\n",
    "from utils.metric import *\n",
    "from inference.main import blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"2022-02-16/5/\"  # 1. deberta-large - 0.8811\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-17/1/\"  # 2. deberta-base - 0.8740\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-17/2/\"  # 3. roberta-large - 0.8763\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-23/1/\"  # 4. deberta-large-v3 - 0.8829\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-24/0/\"  # 5. deberta-large - 0.8811\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-24/3/\"  # 6. deberta-large-v3 - 0.8829\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-25/0/\"  # 7.deberta-large-v3 - 0.8836\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-26/2/\"  # 8. deberta-large-v3 MLM - 0.8808 \n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-28/2/\"  # 12. deberta-large-v3 pretrain pl1 - 0.8867\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-28/3/\"  # 13. deberta-large-v3 pretrain pl1 - 0.8861\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-28/4/\"  # 14. deberta-large-v3 pl1 1/1 - 0.8869\n",
    "EXP_FOLDER = LOG_PATH + \"2022-02-28/5/\"  # 15. deberta-large-v3 pl1 1/1 - 0.8873\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-01/1/\"  # 18. deberta-large-v3 pl1 1/1 - 0.8867\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-01/4/\"  # 21. deberta-large-v3 pl1 all - 0.8890\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-02/0/\"  # 24. deberta-large pl1 all - 0.8896\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-03/0/\"  # 26. deberta-large-v3 pl1 5/1 - 0.8870\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-04/1/\"  # 28. deberta-large-v3 pl1 10/1 bce upper - 0.8898\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-05/1/\"  # 30. deberta-large-v3 pl2 None bce upper - 0.8907\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-06/2/\"  # 32. deberta-large pl2 None - 0.8892\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-08/0/\"  # 33. electra-large ce lower - 0.8894\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-22/0/\"  # 34. deberta-large-v3 soft pl3 None bce - 0.8917\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-23/2/\"  # 35. electra-large soft pl3 None bce - 0.8892\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-24/2/\"  # 36. deberta-large soft pl3 None bce upper - 0.8911\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-24/3/\"  # 37. deberta-v3(large soft pl3 10 mlm bce upper - 0.8911\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-22/0/\"  # 38. deberta-large-v3 soft pl4 None bce upper - 0.8911\n",
    "EXP_FOLDER = LOG_PATH + \"2022-03-27/1/\"  # 39. deberta-large-MLM soft pl4 None bce  - 0.8922\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-02/2/\"  # 40. deberta-large-MLM2 soft pl4 None bce  - 0.8916\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-03/0/\"  # 41. deberta-large-v3 soft pl4 None bce nodroput - 0.8917\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/02-26_3/\" # 9. deberta-large-v3 - 0.8828\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/02-27_5/\" # 11. electra-large - 0.8792\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/02-28_1/\" # 16. electra-large pl1 1/1 - 0.8853\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-01_6/\" # 19. electra-large pl1 1/1 - 0.8860\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-01_7/\" # 20. deberta-large pl1 1/1 - 0.8845\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-02_0/\" # 22. deberta-base-v3 pl1 1/1 - 0.8823\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-02_1/\" # 23. roberta-large pl1 1/1 - 0.8834\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-02_2/\" # 25. electra-large pl1 all - 0.8891\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-03_2/\" # 27. roberta-large pl1 all - 0.8873\n",
    "EXP_FOLDER = LOG_PATH + \"aphrodeep/03-04_0/\" # 29. electra-large pl1 all bce upper - 0.8896\n",
    "\n",
    "\n",
    "\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-04/0/\"   # 43. v3-large - 0.8919\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-07/0/\"   # 46. roberta-large - 0.8890\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-07/3/\"   # 47. deberta-xlarge  soft pl4 x8 - 0.8921\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-08/0/\"   # 48. deberta-v2-xlarge  soft pl4 x8 - 0.8917\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-08/5/\"   # 49. PubMedBERT-base soft pl4 None 4ep- 0.8890\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-08/1/\"   # 51. roberta-large - 0.889\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-09/0/\"   # 52. deberta-base soft pl4 - 0.8910\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-09/1/\"   # 53. roberta-base soft pl4 - 0.8890\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-10/0/\"   # 54. deberta-v3-base soft pl3 - 0.891\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-10/1/\"   # 55. roberta-base soft pl4 - 0.8890\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-10/2/\"   # 56. PubMedBERT-base soft pl3 - 0.8890\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-10/3/\"   # 57. roberta-large soft pl3 - 0.8908\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-11/1/\"   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "EXP_FOLDER = LOG_PATH + \"2022-04-12/3/\"   # 59. deberta-large soft pl3 - 0.8919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # 0.8897 -> PL1\n",
    "    LOG_PATH + \"2022-02-16/5/\",  # 1. deberta-large - 0.8811\n",
    "    LOG_PATH + \"2022-02-25/0/\",  # 7. deberta-large-v3 - 0.8836\n",
    "    LOG_PATH + \"2022-02-28/3/\",  # 13. deberta-large-v3 pretrain pl1 - 0.8861\n",
    "]\n",
    "\n",
    "EXP_FOLDERS = [  # 0.8915 -> PL2\n",
    "    LOG_PATH + \"2022-03-01/1/\",      # 18. deberta-large-v3 pl1 1/1 - 0.8867\n",
    "    LOG_PATH + \"aphrodeep/03-01_6/\", # 19. electra-large 1/1- 0.8860\n",
    "    LOG_PATH + \"2022-03-02/0/\",      # 24. deberta-large pl1 all - 0.8896\n",
    "]\n",
    "\n",
    "EXP_FOLDERS = [  # 0.8921 - LB 0.8889\n",
    "    LOG_PATH + \"2022-03-04/1/\",      # 28. deberta-large-v3 extra diff 891 10/1 bce upper - 0.8898\n",
    "    LOG_PATH + \"2022-03-05/1/\"       # 30. deberta-large-v3 pl2 None bce upper - 0.8907\n",
    "]\n",
    "\n",
    "EXP_FOLDERS = [  # 0.8919 - LB 0.8890 -> PL3\n",
    "    LOG_PATH + \"2022-03-02/0/\",      # 24. deberta-large extra diff 891 all - 0.8896\n",
    "    LOG_PATH + \"2022-03-05/1/\",      # 30. deberta-large-v3 pl2 None bce upper - 0.8907\n",
    "]\n",
    "\n",
    "EXP_FOLDERS = [  # 0.8930 - 0.891 -> PL4\n",
    "    LOG_PATH + \"2022-03-22/0/\",  # 34. deberta-large-v3 soft pl3 None bce - 0.8917\n",
    "    LOG_PATH + \"2022-03-24/2/\",  # 36. deberta-large soft pl3 None bce upper - 0.8911\n",
    "    LOG_PATH + \"2022-03-24/3/\",  # 37. deberta-v3-large soft pl3 10 mlm bce upper - 0.8911\n",
    "]\n",
    "\n",
    "WEIGHTS = None\n",
    "\n",
    "EXP_FOLDERS = [  # CV 0.8939 - LB 0.891\n",
    "    LOG_PATH + \"2022-03-22/0/\",  # 34. deberta-large-v3 soft pl3 None - 0.8917\n",
    "    LOG_PATH + \"2022-04-07/3/\",  # 47. deberta-xlarge  soft pl4 x8 - 0.8921\n",
    "\n",
    "    LOG_PATH + \"2022-04-09/0/\",  # 52. deberta-base  soft pl4 - 0.8910\n",
    "    LOG_PATH + \"2022-04-09/1/\",  # 53. roberta-base  soft pl3 - 0.8890EXP_FOLDER = LOG_PATH + \"2022-04-10/2/\"   # 57. PubMedBERT-base soft pl3 - 0.8890\n",
    "    LOG_PATH + \"2022-04-10/0/\",  # 54. deberta-v3-base  soft pl3 - 0.8911\n",
    "]\n",
    "\n",
    "WEIGHTS = [1, 1, 0.5, 0.5, 0.5]\n",
    "\n",
    "\n",
    "EXP_FOLDERS = [  # CV 0.8938 - LB 0.8919\n",
    "    LOG_PATH + \"2022-03-22/0/\",   # 34. deberta-large-v3 soft pl3 None - 0.8917\n",
    "    LOG_PATH + \"2022-04-07/3/\",   # 47. deberta-xlarge  soft pl4 x8 - 0.8921\n",
    "    LOG_PATH + \"2022-04-10/3/\",   # 57. roberta-large soft pl3 - 0.8908\n",
    "    LOG_PATH + \"2022-04-10/0/\",   # 54. deberta-v3-base soft pl3 - 0.891\n",
    "]\n",
    "\n",
    "WEIGHTS = [1, 1, 1, 1]\n",
    "\n",
    "EXP_FOLDERS = [  # CV 0.8937 - LB 0.8919+ / 0.892 th=0.47\n",
    "    LOG_PATH + \"2022-04-07/3/\",   # 47. deberta-xlarge  soft pl4 x8 - 0.8921\n",
    "    LOG_PATH + \"2022-04-10/3/\",   # 57. roberta-large soft pl3 - 0.8908\n",
    "    LOG_PATH + \"2022-04-10/0/\",   # 54. deberta-v3-base soft pl3 - 0.891\n",
    "    LOG_PATH + \"2022-04-11/1/\",   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "]\n",
    "\n",
    "WEIGHTS = [1, 1, 1, 2]\n",
    "\n",
    "# EXP_FOLDERS = [\n",
    "#     LOG_PATH + \"2022-04-10/0/\",   # 54. deberta-v3-base soft pl3 - 0.891\n",
    "#     LOG_PATH + \"2022-04-11/1/\",   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "#     LOG_PATH + \"2022-04-12/3/\",   # 59. deberta-large soft pl3 - 0.8919\n",
    "# ]\n",
    "\n",
    "# WEIGHTS = [0.25, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "    LOG_PATH + \"2022-04-14/0/\",   # 60. deberta-xlarge  soft pl3 x8 - 0.8911\n",
    "#     LOG_PATH + \"2022-04-13/1/\",   # 60. deberta-xlarge  soft pl3 x8 - 0.8908\n",
    "    \n",
    "    LOG_PATH + \"2022-04-10/0/\",   # 54. deberta-v3-base soft pl3 - 0.891\n",
    "    LOG_PATH + \"2022-04-10/3/\",   # 57. roberta-large soft pl3 - 0.8908\n",
    "    LOG_PATH + \"2022-04-11/1/\",   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "    LOG_PATH + \"2022-04-12/3/\",   # 59. deberta-large soft pl3 - 0.8919\n",
    "]\n",
    "\n",
    "WEIGHTS = [0, 0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDERS = [\n",
    "# #     LOG_PATH + \"2022-04-22/1/\",   # deberta-v3-large soft pl case 5µ\n",
    "#     LOG_PATH + \"2022-04-11/1/\",   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "# #     LOG_PATH + \"2022-04-12/3/\",   # 59. deberta-large soft pl3 - 0.8919\n",
    "# ]\n",
    "# WEIGHTS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_to_kaggle(EXP_FOLDERS, DATASETS_DIR + \"weights_base/\", \"NBME Weights base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(json.load(open(EXP_FOLDERS[0] + \"config.json\", 'r')))\n",
    "\n",
    "try:\n",
    "    _ = config.lower\n",
    "except AttributeError:\n",
    "    config.lower = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_prepare(root=DATA_PATH, lower=config.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = LOG_PATH + \"2022-04-10/0/\"   # 54. deberta-v3-base soft pl3 - 0.891\n",
    "# folder = LOG_PATH + \"2022-04-11/1/\"   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "\n",
    "# pred_oof = np.load(folder + \"pred_oof.npy\", allow_pickle=True)\n",
    "# from sklearn.metrics import *\n",
    "\n",
    "# pred_cls = np.array([p.max() for p in pred_oof])\n",
    "# tgt_cls = df['target'].apply(lambda x: int(np.max(x) > 0))\n",
    "\n",
    "# ConfusionMatrixDisplay.from_predictions(tgt_cls, pred_cls > 0.5)\n",
    "# plt.title(f'Accuracy={accuracy_score(tgt_cls, pred_cls > 0.5) :.4f}')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv(config.folds_file)\n",
    "df = df.merge(folds, how=\"left\", on=[\"case_num\", \"pn_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_oof = blend(\n",
    "    [np.load(f + \"pred_oof.npy\", allow_pickle=True).tolist() for f in EXP_FOLDERS],\n",
    "    weights=WEIGHTS\n",
    ")\n",
    "\n",
    "# pred_oof = np.load('../output/pred_oof.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "df[\"probs\"] = pred_oof\n",
    "# df[\"probs1\"] = pred_oof\n",
    "# pred_oof = np.load(EXP_FOLDERS[0] + \"pred_oof.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import *\n",
    "# df = df[df['fold'] == 0].reset_index(drop=True)\n",
    "\n",
    "# print(accuracy_score(\n",
    "#     df['target'].apply(lambda x: np.max(x > 0.5)), \n",
    "#     df['probs'].apply(lambda x: np.max(x > 0.5)), \n",
    "# ))\n",
    "\n",
    "# print(confusion_matrix(\n",
    "#     preds_cls > 0.5,\n",
    "#     df['target'].apply(lambda x: np.max(x > 0.5)),\n",
    "# ))\n",
    "\n",
    "# print(confusion_matrix(\n",
    "#     df['probs'].apply(lambda x: np.max(x > 0.5)), \n",
    "#     preds_cls > 0.5,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preds_cls = np.load('../output/preds_cls_pl.npy')\n",
    "# th = 1\n",
    "\n",
    "# for i in range(len(preds_cls)):\n",
    "#     if preds_cls[i] < th:\n",
    "#         if df['probs'][i].max() > 0.5 and df['probs'][i].max() < 0.8:\n",
    "#             print('erase')\n",
    "#             df['probs'][i][:] = 0 \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_oof = pd.read_pickle(\"/home/theo/kaggle/nbme/output/oof_v15a.pkl\")\n",
    "\n",
    "# pred_oof_a = df_oof['char_probs'].values.tolist()\n",
    "# pred_oof_b = df_oof['char_probs_b'].values.tolist()\n",
    "\n",
    "# pred_oof = []\n",
    "# for a, b in zip(pred_oof_a, pred_oof_b):\n",
    "#     pred_oof.append(np.array([\n",
    "#         1 - a,\n",
    "#         a - b,\n",
    "#         b,\n",
    "#     ]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXP_FOLDERS = [\n",
    "#     LOG_PATH + \"2022-04-22/1/\",   # deberta-v3-large soft pl case 5\n",
    "# #     LOG_PATH + \"2022-04-11/1/\",   # 58. deberta-v3-large soft pl3 - 0.8921\n",
    "# #     LOG_PATH + \"2022-04-12/3/\",   # 59. deberta-large soft pl3 - 0.8919\n",
    "# ]\n",
    "# WEIGHTS = None\n",
    "\n",
    "# pred_oof = blend(\n",
    "#     [np.load(f + \"pred_oof.npy\", allow_pickle=True).tolist() for f in EXP_FOLDERS],\n",
    "#     weights=WEIGHTS\n",
    "# )\n",
    "\n",
    "# # df[\"probs\"] = pred_oof\n",
    "# df[\"probs1\"] = pred_oof\n",
    "# # pred_oof = np.load(EXP_FOLDERS[0] + \"pred_oof.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def specific_case(p1, p2, case):\n",
    "#     if case in [3, 5]:\n",
    "#         return preds_to_labels([p2])[0]\n",
    "#     else:\n",
    "#         return preds_to_labels([p1])[0]\n",
    "    \n",
    "# df[\"preds\"] = df.apply(lambda x: specific_case(x.probs, x.probs1, x.case_num), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pred_oof[0].shape[-1] == 3:\n",
    "#     pred_oof = [p[:, 1:].sum(-1) for p in pred_oof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROB_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "# PROB_THRESHOLDS = [0.4, 0.6, 0.4, 0.6, 0.8, 0.5, 0.6, 0.6, 0.5, 0.5]\n",
    "# PROB_THRESHOLDS = [0.34, 0.33, 0.45, 0.6, 0.79, 0.47, 0.55, 0.61, 0.75, 0.3]\n",
    "# PROB_THRESHOLDS = [0.4, 0.4, 0.5, 0.5, 0.6, 0.5, 0.5, 0.5, 0.6, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(df)):\n",
    "#     if df['probs'][i].max() > 0.5 and df['probs'][i].max() < 0.8:\n",
    "# #         print('erase')\n",
    "#         df['probs'][i][:] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['probs'] = pred_oof\n",
    "df['preds'] = preds_to_labels(df[\"probs\"].values.tolist(), threshold=PROB_THRESHOLD)\n",
    "# df['preds'] = preds_to_labels_advanced(pred_oof, df['case_num'].values, thresholds=PROB_THRESHOLDS)\n",
    "\n",
    "df['preds_pp'] = df.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_ = df[df['fold'] == i].reset_index()\n",
    "    cv_f1 = micro_f1(df_['preds_pp'], df_['target'])\n",
    "    print(f\"-> Fold #{i} score : {cv_f1:.4f}\")\n",
    "#     break\n",
    "\n",
    "cv_f1 = micro_f1(df['preds_pp'], df['target'])\n",
    "print(f\"\\n-> CV score: {cv_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0\n",
    "preds_pp_blacklist = []\n",
    "preds_pp = []\n",
    "gts = []\n",
    "\n",
    "for fold in range(5):\n",
    "    for case in range(10):\n",
    "        dfc = df[df[\"case_num\"] == case]\n",
    "        for ft in tqdm(dfc['feature_num'].unique()):\n",
    "            dfcft = dfc[dfc[\"feature_num\"] == ft].reset_index(drop=True)\n",
    "            df_train = dfcft[dfcft[\"fold\"] != fold].reset_index(drop=True)\n",
    "            df_val = dfcft[dfcft[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    #         print(f'\\n -> Feature {dfcft[\"feature_text\"][0]}')\n",
    "\n",
    "            tps_train, fps_train, fns_train, tns_train = [], [], [], []\n",
    "\n",
    "            for i, (text, feature_text, pred, tgt) in enumerate(\n",
    "                df_train[[\"pn_history\", \"feature_text\", \"preds_pp\", \"target\"]].values,  start=1\n",
    "            ):\n",
    "                tps, fps, fns = np.zeros(len(text)), np.zeros(len(text)), np.zeros(len(text))\n",
    "\n",
    "                for j in range(len(text)):\n",
    "                    if pred[j] and tgt[j]:\n",
    "                        tps[j] = 1\n",
    "                    elif pred[j] and not tgt[j]:\n",
    "                        fps[j] = 1\n",
    "                    elif not pred[j] and tgt[j]:\n",
    "                        fns[j] = 1\n",
    "\n",
    "                tns = char_target_to_span(1 - np.max([tps, fps, fns], 0))\n",
    "                tps = char_target_to_span(tps)\n",
    "                fps = char_target_to_span(fps)\n",
    "                fns = char_target_to_span(fns)\n",
    "\n",
    "                for span in tps:\n",
    "                    tps_train.append(text[span[0]: span[1]].strip().lower())\n",
    "                for span in fps:\n",
    "                    fps_train.append(text[span[0]: span[1]].strip().lower())\n",
    "                for span in fns:\n",
    "                    fns_train.append(text[span[0]: span[1]].strip().lower())\n",
    "                for span in tns:\n",
    "                    tns_train.append(text[span[0]: span[1]].strip().lower())\n",
    "\n",
    "#             count = Counter(fps_train)\n",
    "            to_remove = [i for i in fps_train if i not in tps_train] # and count[i] > 1]\n",
    "\n",
    "            for i, (text, feature_text, pred, tgt) in enumerate(\n",
    "                df_val[[\"pn_history\", \"feature_text\", \"preds_pp\", \"target\"]].values,  start=1\n",
    "            ):\n",
    "                new_pred = pred.copy()\n",
    "                spans = char_target_to_span(pred)\n",
    "\n",
    "                for span in spans:\n",
    "                    if text[span[0]: span[1]].strip().lower() in to_remove:\n",
    "                        new_pred[span[0]: span[1]] = 0\n",
    "                        p = tgt[span[0]: span[1]]\n",
    "                        print(f'Removed \"{text[span[0]: span[1]].strip().lower()}\" - {p}')\n",
    "                        delta += (p == 0).sum()\n",
    "                        delta -= (p > 0).sum()\n",
    "                    \n",
    "                preds_pp_blacklist.append(new_pred)\n",
    "                preds_pp.append(pred)\n",
    "                gts.append(tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1(preds_pp, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1(preds_pp_blacklist, gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = {}\n",
    "# # for threshold in np.arange(0.4, 0.75, 0.05):\n",
    "# for threshold in np.arange(0.47, 0.511, 0.01):\n",
    "#     df['preds'] = preds_to_labels(pred_oof, threshold=threshold)\n",
    "#     df['preds_pp'] = df.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)\n",
    "\n",
    "#     cv_f1 = micro_f1(df['preds_pp'], df['target'])\n",
    "#     scores[threshold] = cv_f1\n",
    "\n",
    "#     print(f\"Threshold : {threshold:.2f}\\t-  CV score : {cv_f1:.4f}\")\n",
    "\n",
    "# PROB_THRESHOLD = max(scores, key=scores.get)\n",
    "\n",
    "# print(f'\\n -> Best score {max(scores.values()) :.4f} with threshold {PROB_THRESHOLD:.2f}')\n",
    "\n",
    "# PROB_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case_scores = {}\n",
    "\n",
    "for case in sorted(df['case_num'].unique()):\n",
    "    ths = []\n",
    "    for fold in range(5):\n",
    "        scores = {}\n",
    "        for threshold in np.arange(0.3, 0.85, 0.05):\n",
    "            df['preds'] = preds_to_labels(pred_oof, threshold=threshold)\n",
    "\n",
    "            dfc = df[df['case_num'] == case].reset_index()\n",
    "            dfc = dfc[dfc['fold'] == fold].reset_index()\n",
    "            dfc['preds_pp'] = dfc.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)\n",
    "\n",
    "            cv_f1 = micro_f1(dfc['preds_pp'], dfc['target'])\n",
    "            scores[np.round(threshold, 2)] = cv_f1\n",
    "\n",
    "        PROB_THRESHOLD = max(scores, key=scores.get)\n",
    "        print(f'-> Case {case} - Best score {max(scores.values()) :.4f} with threshold {PROB_THRESHOLD:.2f}')\n",
    "        ths.append(PROB_THRESHOLD)\n",
    "\n",
    "        score = cv_f1 = micro_f1(dfc['preds_pp'], dfc['target'])\n",
    "        case_scores[case] = scores\n",
    "    print(f\"Avg : {np.mean(ths):.2f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_scores = []\n",
    "# ref_scores = []\n",
    "# thresholds = []\n",
    "\n",
    "# plt.figure(figsize=(15, 20))\n",
    "# for i in range(len(case_scores)):\n",
    "#     plt.subplot(4, 3, i + 1)\n",
    "#     plt.grid(True)\n",
    "    \n",
    "#     plt.plot(case_scores[i].keys(), case_scores[i].values())\n",
    "    \n",
    "#     th = max(case_scores[i], key=case_scores[i].get)\n",
    "# #     th = np.round(th * 10) / 10\n",
    "#     thresholds.append(th)\n",
    "#     s = case_scores[i][th]\n",
    "#     ref = case_scores[i][0.5]\n",
    "#     plt.title(f'Case {i} - Best {s :.4f} th={th:.2f} (+ {s - ref:.4f})')\n",
    "    \n",
    "#     best_scores.append(s)\n",
    "#     ref_scores.append(ref)\n",
    "\n",
    "# print(f'Best score : {np.mean(best_scores) :.4f}')\n",
    "# print(f'thresholds = {thresholds}')\n",
    "# print(f'Ref score : {np.mean(ref_scores) :.4f}')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preds viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].apply(lambda x: np.sum(x > 0)).sum(), df['preds_pp'].apply(lambda x: np.sum(x > 0.5)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotated'] = df['target'].apply(lambda x: np.max(x) > 0.5)\n",
    "df['found'] = df['preds_pp'].apply(lambda x: np.max(x) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fps = df[~df['annotated'] & df['found']]\n",
    "len(df_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_fns = df[df['annotated'] & ~df['found']]\n",
    "len(df_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tps = df[df['annotated'] & df['found']]\n",
    "len(df_tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tns = df[~df['annotated'] & ~df['found']]\n",
    "len(df_tns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for c in range(10):\n",
    "#     print(c)\n",
    "#     dfg = df[df['case_num'] == c][['annotated', 'found', 'feature_text']].groupby('feature_text').mean()\n",
    "\n",
    "#     display(dfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = df[['case_num', 'pn_num', 'preds_pp', 'target']].groupby(['case_num', 'pn_num']).agg(list).reset_index()\n",
    "dfg['score'] = dfg.apply(lambda x: micro_f1(x.preds_pp, x.target), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 3\n",
    "dfc = dfg[dfg['case_num'] == c].reset_index(drop=True)\n",
    "dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for c in range(10):\n",
    "    dfc = dfg[dfg['case_num'] == c].reset_index(drop=True)\n",
    "    plt.scatter(\n",
    "#         dfc['pn_num'] % 10000, \n",
    "        [i for i in range(len(dfc))],\n",
    "        dfc['score'], \n",
    "        marker=\"x\", \n",
    "        label=c\n",
    "    )\n",
    "\n",
    "plt.legend()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = df[df['location'].apply(lambda x: any([';' in y for y in x]))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['pn_history'].apply(lambda x: \"  \" in x)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pn_history'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case = 5\n",
    "\n",
    "for k, i in enumerate(df[df[\"case_num\"] == case]['pn_num'].unique()):\n",
    "    plot_predictions(df, i, pred_col=\"preds_pp\", max_score=1)\n",
    "\n",
    "#     if k > 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score thresh PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for threshold in np.arange(PROB_THRESHOLD, PROB_THRESHOLD + 0.25, 0.01):\n",
    "    df['preds_pp_score'] = df.apply(\n",
    "        lambda x: post_process_score(x['probs'], x['preds_pp'], threshold=threshold), 1\n",
    "    )\n",
    "\n",
    "#     df['preds_pp_score'] = df.apply(\n",
    "#         lambda x: post_process_score_grouped(\n",
    "#             x['probs'], x['preds_pp'], proba_threshold=PROB_THRESHOLD, score_threshold=threshold\n",
    "#         ), 1\n",
    "#     )\n",
    "#     df['preds_pp_score'] = df.apply(\n",
    "#         lambda x: post_process_spaces(x['preds_pp_score'], x['clean_text']), 1\n",
    "#     )\n",
    "\n",
    "    cv_f1 = micro_f1(df['preds_pp_score'], df['target'])\n",
    "    scores[threshold] = cv_f1\n",
    "\n",
    "    print(f\"Threshold : {threshold:.2f}\\t-  CV score PP: {cv_f1:.4f}\")\n",
    "\n",
    "SCORE_THRESHOLD = max(scores, key=scores.get)\n",
    "\n",
    "print(f'\\n -> Best score {max(scores.values()) :.4f} with threshold {SCORE_THRESHOLD:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preds_pp_score'] = df.apply(\n",
    "    lambda x: post_process_score(x['probs'], x['preds_pp'], threshold=SCORE_THRESHOLD), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_diff = df[df.apply(lambda x: (x['preds_pp'] != x['preds_pp_score']).any(), 1)].reset_index()\n",
    "\n",
    "\n",
    "for k, i in enumerate(df_diff['pn_num'].unique()):\n",
    "    plot_predictions(df_diff, i, pred_col=\"preds_pp\")\n",
    "    print('\\n')\n",
    "\n",
    "    plot_predictions(df_diff, i, pred_col=\"preds_pp_score\")\n",
    "    print('\\n')\n",
    "    \n",
    "        \n",
    "    if k > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def resplit_words(words):\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        for c in [\"/\", \"-\", \".\", \"\\\\\", ':', ',', '+', '=']:\n",
    "            w = f\" {c} \".join(w.split(c))\n",
    "            w = f\" {c} \".join(w.split(c))\n",
    "        new_words += filter(None, w.split(' '))\n",
    "        \n",
    "    return new_words\n",
    "\n",
    "def char_to_words(text, target):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = resplit_words(words)\n",
    "    \n",
    "    word_targets = []\n",
    "    current_idx = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in ['``', \"''\"]:\n",
    "            word = '\"'\n",
    "            \n",
    "        start_idx = text.find(word, current_idx)\n",
    "\n",
    "        if start_idx >= 0:\n",
    "            word_targets.append(target[start_idx: start_idx + len(word)] )\n",
    "            current_idx = start_idx + len(word)\n",
    "        else:\n",
    "            print(word)\n",
    "        \n",
    "    return words, word_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "words, word_targets = char_to_words(df['clean_text'][idx], df['target'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "for idx in tqdm(range(len(df))):\n",
    "    \n",
    "    words, word_targets = char_to_words(df['clean_text'][idx], df['target'][idx])\n",
    "    \n",
    "#     print(words)\n",
    "    \n",
    "    for w, t in zip(words, word_targets):\n",
    "        if not len(np.unique(t > 0)) == 1:\n",
    "            print(idx, w, t)\n",
    "            \n",
    "#             break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['clean_text'][0]\n",
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text.find(words[0], current_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Af-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = df[df['feature_num'] == 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, i in enumerate(df_ft['pn_num'].unique()):\n",
    "    plot_predictions(df_ft, i, pred_col=\"preds_pp\", max_score=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def split_to_words(chars):\n",
    "    for c in ['?', '!', '-', '/', ',', '.']:\n",
    "        chars = chars.replace(c, f\" {c} \")\n",
    "    return nltk.word_tokenize(chars)\n",
    "#     return chars.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def af_idf(df):\n",
    "\n",
    "    annotated_words, words = [], []\n",
    "    for i in range(len(df)):\n",
    "        text = df['clean_text'][i].lower()\n",
    "        annotation = \" \".join(df['annotation'][i]).lower()\n",
    "\n",
    "        words += split_to_words(text)\n",
    "        if len(annotation):\n",
    "            annotated_words += split_to_words(annotation)   \n",
    "\n",
    "    af_idf_dic = dict(Counter(annotated_words))\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    for k in af_idf_dic.keys():\n",
    "        try:\n",
    "            af_idf_dic[k] /= word_count[k]\n",
    "        except:\n",
    "            af_idf_dic[k] = 1\n",
    "    \n",
    "    return dict(sorted(af_idf_dic.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft = df_ft['feature_text'][0]\n",
    "idf = af_idf(df_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Len PP\n",
    "- Doesn't work unless you overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preds viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, i in enumerate(df['pn_num'].unique()):\n",
    "    plot_predictions(df, i, pred_col=\"preds_pp_score\")\n",
    "    print('\\n')\n",
    "    \n",
    "#     plot_predictions(df, i, pred_col=\"preds_pp_inc\")\n",
    "#     print('\\n')\n",
    "        \n",
    "    if k > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"preds_pp_score\"\n",
    "col_group = \"feature_num\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=5000)\n",
    "splits = list(skf.split(df, y=df[col_group]))\n",
    "\n",
    "\n",
    "split = 0\n",
    "stats_idx = splits[split][0]\n",
    "select_idx = splits[split][0]\n",
    "val_idx = splits[split][1]\n",
    "\n",
    "# stats_idx, select_idx, val_idx = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df.iloc[stats_idx].reset_index(drop=True) if stats_idx is not None else df.copy()\n",
    "\n",
    "df_stats['annot_lens'] = df_stats['annotation'].apply(lambda x: [len(y) for y in x])\n",
    "# df_stats['annot_lens'] = df_stats['annot_lens'].apply(lambda x: np.min(y) if len(y) else 0)\n",
    "\n",
    "min_lens = df_stats[[\"feature_num\", \"annot_lens\"]].groupby('feature_num').agg(\n",
    "    lambda x: np.min(np.concatenate(list(x))) if len(np.concatenate(list(x))) else 0\n",
    ").reset_index()\n",
    "\n",
    "df_stats = df_stats.drop('annot_lens', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find features to PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_len(text, pred, min_len=0, delta=2):\n",
    "    if pred.sum() < min_len - delta:\n",
    "        pred = np.zeros(len(pred))\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_f1_grouped(df, col_group, col_pred=\"pred\", col_truth=\"target\"):\n",
    "    dfg = df[[col_group, col_pred, col_truth]].groupby(col_group).agg(list)\n",
    "\n",
    "    dfg['score'] = dfg.apply(lambda x: micro_f1(x[col_pred], x[col_truth]), 1)\n",
    "\n",
    "    return dfg['score'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[select_idx].reset_index(drop=True) if select_idx is not None else df.copy()\n",
    "df_train = df_train.merge(min_lens, on=\"feature_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['preds_pp_len'] = df_train.apply(\n",
    "    lambda x: post_process_len(x['clean_text'], x['preds_pp_score'], x['annot_lens']), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = micro_f1_grouped(df_train, col_group, col)\n",
    "scores_ref = dfg['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = micro_f1_grouped(df_train, col_group, \"preds_pp_len\")\n",
    "scores_pp = dfg['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_gap = 0.001\n",
    "to_replace = []\n",
    "\n",
    "for i in range(len(scores_ref)):\n",
    "    if scores_pp[i] > 0 and scores_ref[i] > 0:\n",
    "        gap = scores_pp[i] - scores_ref[i]\n",
    "        if gap > min_gap: # or gap < -0.001:\n",
    "            print(f'{col_group} {dfg[col_group][i]:03d} : {gap:.3f}')\n",
    "\n",
    "        if gap > min_gap:\n",
    "            to_replace.append(dfg[col_group][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df.iloc[val_idx].reset_index(drop=True) if select_idx is not None else df_.copy()\n",
    "df_val = df_val.merge(min_lens, on=\"feature_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['preds_pp_len'] = df_val.apply(\n",
    "    lambda x: post_process_len(x['clean_text'], x[col], x['annot_lens']), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_val['pred_pp_len_opti'] = df_val.apply(\n",
    "    lambda x: x['preds_pp_len'] if x[col_group] in to_replace else x[col], 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_f1 = micro_f1(df_val[col], df_val['target'])\n",
    "\n",
    "print(f\"-> Val score PP: {cv_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_f1 = micro_f1(df_val['preds_pp_len'], df_val['target'])\n",
    "\n",
    "print(f\"-> Val score PP: {cv_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_f1 = micro_f1(df_val['pred_pp_len_opti'], df_val['target'])\n",
    "\n",
    "print(f\"-> Val score PP: {cv_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_diff = df_val[df_val.apply(lambda x: (x['pred_pp_len_opti'] != x['preds_pp_score']).any(), 1)].reset_index()\n",
    "\n",
    "for k, i in enumerate(df_diff['pn_num'].unique()):\n",
    "    plot_predictions(df_diff, i, pred_col=\"preds_pp_score\")\n",
    "    print('\\n')\n",
    "\n",
    "    plot_predictions(df_diff, i, pred_col=\"pred_pp_len_opti\")\n",
    "    print('\\n')\n",
    "        \n",
    "#     if k > 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test adj pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_adj(preds):\n",
    "    preds = np.copy(preds)\n",
    "    for i in range(1, len(preds) - 1):\n",
    "        if preds[i]:\n",
    "            continue\n",
    "        if preds[i + 1] and preds[i - 1]:\n",
    "            preds[i] = 1\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['preds_pp_ajd'] = df['preds_pp'].apply(post_process_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_f1 = micro_f1(df['preds_pp_ajd'].values.tolist(), df['target'].values.tolist())\n",
    "\n",
    "# print(f\"-> CV score PP: {cv_f1:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pos(target, idx, error=\"none\"):\n",
    "    if target[idx] == \"\":\n",
    "        if error == \"left\":\n",
    "            return is_pos(target, idx - 1, error=error)\n",
    "        elif error == \"right\": \n",
    "            return is_pos(target, idx + 1, error=error) \n",
    "        else:\n",
    "            return False\n",
    "    return int(target[idx]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_ajd_w(preds, text):\n",
    "    preds = preds.astype(str)\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char == \" \":\n",
    "            preds[i] = \" \"\n",
    "            \n",
    "    target = \"\".join(preds)\n",
    "    \n",
    "    target_words = target.split(' ')\n",
    "    \n",
    "#     for i in range(1, len(target_words) - 1):\n",
    "# #         if not len(target_words[i]):\n",
    "# #             continue\n",
    "\n",
    "#         if is_pos(target_words, i):\n",
    "#             continue\n",
    "\n",
    "#         if is_pos(target_words, i - 1) and is_pos(target_words, i + 1):\n",
    "#             target_words[i] = \"1\" * len(target_words[i])\n",
    "\n",
    "    target = \"0\".join(target_words)\n",
    "    target = np.array(list(target)).astype(int)\n",
    "    \n",
    "    target = post_process_spaces(target, text)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['preds_pp_adj_w'] = df.apply(lambda x: post_process_ajd_w(x['preds'], x['clean_text']), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_f1 = micro_f1(df['preds_pp_adj_w'], df['target'])\n",
    "\n",
    "print(f\"-> CV score PP: {cv_f1:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PP Incomplete words\n",
    "- Doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['preds_pp_inc'] = df.apply(lambda x: post_process_incomplete(x['preds_pp_score'], x['pn_history']), 1)\n",
    "df['preds_pp_inc'] = df.apply(lambda x: post_process_spaces(x['preds_pp_inc'], x['clean_text']), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_f1 = micro_f1(df['preds_pp_inc'].values.tolist(), df['target'].values.tolist())\n",
    "\n",
    "print(f\"-> CV score PP: {cv_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_diff = df[df.apply(lambda x: (x['preds_pp_score'] != x['preds_pp_inc']).any(), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k, i in enumerate(df_diff['pn_num'].unique()):\n",
    "#     plot_predictions(df_diff, i, pred_col=\"preds_pp\")\n",
    "#     print('\\n')\n",
    "    \n",
    "#     plot_predictions(df_diff, i, pred_col=\"preds_pp_inc\")\n",
    "#     print('\\n')\n",
    "        \n",
    "# #     if k > 1:\n",
    "# #         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
