{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preparation import *\n",
    "from data.dataset import *\n",
    "from data.processing import *\n",
    "from data.tokenization import *\n",
    "from params import *\n",
    "from training.main import k_fold\n",
    "from models import NERTransformer\n",
    "\n",
    "from utils.plot import plot_annotation\n",
    "from utils.logger import prepare_log_folder, save_config, create_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.tokenization import *\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "\n",
    "for name in tqdm([\n",
    "#     \"roberta-base\", \n",
    "#     \"roberta-large\", \n",
    "#     \"microsoft/deberta-base\",  \n",
    "#     \"microsoft/deberta-large\",\n",
    "#     \"microsoft/deberta-v3-large\",\n",
    "#     \"microsoft/deberta-xlarge\",\n",
    "#     \"google/electra-large-discriminator\"\n",
    "]):\n",
    "\n",
    "    tokenizer = get_tokenizer(name)\n",
    "    tokenizer.save_pretrained(OUT_PATH + f'{name.split(\"/\")[-1]}/tokenizers/')\n",
    "\n",
    "    config = AutoConfig.from_pretrained(name, output_hidden_states=True)\n",
    "    torch.save(config, OUT_PATH + f'{name.split(\"/\")[-1]}/' +'config.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = load_and_prepare_pretrain('../output/pl_6/df_pl.csv', root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('../output/pl_6/df_pl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "probs = np.load('../output/pl_6/probs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['soft_target'] = probs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['soft_target'] = df.apply(lambda x: np.array(x.soft_target[:len(x.pn_history)]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df['target'] = df['soft_target'].progress_apply(lambda x: np.array(x) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[df['pn_num'] < 10].reset_index()\n",
    "df_['span'] = df_['target'].progress_apply(char_target_to_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_annotation(df_, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../output/pl_5/df_pl.csv')\n",
    "# probs = np.zeros((len(df), 950))\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     if i % 25000 == 0:\n",
    "#         print(i)\n",
    "#     x = np.array(ast.literal_eval(re.sub('\\n', ',', df['probs'][i])))[:, 0]\n",
    "#     probs[i, :len(x)] = x\n",
    "    \n",
    "# np.save('../output/pl_5/probs.npy', probs)\n",
    "# df[['id', 'case_num', 'pn_num', 'feature_num', 'feature_text', 'pn_history',\n",
    "#        'ft_ref', 'text_ref']].to_csv('../output/pl_5/df_pl_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH + \"patient_notes.csv\")\n",
    "\n",
    "df['len'] = df['pn_history'].apply(len)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[df['len'] == 950].reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgm = df.groupby('case_num')['pn_num'].agg(lambda x: np.max(list(x)) % 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfgl = df.groupby('case_num')['pn_num'].agg(lambda x: len(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['pn_history'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['pn_history'].apply(lambda x: x.strip()).apply(clean_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df[df[\"clean_text\"].apply(lambda x: x[0]) !=  df[\"pn_history\"].apply(lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_prepare(root=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='pn_history', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['pn_history'].apply(len)\n",
    "\n",
    "# df = df[df['len'] == 950].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['pn_history'].apply(len)\n",
    "len(df[df['len'] == 950]['pn_history'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['len'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfd = df[df['pn_history'] != df['pn_history'].apply(lambda x: x.strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfd = dfd[dfd['target'].apply(lambda x: x[-5:].max() > 0)].reset_index(drop=True)\n",
    "dfd['len'] = dfd['pn_history'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfd.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv(OUT_PATH + \"folds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"microsoft/deberta-v3-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(name, precompute=True, df=df, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('family history of mi or family history of myocardial infarction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['pn_history'].apply(len)\n",
    "df = df[df['len'] == 950].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in df['pn_num'].unique():\n",
    "    plot_annotation(df, i)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = []\n",
    "\n",
    "# for i in tqdm(range(len(df))):\n",
    "#     lens.append(\n",
    "#         len(tokenizer(\n",
    "#             clean_spaces(df['feature_text'][i].lower()),\n",
    "#             clean_spaces(df['pn_history'][i].lower()),\n",
    "#         )['input_ids'])\n",
    "#     )\n",
    "# np.max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PatientNoteDataset(df, tokenizer, max_len=310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['pn_history'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.loader import define_loaders\n",
    "\n",
    "# train_loader, val_loader = define_loaders(dataset, dataset, val_bs=2)\n",
    "# for batch in val_loader:\n",
    "#     print(batch['offsets'], batch['offsets'].size())\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(len(dataset))):\n",
    "#     data = dataset[i]\n",
    "    \n",
    "#     assert len(data['text']) == np.max(data['offsets']), i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERTransformer(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
