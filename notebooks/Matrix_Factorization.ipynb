{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Generates candidates.\n",
    "\n",
    "**TODO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "from merlin.io import Dataset\n",
    "from torch.optim import SparseAdam\n",
    "from merlin.loader.torch import Loader\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.load import load_sessions\n",
    "from utils.metrics import get_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val\"\n",
    "NO_CLICKS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "#     files = glob.glob(\"../output/full_train_parquet/*\") + glob.glob(\n",
    "#         \"../output/val_parquet/*\"\n",
    "#     )\n",
    "    files = glob.glob(\"../output/full_train_2_parquet/*\") + glob.glob(\n",
    "        \"../output/val_2_parquet/*\"\n",
    "    )\n",
    "elif MODE == \"test\":\n",
    "    files = glob.glob(\"../output/full_train_val_parquet/*\") + glob.glob(\n",
    "        \"../output/test_parquet/*\"\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = cudf.concat([cudf.read_parquet(f) for f in files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NO_CLICKS:\n",
    "    train_pairs = train_pairs[train_pairs['type'] != \"clicks\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT = 1\n",
    "SHIFTS = None\n",
    "\n",
    "train_pairs['aid_next'] = train_pairs.groupby('session').aid.shift(-1 * SHIFT)\n",
    "train_pairs = train_pairs[['aid', 'aid_next']].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHIFTS =  [1, 2, 3, 4, 5]\n",
    "# SHIFT = \"1-5\"\n",
    "\n",
    "# train_pairs_ = []\n",
    "\n",
    "# for shift in tqdm(SHIFTS):\n",
    "#     train_pairs['aid_next'] = train_pairs.groupby('session').aid.shift(-1 * shift)\n",
    "#     train_pairs_.append(train_pairs[['aid', 'aid_next']].dropna().reset_index(drop=True).to_pandas())\n",
    "\n",
    "# train_pairs = cudf.from_pandas(pd.concat(train_pairs_, ignore_index=True).drop_duplicates(keep=\"first\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of pairs', numerize(len(train_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs.to_pandas().to_parquet(\n",
    "    f\"../output/matrix_factorization/{MODE}_pairs.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs.tail(10_000_000).to_parquet(\n",
    "    f\"../output/matrix_factorization/{MODE}_pairs_val.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n",
    "- TODO : Cart -> Buy / Buy -> Buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_aids, n_factors):\n",
    "        super().__init__()\n",
    "        self.aid_factors = nn.Embedding(n_aids, n_factors, sparse=True)\n",
    "\n",
    "    def forward(self, aid1, aid2):\n",
    "        aid1 = self.aid_factors(aid1)\n",
    "        aid2 = self.aid_factors(aid2)\n",
    "\n",
    "        return (aid1 * aid2).sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MatrixFactorization2(nn.Module):\n",
    "    def __init__(self, n_aids, n_factors):\n",
    "        super().__init__()\n",
    "        self.aid_factors = nn.Embedding(n_aids, n_factors, sparse=True)\n",
    "        self.aid_factors_next = nn.Embedding(n_aids, n_factors, sparse=True)\n",
    "\n",
    "    def forward(self, aid1, aid2):\n",
    "        aid1 = self.aid_factors(aid1)\n",
    "        aid2 = self.aid_factors_next(aid2)\n",
    "\n",
    "        return (aid1 * aid2).sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(f\"../output/matrix_factorization/{MODE}_pairs.parquet\")\n",
    "train_dl_merlin = Loader(train_ds, 65536, True)\n",
    "\n",
    "valid_ds = Dataset(f\"../output/matrix_factorization/{MODE}_pairs_val.parquet\")\n",
    "valid_dl_merlin = Loader(valid_ds, 65536, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 64\n",
    "\n",
    "N_AIDS = 1855602\n",
    "EPOCHS = 20\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MatrixFactorization(N_AIDS + 1, DIM)\n",
    "model = MatrixFactorization2(N_AIDS + 1, DIM)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "optimizer = SparseAdam(model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    for batch, _ in train_dl_merlin:\n",
    "        model.train()\n",
    "        losses = AverageMeter(\"Loss\", \":.4e\")\n",
    "\n",
    "        aid1, aid2 = batch[\"aid\"], batch[\"aid_next\"]\n",
    "        aid1 = aid1.to(\"cuda\")\n",
    "        aid2 = aid2.to(\"cuda\")\n",
    "        output_pos = model(aid1, aid2)\n",
    "        output_neg = model(aid1, aid2[torch.randperm(aid2.shape[0])])\n",
    "\n",
    "        output = torch.cat([output_pos, output_neg])\n",
    "        targets = torch.cat([torch.ones_like(output_pos), torch.zeros_like(output_pos)])\n",
    "        loss = criterion(output, targets)\n",
    "        losses.update(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        accuracy = AverageMeter(\"accuracy\")\n",
    "        for batch, _ in valid_dl_merlin:\n",
    "            aid1, aid2 = batch[\"aid\"], batch[\"aid_next\"]\n",
    "            output_pos = model(aid1, aid2)\n",
    "            output_neg = model(aid1, aid2[torch.randperm(aid2.shape[0])])\n",
    "            accuracy_batch = (\n",
    "                torch.cat([output_pos.sigmoid() > 0.5, output_neg.sigmoid() < 0.5])\n",
    "                .float()\n",
    "                .mean()\n",
    "            )\n",
    "            accuracy.update(accuracy_batch, aid1.shape[0])\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS} \\t loss={losses.avg:.3f} \\t val_acc={accuracy.avg:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = model.aid_factors.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "# name = f\"embed_{SHIFT}_{DIM}{'_cartbuy' if NO_CLICKS else ''}_{MODE}.npy\"\n",
    "# np.save(f\"../output/matrix_factorization/{name}\", embeddings)\n",
    "\n",
    "# print(\n",
    "#     f\"Saved matrix of shape {embeddings.shape} to\",\n",
    "#     f\"../output/matrix_factorization/{name}\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.aid_factors.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "name = f\"embed_{SHIFT}_{DIM}{'_cartbuy' if NO_CLICKS else ''}_prev_{MODE}.npy\"\n",
    "np.save(f\"../output/matrix_factorization/{name}\", embeddings)\n",
    "\n",
    "print(\n",
    "    f\"Saved matrix of shape {embeddings.shape} to\",\n",
    "    f\"../output/matrix_factorization/{name}\",\n",
    ")\n",
    "\n",
    "embeddings = model.aid_factors_next.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "name = f\"embed_{SHIFT}_{DIM}{'_cartbuy' if NO_CLICKS else ''}_next_{MODE}.npy\"\n",
    "np.save(f\"../output/matrix_factorization/{name}\", embeddings)\n",
    "\n",
    "print(\n",
    "    f\"Saved matrix of shape {embeddings.shape} to\",\n",
    "    f\"../output/matrix_factorization/{name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    REGEX = \"../output/val_parquet/*\"\n",
    "elif MODE == \"test\":\n",
    "    REGEX = \"../output/test_parquet/*\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "N_NEIGHBORS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "\n",
    "def find_matches(preds, df, n_neighbors=100):\n",
    "    \n",
    "\n",
    "    matcher.fit(preds)\n",
    "\n",
    "    dists, indices = matcher.kneighbors(preds)\n",
    "\n",
    "    ids = df.index[indices.flatten()].values.reshape(-1, n_neighbors)\n",
    "    df[\"matches\"] = list(ids)\n",
    "    matches = df[[\"matches\"]].to_dict(orient=\"dict\")[\"matches\"]\n",
    "    for k in matches:\n",
    "        matches[k] = [m for m in matches[k] if m != k]\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prev = np.load(\"../output/matrix_factorization/embed_1_64_val.npy\")\n",
    "x_next = np.load(\"../output/matrix_factorization/embed_1_64_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = cuml.neighbors.NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\n",
    "matcher.fit(x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_sessions(REGEX)\n",
    "df = df.sort_values(['session', 'ts']).groupby('session').agg('last').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prev = x_prev[df['aid'].to_pandas().values]\n",
    "dists, indices = matcher.kneighbors(x_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['candidates'] = indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = load_sessions(REGEX)\n",
    "df_ = df_.sort_values(['session', 'ts'], ascending=[True, False]).groupby('session').agg(list).reset_index()\n",
    "\n",
    "df['aids'] = df_['aid'].to_pandas().values\n",
    "df['candidates'] = df.apply(lambda x: list(x.candidates) + list(x.aids), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE != \"test\":\n",
    "    recalls = []\n",
    "    gt = pd.read_parquet(f\"../output/val_labels.parquet\")\n",
    "\n",
    "    for col in CLASSES:\n",
    "        if f\"gt_{col}\" not in df.columns:\n",
    "            df = df.merge(\n",
    "                gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\"\n",
    "            ).rename(columns={\"ground_truth\": f\"gt_{col}\"})\n",
    "\n",
    "        n_preds, n_gts, n_found = get_coverage(\n",
    "            df[\"candidates\"].values, df[f\"gt_{col}\"].values\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"- {col} \\t- Found {numerize(n_found)} GTs with {numerize(n_preds)} candidates (pos_prop={n_found / n_preds * 100 :.2f}%)\\t-  Highest reachable Recall : {n_found / n_gts :.4f}\"\n",
    "        )\n",
    "        recalls.append(n_found / n_gts)\n",
    "\n",
    "    cv = np.average(recalls, weights=WEIGHTS)\n",
    "    print(f\"\\n-> Highest reachable CV : {cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+0\n",
    "- clicks \t- Found 568.89K GTs with 7.7M candidates (pos_prop=7.39%)\t-  Highest reachable Recall : 0.3241\n",
    "- carts \t- Found 181.67K GTs with 7.7M candidates (pos_prop=2.36%)\t-  Highest reachable Recall : 0.3154\n",
    "- orders \t- Found 187K GTs with 7.7M candidates (pos_prop=2.43%)\t-  Highest reachable Recall : 0.5970\n",
    "\n",
    "+10\n",
    "- clicks \t- Found 583.5K GTs with 25.71M candidates (pos_prop=2.27%)\t-  Highest reachable Recall : 0.3324\n",
    "- carts \t- Found 184.66K GTs with 25.71M candidates (pos_prop=0.72%)\t-  Highest reachable Recall : 0.3206\n",
    "- orders \t- Found 187.44K GTs with 25.71M candidates (pos_prop=0.73%)\t-  Highest reachable Recall : 0.5984\n",
    "\n",
    "+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, test=False):\n",
    "    if \"aid\" in df.columns:\n",
    "        df.drop([\"aid\", \"type\"], axis=1, inplace=True)\n",
    "\n",
    "    df = cudf.from_pandas(df)\n",
    "    df = df.explode(\"candidates\")\n",
    "    df = df.drop_duplicates(keep=\"first\", subset=[\"session\", \"candidates\"])\n",
    "\n",
    "    df[\"candidates\"] = df[\"candidates\"].astype(\"uint32\")\n",
    "    df[\"session\"] = df[\"session\"].astype(\"uint32\")\n",
    "\n",
    "    df = df.sort_values([\"session\", \"candidates\"]).reset_index(drop=True)\n",
    "\n",
    "    if not test:\n",
    "        for col in [\"gt_clicks\", \"gt_carts\", \"gt_orders\"]:\n",
    "            df_tgt = (\n",
    "                df[[\"session\", \"candidates\", col]].explode(col).reset_index(drop=True)\n",
    "            ).fillna(-1)\n",
    "            df_tgt[col] = df_tgt[col].astype(\"int64\") == df_tgt[\"candidates\"].astype(\n",
    "                \"int64\"\n",
    "            )\n",
    "\n",
    "            assert not df_tgt.isna().any().max()\n",
    "\n",
    "            df_tgt = df_tgt.groupby([\"session\", \"candidates\"]).max().reset_index()\n",
    "            df_tgt = df_tgt.sort_values([\"session\", \"candidates\"]).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            assert not df_tgt.isna().any().max()\n",
    "\n",
    "            df[col] = df_tgt[col].astype(\"uint8\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_candids = explode(df, test=(MODE == \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_candids.to_parquet(\n",
    "#     f\"../output/candidates/candidates_matrix_factorization_{MODE}.parquet\", index=False\n",
    "# )\n",
    "# print(f\"Saved to ../output/candidates/candidates_matrix_factorization_{MODE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
