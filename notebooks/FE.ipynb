{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Computes Features.\n",
    "\n",
    "**TODO**:\n",
    "- not leaky matrices\n",
    "- not leaky tgt enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import sys\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import numba\n",
    "import pickle\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from data.covisitation import compute_covisitation_matrix\n",
    "from data.candidates import load_parquets, create_candidates, explode\n",
    "\n",
    "from utils.metrics import get_coverage\n",
    "from utils.chris import suggest_clicks, suggest_buys, read_file_to_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sessions(regex):\n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob(regex)):\n",
    "        chunk = cudf.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts / 1000).astype(\"int32\")\n",
    "        chunk[\"type\"] = chunk[\"type\"].map(TYPE_LABELS).astype(\"int8\")\n",
    "        dfs.append(chunk)\n",
    "    \n",
    "    return cudf.concat(dfs).sort_values(['session', 'aid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"val_c\"\n",
    "SUFFIX = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    CANDIDATE_FILE = '../output/candidates_val_592.parquet'\n",
    "    PARQUET_FILES = \"../output/val_parquet/*\"\n",
    "elif MODE == \"val_c\":\n",
    "    CANDIDATE_FILE = '../output/candidates_val_c_592.parquet'\n",
    "    PARQUET_FILES = \"../output/val_c_parquet/*\"\n",
    "else:  # train\n",
    "    CANDIDATE_FILE = '../output/candidates_train_592.parquet'\n",
    "    PARQUET_FILES = \"../output/train_parquet/*\"\n",
    "    \n",
    "pairs = cudf.read_parquet(CANDIDATE_FILE)\n",
    "pairs = pairs.sort_values(['session', 'candidates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(sessions):\n",
    "    sessions.sort_values(['session', \"ts\"], ascending=[True, False]).reset_index(drop=True)\n",
    "    \n",
    "    sessions['w'] = sessions.groupby('session')['aid'].cumcount()\n",
    "    \n",
    "    sessions = sessions.merge(\n",
    "        cudf.DataFrame(sessions.groupby('session')['aid'].size()),\n",
    "        on=\"session\",\n",
    "        how=\"left\"\n",
    "    ).rename(columns={0: \"n\"})\n",
    "    \n",
    "    sessions[\"logspace_w\"] = sessions.apply(\n",
    "        lambda x : 1 if x.n == 1 else 2 ** (0.1 + 0.9 * (x.n - x.w - 1) / (x.n - 1)) - 1,\n",
    "        axis=1\n",
    "    )\n",
    "    sessions[\"linspace_w\"] = sessions['w'].apply(\n",
    "        lambda x : 0.05 if x >= 20 else 0.1 + 0.9 * (18 - x) / 18\n",
    "    )\n",
    "    \n",
    "    weights = sessions[[\"session\", \"aid\", \"logspace_w\", \"linspace_w\"]].groupby(['session', 'aid']).sum().reset_index()\n",
    "    \n",
    "    weights = weights.sort_values(['session', \"aid\"]).reset_index(drop=True).rename(columns={\"aid\": \"candidates\"})\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "weights = compute_weights(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.merge(weights, how=\"left\", on=[\"session\", \"candidates\"])\n",
    "pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)\n",
    "\n",
    "pairs['logspace_w'] = pairs['logspace_w'].fillna(pairs[\"logspace_w\"].min() / 2).astype(\"float32\")\n",
    "pairs['linspace_w'] = pairs['linspace_w'].fillna(pairs[\"linspace_w\"].min() / 2).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sessions\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covisitation features\n",
    "TODO :\n",
    "- time weighted agg, agg last n\n",
    "- merge rank in matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coocurence_features(pairs, matrix_file, weights):\n",
    "    pairs['group'] = pairs['session'] // 100000\n",
    "    \n",
    "    weights = weights.rename(columns={\"candidates\": \"aid\"})\n",
    "\n",
    "    mat = cudf.read_parquet(matrix_file)\n",
    "    mat.columns = ['aid', 'candidates', 'w']\n",
    "\n",
    "    fts = []\n",
    "    for _, df in tqdm(pairs.groupby('group')):\n",
    "        df = df[['session', 'candidates', 'aid']].explode('aid').reset_index(drop=True)\n",
    "\n",
    "        df = df.merge(mat, how=\"left\", on=[\"aid\", \"candidates\"]).reset_index().fillna(0)\n",
    "\n",
    "        df = df.merge(weights, how=\"left\", on=[\"session\", \"aid\"])\n",
    "        df['logspace_w'] *= df['w']\n",
    "        df['linspace_w'] *= df['w']\n",
    "\n",
    "        df = df[['candidates', 'session', 'w', 'logspace_w', 'linspace_w']].groupby(['session', 'candidates']).agg([\"mean\", \"sum\", \"max\"])\n",
    "        df.columns = ['_'.join(col) for col in df.columns.values]\n",
    "\n",
    "        df[df.columns] = df[df.columns].astype(\"float32\")\n",
    "        fts.append(df.reset_index())\n",
    "\n",
    "    fts = cudf.concat(fts, ignore_index=True)\n",
    "    fts = fts.sort_values(['session', 'candidates']).reset_index(drop=True)\n",
    "\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATRIX_FOLDER = \"../output/matrices/\"\n",
    "MATRIX_NAMES = [\"matrix_123_temporal_20\", \"matrix_123_type136_20\", \"matrix_12__20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "\n",
    "sessions = sessions.sort_values(['session', \"aid\"]).groupby('session').agg(list).reset_index()\n",
    "pairs = pairs.merge(sessions[[\"session\", \"aid\"]], how=\"left\", on=\"session\")\n",
    "pairs = pairs.sort_values(['session', 'candidates']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in MATRIX_NAMES:\n",
    "    print(f' -> Features from {name}')\n",
    "\n",
    "    fts = compute_coocurence_features(\n",
    "        pairs[['session', 'candidates', 'aid']],\n",
    "        os.path.join(MATRIX_FOLDER, name + \".pqt\"),\n",
    "        weights\n",
    "    )\n",
    "    \n",
    "    for c in fts.columns[2:]:\n",
    "        pairs[f\"{name}_{re.sub('w_', '', c)}\"] = fts[c].values\n",
    "\n",
    "    del fts\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()\n",
    "    \n",
    "pairs.drop('aid', axis=1, inplace=True)\n",
    "\n",
    "del sessions, weights\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity\n",
    "TODO :\n",
    "- Popularity of items in session\n",
    "- Popularity over different periods  (day / month)\n",
    "- Time weighted popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "\n",
    "for i, c in enumerate(CLASSES):\n",
    "    print(f'-> Popularity for {c}')\n",
    "    popularity = cudf.DataFrame(sessions.loc[sessions[\"type\"] == i, \"aid\"].value_counts()).reset_index()\n",
    "    popularity.columns = ['candidates', f'{c}_popularity']\n",
    "    popularity[f'{c}_popularity'] = np.clip(popularity[f'{c}_popularity'], 0, 2 ** 16 - 1).astype(\"uint16\")\n",
    "\n",
    "    pairs = pairs.merge(popularity, how=\"left\", on=\"candidates\").fillna(0)\n",
    "\n",
    "del sessions, popularity\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session features\n",
    "- Count views/clicks/carts/orders of session\n",
    "- Count views/clicks/carts/orders of each candidate\n",
    "\n",
    "TODO :\n",
    "- Distance to last view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_actions(pairs, sessions):\n",
    "    pairs = pairs.merge(sessions[[\"session\", \"aid\"]], how=\"left\", on=\"session\")\n",
    "    pairs['group'] = pairs['session'] // 100000\n",
    "\n",
    "    fts = []\n",
    "    for _, df in tqdm(pairs.groupby('group')):\n",
    "        df = df[['session', 'candidates', 'aid']].explode('aid')\n",
    "        df['aid'] = (df['aid'] == df['candidates']).astype(np.uint16)\n",
    "\n",
    "        df = df.groupby(\n",
    "            [\"session\", \"candidates\"]\n",
    "        ).sum().reset_index()\n",
    "        \n",
    "        fts.append(df)\n",
    "    \n",
    "    ft = cudf.concat(fts, ignore_index=True)\n",
    "    ft = ft.sort_values(['session', 'candidates'])['aid'].values\n",
    "\n",
    "    return np.clip(ft, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(CLASSES + [\"*\"]):\n",
    "    print(f'-> Candidate {c if c != \"*\" else \"views\"} in session')\n",
    "\n",
    "    sessions = load_sessions(PARQUET_FILES)\n",
    "    if c != \"*\":\n",
    "        sessions.loc[sessions[\"type\"] != i, \"aid\"] = -1\n",
    "\n",
    "    sessions = sessions.groupby('session').agg(list).reset_index()\n",
    "\n",
    "    pairs[f'candidate_{c}_before'] = count_actions(\n",
    "        pairs[['session', 'candidates']],\n",
    "        sessions\n",
    "    )\n",
    "    \n",
    "    del sessions\n",
    "    numba.cuda.current_context().deallocations.clear()\n",
    "    gc.collect()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = load_sessions(PARQUET_FILES)\n",
    "\n",
    "n_views = sessions[['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_views\"})\n",
    "n_clicks = sessions[sessions['type'] == 0][['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_clicks\"})\n",
    "n_carts = sessions[sessions['type'] == 1][['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_carts\"})\n",
    "n_orders = sessions[sessions['type'] == 2][['session', 'ts']].groupby('session').count().reset_index().rename(columns={\"ts\": \"n_orders\"})\n",
    "\n",
    "# sessions = sessions.merge(n_views, how=\"left\", on=\"session\").fillna(0)\n",
    "sessions_fts = n_views.merge(n_clicks, how=\"left\", on=\"session\").fillna(0)\n",
    "sessions_fts = sessions_fts.merge(n_carts, how=\"left\", on=\"session\").fillna(0)\n",
    "sessions_fts = sessions_fts.merge(n_orders, how=\"left\", on=\"session\").fillna(0)\n",
    "\n",
    "for c in sessions_fts.columns[1:]:\n",
    "    sessions_fts[c] = np.clip(sessions_fts[c], 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.merge(sessions_fts, on=\"session\", how=\"left\")\n",
    "pairs = pairs.sort_values(['session', 'candidates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_by_chunks(pairs, folder):\n",
    "    print(f'-> Saving chunks to {folder}')\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "    pairs['group'] = pairs['session'] // 100000\n",
    "\n",
    "    for i, (_, df) in enumerate(tqdm(pairs.groupby('group'))):\n",
    "        df.drop('group', axis=1, inplace=True)\n",
    "        df.to_parquet(os.path.join(folder, f'{i:03d}.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_by_chunks(pairs, f\"../output/fts_{MODE}_{SUFFIX}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.to_csv(\n",
    "#     f\"../output/fts_{MODE}_{SUFFIX}.csv\", index=False, chunksize=100000\n",
    "# )\n",
    "# print(f\"Saved to ../output/fts_{MODE}_{SUFFIX}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs.to_pandas().to_parquet(\n",
    "#     f\"../output/fts_{MODE}_{SUFFIX}.parquet\", index=False\n",
    "# )\n",
    "# print(f\"Saved to ../output/fts_{MODE}_{SUFFIX}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
