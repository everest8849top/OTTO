{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About :** Trains XGBoost models.\n",
    "\n",
    "**TODO**:\n",
    "- better neg sampling technique ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cudf\n",
    "import json\n",
    "import glob\n",
    "import numba\n",
    "import xgboost\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from numerize.numerize import numerize\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.metrics import get_coverage\n",
    "from utils.plot import plot_importances\n",
    "from utils.load import *\n",
    "from utils.logger import save_config, prepare_log_folder, create_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"c-orders-v4.7\"\n",
    "GT_VERSION = \"gt.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data\n",
    "- neg sampling could use candidates from lower versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_RATIO = 0.2\n",
    "TARGET = \"gt_orders\"   # \"gt_clicks\", \"gt_carts\", \"gt_orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_sessions(regex):\n",
    "#     dfs = []\n",
    "#     for idx, chunk_file in enumerate(glob.glob(regex)):\n",
    "#         df = cudf.read_parquet(chunk_file, columns=[\"session\"])\n",
    "#         dfs.append(df.drop_duplicates(keep=\"first\"))\n",
    "\n",
    "#     return cudf.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "# sessions = load_sessions( f\"../output/features/fts_val_{VERSION}/*\")\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# K = 4\n",
    "\n",
    "# kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "# splits = kf.split(sessions)\n",
    "\n",
    "# sessions['fold'] = -1\n",
    "# for i, (_, val_idx) in enumerate(splits):\n",
    "#     sessions.loc[val_idx, \"fold\"] = i\n",
    "\n",
    "# sessions.to_csv(f\"../input/folds_{K}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [  # REMOVE CORRELATED\n",
    "    'clicks_popularity_w_pos-log', 'clicks_popularity_w_type-163', 'clicks_popularity_w_lastday', 'clicks_popularity_w_recsys', \n",
    "    'carts_popularity_w_pos-log', 'carts_popularity_w_type-163', 'carts_popularity_w_lastday', 'carts_popularity_w_recsys', \n",
    "    'orders_popularity_w_pos-log', 'orders_popularity_w_type-163', 'orders_popularity_w_lastday', 'orders_popularity_w_recsys', \n",
    "    'clicks_popularity_w_pos-log_w', 'clicks_popularity_w_type-163_w', 'clicks_popularity_w_recsys_w', \n",
    "    'carts_popularity_w_pos-log_w', 'carts_popularity_w_type-163_w', 'carts_popularity_w_recsys_w', \n",
    "    'orders_popularity_w_pos-log_w', 'orders_popularity_w_type-163_w', 'orders_popularity_w_recsys_w',\n",
    "    'w_pos-log', 'w_type-163', 'w_lastday', 'w_time', 'w_recsys',\n",
    "    'matrix_123_temporal_20_mean', 'matrix_123_temporal_20_sum', 'matrix_123_temporal_20_max', 'matrix_123_temporal_20_pos-log_mean', 'matrix_123_temporal_20_pos-log_sum', 'matrix_123_temporal_20_pos-log_max', 'matrix_123_temporal_20_type-163_mean', 'matrix_123_temporal_20_type-163_sum', 'matrix_123_temporal_20_type-163_max', 'matrix_123_temporal_20_lastday_mean', 'matrix_123_temporal_20_lastday_sum', 'matrix_123_temporal_20_lastday_max', 'matrix_123_temporal_20_time_mean', 'matrix_123_temporal_20_time_sum', 'matrix_123_temporal_20_time_max', 'matrix_123_temporal_20_recsys_mean', 'matrix_123_temporal_20_recsys_sum', 'matrix_123_temporal_20_recsys_max',\n",
    "    'matrix_123_type136_20_mean', 'matrix_123_type136_20_sum', 'matrix_123_type136_20_max', 'matrix_123_type136_20_pos-log_mean', 'matrix_123_type136_20_pos-log_sum', 'matrix_123_type136_20_pos-log_max', 'matrix_123_type136_20_type-163_mean', 'matrix_123_type136_20_type-163_sum', 'matrix_123_type136_20_type-163_max', 'matrix_123_type136_20_lastday_mean', 'matrix_123_type136_20_lastday_sum', 'matrix_123_type136_20_lastday_max', 'matrix_123_type136_20_time_mean', 'matrix_123_type136_20_time_sum', 'matrix_123_type136_20_time_max', 'matrix_123_type136_20_recsys_mean', 'matrix_123_type136_20_recsys_sum', 'matrix_123_type136_20_recsys_max',\n",
    "    'matrix_12__20_mean', 'matrix_12__20_sum', 'matrix_12__20_max', 'matrix_12__20_pos-log_mean', 'matrix_12__20_pos-log_sum', 'matrix_12__20_pos-log_max', 'matrix_12__20_type-163_mean', 'matrix_12__20_type-163_sum', 'matrix_12__20_type-163_max', 'matrix_12__20_lastday_mean', 'matrix_12__20_lastday_sum', 'matrix_12__20_lastday_max', 'matrix_12__20_time_mean', 'matrix_12__20_time_sum', 'matrix_12__20_time_max', 'matrix_12__20_recsys_mean', 'matrix_12__20_recsys_sum', 'matrix_12__20_recsys_max',\n",
    "    'matrix_123_type0.590.5_20_mean', 'matrix_123_type0.590.5_20_sum', 'matrix_123_type0.590.5_20_max', 'matrix_123_type0.590.5_20_pos-log_mean', 'matrix_123_type0.590.5_20_pos-log_sum', 'matrix_123_type0.590.5_20_pos-log_max', 'matrix_123_type0.590.5_20_type-163_mean', 'matrix_123_type0.590.5_20_type-163_sum', 'matrix_123_type0.590.5_20_type-163_max', 'matrix_123_type0.590.5_20_lastday_mean', 'matrix_123_type0.590.5_20_lastday_sum', 'matrix_123_type0.590.5_20_lastday_max', 'matrix_123_type0.590.5_20_time_mean', 'matrix_123_type0.590.5_20_time_sum', 'matrix_123_type0.590.5_20_time_max', 'matrix_123_type0.590.5_20_recsys_mean', 'matrix_123_type0.590.5_20_recsys_sum', 'matrix_123_type0.590.5_20_recsys_max',\n",
    "    'matrix_cpu-90_mean', 'matrix_cpu-90_sum', 'matrix_cpu-90_max', 'matrix_cpu-90_pos-log_mean', 'matrix_cpu-90_pos-log_sum', 'matrix_cpu-90_pos-log_max', 'matrix_cpu-90_type-163_mean', 'matrix_cpu-90_type-163_sum', 'matrix_cpu-90_type-163_max', 'matrix_cpu-90_lastday_mean', 'matrix_cpu-90_lastday_sum', 'matrix_cpu-90_lastday_max', 'matrix_cpu-90_time_mean', 'matrix_cpu-90_time_sum', 'matrix_cpu-90_time_max', 'matrix_cpu-90_recsys_mean', 'matrix_cpu-90_recsys_sum', 'matrix_cpu-90_recsys_max',\n",
    "    'matrix_cpu-95_mean', 'matrix_cpu-95_sum', 'matrix_cpu-95_max', 'matrix_cpu-95_pos-log_mean', 'matrix_cpu-95_pos-log_sum', 'matrix_cpu-95_pos-log_max', 'matrix_cpu-95_type-163_mean', 'matrix_cpu-95_type-163_sum', 'matrix_cpu-95_type-163_max', 'matrix_cpu-95_lastday_mean', 'matrix_cpu-95_lastday_sum', 'matrix_cpu-95_lastday_max', 'matrix_cpu-95_time_mean', 'matrix_cpu-95_time_sum', 'matrix_cpu-95_time_max', 'matrix_cpu-95_recsys_mean', 'matrix_cpu-95_recsys_sum', 'matrix_cpu-95_recsys_max',\n",
    "    'matrix_cpu-99_mean', 'matrix_cpu-99_sum', 'matrix_cpu-99_max', 'matrix_cpu-99_pos-log_mean', 'matrix_cpu-99_pos-log_sum', 'matrix_cpu-99_pos-log_max', 'matrix_cpu-99_type-163_mean', 'matrix_cpu-99_type-163_sum', 'matrix_cpu-99_type-163_max', 'matrix_cpu-99_lastday_mean', 'matrix_cpu-99_lastday_sum', 'matrix_cpu-99_lastday_max', 'matrix_cpu-99_time_mean', 'matrix_cpu-99_time_sum', 'matrix_cpu-99_time_max', 'matrix_cpu-99_recsys_mean', 'matrix_cpu-99_recsys_sum', 'matrix_cpu-99_recsys_max',\n",
    "    'matrix_gpu-116_mean', 'matrix_gpu-116_sum', 'matrix_gpu-116_max', 'matrix_gpu-116_pos-log_mean', 'matrix_gpu-116_pos-log_sum', 'matrix_gpu-116_pos-log_max', 'matrix_gpu-116_type-163_mean', 'matrix_gpu-116_type-163_sum', 'matrix_gpu-116_type-163_max', 'matrix_gpu-116_lastday_mean', 'matrix_gpu-116_lastday_sum', 'matrix_gpu-116_lastday_max', 'matrix_gpu-116_time_mean', 'matrix_gpu-116_time_sum', 'matrix_gpu-116_time_max', 'matrix_gpu-116_recsys_mean', 'matrix_gpu-116_recsys_sum', 'matrix_gpu-116_recsys_max',\n",
    "    'matrix_gpu-115_mean', 'matrix_gpu-115_sum', 'matrix_gpu-115_max', 'matrix_gpu-115_pos-log_mean', 'matrix_gpu-115_pos-log_sum', 'matrix_gpu-115_pos-log_max', 'matrix_gpu-115_type-163_mean', 'matrix_gpu-115_type-163_sum', 'matrix_gpu-115_type-163_max', 'matrix_gpu-115_lastday_mean', 'matrix_gpu-115_lastday_sum', 'matrix_gpu-115_lastday_max', 'matrix_gpu-115_time_mean', 'matrix_gpu-115_time_sum', 'matrix_gpu-115_time_max', 'matrix_gpu-115_recsys_mean', 'matrix_gpu-115_recsys_sum', 'matrix_gpu-115_recsys_max',\n",
    "    'matrix_gpu-93_mean', 'matrix_gpu-93_sum', 'matrix_gpu-93_max', 'matrix_gpu-93_pos-log_mean', 'matrix_gpu-93_pos-log_sum', 'matrix_gpu-93_pos-log_max', 'matrix_gpu-93_type-163_mean', 'matrix_gpu-93_type-163_sum', 'matrix_gpu-93_type-163_max', 'matrix_gpu-93_lastday_mean', 'matrix_gpu-93_lastday_sum', 'matrix_gpu-93_lastday_max', 'matrix_gpu-93_time_mean', 'matrix_gpu-93_time_sum', 'matrix_gpu-93_time_max', 'matrix_gpu-93_recsys_mean', 'matrix_gpu-93_recsys_sum', 'matrix_gpu-93_recsys_max',\n",
    "    'matrix_gpu-217_mean', 'matrix_gpu-217_sum', 'matrix_gpu-217_max', 'matrix_gpu-217_pos-log_mean', 'matrix_gpu-217_pos-log_sum', 'matrix_gpu-217_pos-log_max', 'matrix_gpu-217_type-163_mean', 'matrix_gpu-217_type-163_sum', 'matrix_gpu-217_type-163_max', 'matrix_gpu-217_lastday_mean', 'matrix_gpu-217_lastday_sum', 'matrix_gpu-217_lastday_max', 'matrix_gpu-217_time_mean', 'matrix_gpu-217_time_sum', 'matrix_gpu-217_time_max', 'matrix_gpu-217_recsys_mean', 'matrix_gpu-217_recsys_sum', 'matrix_gpu-217_recsys_max',\n",
    "    'matrix_gpu-226_mean','matrix_gpu-226_sum','matrix_gpu-226_max','matrix_gpu-226_pos-log_mean','matrix_gpu-226_pos-log_sum','matrix_gpu-226_pos-log_max','matrix_gpu-226_type-163_mean','matrix_gpu-226_type-163_sum','matrix_gpu-226_type-163_max','matrix_gpu-226_lastday_mean','matrix_gpu-226_lastday_sum','matrix_gpu-226_lastday_max','matrix_gpu-226_time_mean','matrix_gpu-226_time_sum','matrix_gpu-226_time_max','matrix_gpu-226_recsys_mean','matrix_gpu-226_recsys_sum','matrix_gpu-226_recsys_max',\n",
    "    'matrix_gpu-232_mean', 'matrix_gpu-232_sum', 'matrix_gpu-232_max', 'matrix_gpu-232_pos-log_mean', 'matrix_gpu-232_pos-log_sum', 'matrix_gpu-232_pos-log_max', 'matrix_gpu-232_type-163_mean', 'matrix_gpu-232_type-163_sum', 'matrix_gpu-232_type-163_max', 'matrix_gpu-232_lastday_mean', 'matrix_gpu-232_lastday_sum', 'matrix_gpu-232_lastday_max', 'matrix_gpu-232_time_mean', 'matrix_gpu-232_time_sum', 'matrix_gpu-232_time_max', 'matrix_gpu-232_recsys_mean', 'matrix_gpu-232_recsys_sum', 'matrix_gpu-232_recsys_max',\n",
    "    'matrix_gpu-239_mean', 'matrix_gpu-239_sum', 'matrix_gpu-239_max', 'matrix_gpu-239_pos-log_mean', 'matrix_gpu-239_pos-log_sum', 'matrix_gpu-239_pos-log_max', 'matrix_gpu-239_type-163_mean', 'matrix_gpu-239_type-163_sum', 'matrix_gpu-239_type-163_max', 'matrix_gpu-239_lastday_mean', 'matrix_gpu-239_lastday_sum', 'matrix_gpu-239_lastday_max', 'matrix_gpu-239_time_mean', 'matrix_gpu-239_time_sum', 'matrix_gpu-239_time_max', 'matrix_gpu-239_recsys_mean', 'matrix_gpu-239_recsys_sum', 'matrix_gpu-239_recsys_max',\n",
    "    'matrix_gpu-700_mean', 'matrix_gpu-700_sum', 'matrix_gpu-700_max', 'matrix_gpu-700_pos-log_mean', 'matrix_gpu-700_pos-log_sum', 'matrix_gpu-700_pos-log_max', 'matrix_gpu-700_type-163_mean', 'matrix_gpu-700_type-163_sum', 'matrix_gpu-700_type-163_max', 'matrix_gpu-700_lastday_mean', 'matrix_gpu-700_lastday_sum', 'matrix_gpu-700_lastday_max', 'matrix_gpu-700_time_mean', 'matrix_gpu-700_time_sum', 'matrix_gpu-700_time_max', 'matrix_gpu-700_recsys_mean', 'matrix_gpu-700_recsys_sum', 'matrix_gpu-700_recsys_max',\n",
    "    'matrix_gpu-701_mean', 'matrix_gpu-701_sum', 'matrix_gpu-701_max', 'matrix_gpu-701_pos-log_mean', 'matrix_gpu-701_pos-log_sum', 'matrix_gpu-701_pos-log_max', 'matrix_gpu-701_type-163_mean', 'matrix_gpu-701_type-163_sum', 'matrix_gpu-701_type-163_max', 'matrix_gpu-701_lastday_mean', 'matrix_gpu-701_lastday_sum', 'matrix_gpu-701_lastday_max', 'matrix_gpu-701_time_mean', 'matrix_gpu-701_time_sum', 'matrix_gpu-701_time_max', 'matrix_gpu-701_recsys_mean', 'matrix_gpu-701_recsys_sum', 'matrix_gpu-701_recsys_max',\n",
    "    'candidate_clicks_before', 'candidate_carts_before', 'candidate_orders_before', 'candidate_*_before', 'n_views', 'n_clicks', 'n_carts', 'n_orders',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_REMOVE = []\n",
    "TO_REMOVE = [f for f in FEATURES if \"type-191\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if \"matrix_gpu-220\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if \"matrix_gpu-235\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if \"lasthour\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if \"lastmin\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if f.startswith(\"popularity\")]\n",
    "TO_REMOVE += [f for f in FEATURES if \"_old\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if \"popularity_w_time\" in f]\n",
    "TO_REMOVE += [f for f in FEATURES if \"popularity_w_lastday_w\" in f]\n",
    "\n",
    "FEATURES = [f for f in FEATURES if f not in TO_REMOVE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = load_parquets_cudf_folds(\n",
    "#     f\"../output/features/fts_val_{VERSION}/*\",\n",
    "#     \"../input/folds_4.csv\",\n",
    "#     fold=0,\n",
    "#     pos_ratio=POS_RATIO,\n",
    "#     target=TARGET,\n",
    "#     use_gt=USE_GT_SESSIONS,\n",
    "# #     max_n=5,\n",
    "#     train_only=True,\n",
    "#     columns=['session','candidates','gt_clicks','gt_carts','gt_orders'] + FEATURES,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = cudf.from_pandas(df_train)\n",
    "# corr = df_train[FEATURES].corr()\n",
    "# corr = corr.to_pandas()\n",
    "# corr = corr.values\n",
    "\n",
    "# mask = np.zeros_like(corr, dtype=bool)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "# corr[mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TH = 0.99\n",
    "\n",
    "# for i in range(len(corr)):\n",
    "#     for j in range(len(corr)):\n",
    "#         if corr[i, j] > TH:\n",
    "#             if FEATURES[i] in TO_REMOVE or FEATURES[j] in TO_REMOVE:\n",
    "#                 continue\n",
    "#             print(FEATURES[i], FEATURES[j], f'{corr[i, j] :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REGEX = f\"../output/features/fts_val_{VERSION}/*\"\n",
    "len(glob.glob(REGEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_REGEX = f\"../output/features/fts_test_{VERSION}/*\"\n",
    "len(glob.glob(TEST_REGEX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GT_REGEX = f\"../output/features/fts_val_{GT_VERSION}/*\"\n",
    "len(glob.glob(GT_REGEX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "import cuml\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numerize.numerize import numerize\n",
    "from utils.torch import seed_everything\n",
    "\n",
    "from model_zoo import TRAIN_FCTS, PREDICT_FCTS\n",
    "\n",
    "def train(df_train, val_regex, config, log_folder=None, optimize=False, fold=0, debug=False):\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    txt = f\"{'Optimizing' if optimize else 'Training'} {config.model.upper()} Model\"\n",
    "    print(f\"\\n-------------   {txt}   -------------\\n\")\n",
    "\n",
    "    if optimize:  # TODO\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        objective = lambda x: objective_xgb(x, df_train, val_regex, features, target)\n",
    "        study.optimize(objective, n_trials=50)\n",
    "        print(study.best_params)\n",
    "        return study.best_params\n",
    "\n",
    "    val_candids = sum([len(cudf.read_parquet(f, columns=['gt_orders'])) for f in glob.glob(val_regex)])\n",
    "    print(f\"    -> {numerize(len(df_train))} training candidates\")\n",
    "    print(f\"    -> {numerize(val_candids)} validation candidates\\n\")\n",
    "    \n",
    "    train_fct = TRAIN_FCTS[config.model]\n",
    "    df_val, model = train_fct(\n",
    "        df_train,\n",
    "        val_regex,\n",
    "        features=config.features,\n",
    "        target=config.target,\n",
    "        params=config.params,\n",
    "        use_es=config.use_es,\n",
    "        num_boost_round=config.num_boost_round,\n",
    "        folds_file=config.folds_file,\n",
    "        fold=fold,\n",
    "        debug=debug,\n",
    "    )\n",
    "\n",
    "    # Feature importance\n",
    "    if config.model == \"xgb\":\n",
    "        ft_imp = model.get_score()\n",
    "    else:\n",
    "        ft_imp = model.feature_importances_  # TODO\n",
    "    try:\n",
    "        ft_imp = pd.DataFrame(\n",
    "            pd.Series(ft_imp, index=config.features), columns=[\"importance\"]\n",
    "        )\n",
    "    except:\n",
    "        ft_imp = None\n",
    "        \n",
    "    if config.mode == \"test\":\n",
    "        return df_val, ft_imp, model\n",
    "\n",
    "    # Score\n",
    "    try:\n",
    "        auc = roc_auc_score(df_val[config.target], df_val[\"pred\"])\n",
    "    except:\n",
    "        auc = cuml.metrics.roc_auc_score(df_val[config.target].astype('int32'), df_val[\"pred\"].values)\n",
    "    \n",
    "    print(f'\\n -> AUC : {auc:.4f}\\n')\n",
    "\n",
    "    if log_folder is None:\n",
    "        return df_val, ft_imp, model\n",
    "\n",
    "    # Save model\n",
    "    if config.model == \"xgb\":\n",
    "        model.save_model(log_folder + f\"{config.model}_{fold}.json\")\n",
    "    elif config.model == \"lgbm\":\n",
    "        try:\n",
    "            model.booster_.save_model(log_folder + f\"{config.model}_{fold}.txt\")\n",
    "        except Exception:\n",
    "            model.save_model(log_folder + f\"{config.model}_{fold}.txt\")\n",
    "    else:   # catboost, verif\n",
    "        model.save_model(log_folder + f\"{config.model}_{fold}.txt\")\n",
    "\n",
    "    return df_val, ft_imp, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(regex, test_regex, config, log_folder, debug=False):\n",
    "    dfs_val, ft_imps, dfs_test = [], [], []\n",
    "    for fold in range(config.k):\n",
    "        print(f\"\\n-------------   Fold {fold + 1} / {config.k}  -------------\\n\")\n",
    "\n",
    "        df_train = load_parquets_cudf_folds(\n",
    "            regex,\n",
    "            config.folds_file,\n",
    "            fold=fold,\n",
    "            pos_ratio=config.pos_ratio,\n",
    "            target=config.target,\n",
    "            use_gt=config.use_gt_sessions,\n",
    "            train_only=True,\n",
    "            columns=['session', 'candidates', 'gt_clicks', 'gt_carts', 'gt_orders'] + config.features,\n",
    "            max_n=5 if debug else 0\n",
    "        )\n",
    "\n",
    "        if config.use_gt_pos:\n",
    "            df_train_gt = load_parquets_cudf_folds(\n",
    "                config.gt_regex,\n",
    "                config.folds_file,\n",
    "                fold=fold,\n",
    "                pos_ratio=-1,\n",
    "                target=config.target,\n",
    "                use_gt=config.use_gt_sessions,\n",
    "                train_only=True,\n",
    "                columns=['session', 'candidates', 'gt_clicks', 'gt_carts', 'gt_orders'] + config.features,\n",
    "                max_n=3 if debug else 0\n",
    "            )\n",
    "            df_train = pd.concat([df_train, df_train_gt], ignore_index=True)\n",
    "            df_train = df_train.drop_duplicates(subset=['session', 'candidates'], keep=\"first\").reset_index(drop=True)\n",
    "            \n",
    "        df_val, ft_imp, model = train(df_train, regex, config, log_folder=log_folder, fold=fold, debug=debug)\n",
    "        dfs_val.append(df_val)\n",
    "        ft_imps.append(ft_imp)\n",
    "        \n",
    "        try:\n",
    "            train_sessions = set(list(df_train[\"session\"].unique()))\n",
    "            val_sessions = set(list(df_val[\"session\"].unique().to_pandas()))\n",
    "            print('Train / val sess inter', len(train_sessions.intersection(val_sessions)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if log_folder is None:\n",
    "            return df_val, ft_imp, None\n",
    "\n",
    "        predict_fct = PREDICT_FCTS[config.model]\n",
    "        df_test = predict_fct(model, test_regex, config.features, debug=debug)\n",
    "        dfs_test.append(df_test)\n",
    "        \n",
    "        print('\\n -> Saving predictions \\n')\n",
    "        df_val[['session', 'candidates', 'pred']].to_parquet(log_folder + f\"df_val_{fold}.parquet\")\n",
    "        \n",
    "        df_test[['session', 'candidates']] = df_test[['session', 'candidates']].astype('int32')\n",
    "        df_test['pred'] = df_test['pred'].astype('float32')\n",
    "        df_test[['session', 'candidates', 'pred']].to_parquet(log_folder + f\"df_test_{fold}.parquet\")\n",
    "\n",
    "        del df_train, df_val, ft_imp, df_test, model\n",
    "        numba.cuda.current_context().deallocations.clear()\n",
    "        gc.collect()\n",
    "\n",
    "    dfs_test = cudf.concat(dfs_test).groupby(['session', 'candidates']).mean().reset_index()\n",
    "    dfs_val =  cudf.concat(dfs_val).sort_values(['session', 'candidates'], ignore_index=True)\n",
    "    ft_imps = pd.concat(ft_imps).reset_index().groupby('index').mean()\n",
    "\n",
    "    if log_folder is not None:\n",
    "        ft_imps.to_csv(log_folder + \"ft_imp.csv\")\n",
    "        dfs_test[['session', 'candidates', 'pred']].to_parquet(log_folder + f\"df_test.parquet\")\n",
    "\n",
    "    return dfs_val, ft_imps, dfs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"xgb\":\n",
    "    {\n",
    "        \"learning_rate\": 0.01,\n",
    "        'max_depth': 8,  # or 6\n",
    "        \"subsample\": 0.75,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'reg_alpha': 0.01,\n",
    "        'reg_lambda': 0.1,\n",
    "#         \"min_child_weight\": 0.01,\n",
    "#         \"gamma\": 0.01,\n",
    "#         'scale_pos_weight': 1,\n",
    "        'eval_metric': 'auc',\n",
    "        'objective': 'rank:pairwise',  # binary:logistic\n",
    "        'tree_method':'gpu_hist',\n",
    "        'predictor':'gpu_predictor',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO_REMOVE = [\n",
    "#     'candidate_*_before', 'matrix_gpu-700_lastday_max', 'matrix_12__20_lastday_max', 'matrix_gpu-226_lastday_max', 'matrix_cpu-90_lastday_max', 'matrix_gpu-700_sum', 'matrix_gpu-700_pos-log_sum', 'matrix_12__20_lastday_sum',\n",
    "#     'matrix_gpu-700_pos-log_max', 'matrix_gpu-226_lastday_sum', 'matrix_gpu-700_max', 'matrix_gpu-700_time_sum', 'matrix_123_type136_20_lastday_max', 'matrix_cpu-90_lastday_sum', 'matrix_cpu-90_type-163_max', 'matrix_gpu-700_time_max',\n",
    "#     'matrix_12__20_time_sum', 'matrix_gpu-700_type-163_sum', 'matrix_gpu-700_lastday_sum', 'matrix_gpu-700_type-163_max', 'matrix_cpu-90_time_sum', 'matrix_123_type136_20_time_sum', 'matrix_gpu-217_lastday_max', 'matrix_12__20_pos-log_sum',\n",
    "#     'matrix_12__20_type-163_max', 'matrix_12__20_time_max', 'matrix_cpu-90_max', 'matrix_cpu-90_type-163_sum', 'matrix_cpu-99_lastday_max', 'matrix_cpu-90_sum', 'matrix_gpu-226_sum', 'matrix_gpu-226_time_sum', 'matrix_12__20_time_mean',\n",
    "#     'matrix_12__20_type-163_mean', 'matrix_gpu-700_pos-log_mean', 'matrix_123_type0.590.5_20_lastday_max', 'matrix_gpu-700_time_mean', 'matrix_12__20_type-163_sum', 'matrix_12__20_pos-log_max', 'matrix_123_type136_20_lastday_sum',\n",
    "#     'matrix_cpu-90_time_mean', 'matrix_gpu-226_max', 'matrix_123_type136_20_type-163_max', 'matrix_gpu-226_type-163_max', 'matrix_gpu-226_lastday_mean', 'matrix_gpu-226_type-163_sum', 'matrix_cpu-99_time_sum', 'matrix_12__20_lastday_mean',\n",
    "#     'matrix_gpu-700_type-163_mean','matrix_123_type136_20_type-163_sum'\n",
    "# ][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 100\n",
    "    version = VERSION\n",
    "    \n",
    "    folds_file = \"../input/folds_4.csv\"\n",
    "    k = 4\n",
    "    mode = \"\"\n",
    "\n",
    "    features = FEATURES\n",
    "#     features = [ft for ft in features if ft not in TO_REMOVE]\n",
    "\n",
    "    cat_features = []\n",
    "\n",
    "    target = TARGET\n",
    "    pos_ratio = POS_RATIO\n",
    "\n",
    "    use_gt_sessions = True  # filter out sessions with no gt\n",
    "    use_gt_pos = False  # add candidates from gt\n",
    "    gt_regex = GT_REGEX\n",
    "    \n",
    "    model = \"xgb\"\n",
    "\n",
    "    params = PARAMS[model]\n",
    "\n",
    "    use_es = True\n",
    "    num_boost_round = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "DEBUG_MORE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------   Fold 1 / 4  -------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [07:03<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------   Training XGB Model   -------------\n",
      "\n",
      "    -> 992.87K training candidates\n",
      "    -> 127.05M validation candidates\n",
      "\n",
      "[0]\tval-auc:0.94848\n",
      "[100]\tval-auc:0.95733\n",
      "[200]\tval-auc:0.95868\n",
      "[300]\tval-auc:0.95962\n",
      "[400]\tval-auc:0.96022\n",
      "[500]\tval-auc:0.96063\n",
      "[600]\tval-auc:0.96098\n",
      "[700]\tval-auc:0.96124\n",
      "[800]\tval-auc:0.96144\n",
      "[900]\tval-auc:0.96154\n",
      "[1000]\tval-auc:0.96165\n",
      "[1100]\tval-auc:0.96172\n",
      "[1200]\tval-auc:0.96174\n",
      "[1300]\tval-auc:0.96174\n",
      "[1400]\tval-auc:0.96178\n",
      "[1500]\tval-auc:0.96182\n",
      "[1600]\tval-auc:0.96184\n",
      "[1700]\tval-auc:0.96190\n",
      "[1800]\tval-auc:0.96193\n",
      "[1900]\tval-auc:0.96190\n",
      "[1992]\tval-auc:0.96185\n",
      "\n",
      "[Infering]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [07:14<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> AUC : 0.9668\n",
      "\n",
      "Train / val sess inter 0\n",
      "CPU times: user 10min 21s, sys: 8min 25s, total: 18min 47s\n",
      "Wall time: 21min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "log_folder = None\n",
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f'Logging results to {log_folder}')\n",
    "    save_config(Config, log_folder + 'config')\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "df_val, ft_imp, df_test = kfold(REGEX, TEST_REGEX, Config, log_folder=log_folder, debug=DEBUG_MORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_importances(ft_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df_val[['session', 'candidates', 'pred']].copy()\n",
    "\n",
    "preds = preds.sort_values(['session', 'pred'], ascending=[True, False])\n",
    "preds = preds[['session', 'candidates', 'pred']].groupby('session').agg(list).reset_index()\n",
    "\n",
    "preds = preds.to_pandas()\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill less than 20 candidates.\n",
    "\n",
    "dfs = load_sessions(f\"../output/val_parquet/*\")\n",
    "\n",
    "if Config.target == \"gt_carts\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 1, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "elif Config.target == \"gt_orders\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 2, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "else:\n",
    "    top = dfs.loc[dfs[\"type\"] == 0, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: list(x) + top[: 20 - len(x)])\n",
    "\n",
    "del dfs\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450314, 450314)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = pd.read_csv(f\"../input/folds_4.csv\")\n",
    "len(folds[folds['fold'] == 0]), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- orders\t-  Found 51.91K GTs\t-  Recall : 0.6655\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_parquet(\"../output/val_labels.parquet\")\n",
    "\n",
    "recalls = []\n",
    "print()\n",
    "for col in CLASSES:\n",
    "    if \"gt_\" + col not in [Config.target]:\n",
    "        continue\n",
    "\n",
    "    if f\"gt_{col}\" not in preds.columns:\n",
    "        preds = preds.merge(gt[gt[\"type\"] == col].drop(\"type\", axis=1), how=\"left\").rename(\n",
    "            columns={\"ground_truth\": f\"gt_{col}\"}\n",
    "        )\n",
    "\n",
    "    n_preds, n_gts, n_found = get_coverage(\n",
    "        preds[\"candidates\"].values, preds[f\"gt_{col}\"].values\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"- {col}\\t-  Found {numerize(n_found)} GTs\\t-  Recall : {n_found / n_gts :.4f}\"\n",
    "    )\n",
    "    recalls.append(n_found / n_gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- orders\t-  Found 51.98K GTs\t-  Recall : 0.6664  MORE CANDIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- orders\t-  Found 207.74K GTs\t-  Recall : 0.6632\n",
    "- carts\t-  Found 242.41K GTs\t-  Recall : 0.4208\n",
    "- clicks\t-  Found 927.04K GTs\t-  Recall : 0.5281\n",
    "\n",
    "CHRIS :\n",
    "- orders - CV 0.666 - LB 0.678\n",
    "- carts - CV 0.437 - LB 0.450\n",
    "- clicks - CV 0.554 - LB 0.560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = np.average([0.5270, 0.4203, 0.6577], weights=WEIGHTS)\n",
    "# # cv = np.average([0.5059, 0.4139, 0.6540], weights=WEIGHTS)\n",
    "# print(f\"-> CV : {cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df_test[['session', 'candidates', 'pred']].copy()\n",
    "\n",
    "preds = preds.sort_values(['session', 'pred'], ascending=[True, False])\n",
    "preds = preds[['session', 'candidates', 'pred']].groupby('session').agg(list).reset_index()\n",
    "\n",
    "preds = preds.to_pandas()\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill less than 20 candidates. This should be useless in the future\n",
    "\n",
    "dfs = load_sessions(f\"../output/test_parquet/*\")\n",
    "\n",
    "if Config.target == \"gt_carts\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 1, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "elif Config.target == \"gt_orders\":\n",
    "    top = dfs.loc[dfs[\"type\"] == 2, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "else:\n",
    "    top = dfs.loc[dfs[\"type\"] == 0, \"aid\"].value_counts().index.values[:20].tolist()\n",
    "\n",
    "preds['candidates'] = preds['candidates'].apply(lambda x: list(x) + top[: 20 - len(x)])\n",
    "\n",
    "del dfs\n",
    "numba.cuda.current_context().deallocations.clear()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder_2 = LOG_PATH + f\"{VERSION}.0/\"\n",
    "os.makedirs(log_folder_2, exist_ok=True)\n",
    "save_config(Config, log_folder_2 + 'config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    sub = preds[['session', 'candidates']].copy()\n",
    "    assert len(sub) == 1671803\n",
    "\n",
    "    sub['candidates'] = sub['candidates'].parallel_apply(lambda x: \" \".join(map(str, x)))\n",
    "    sub['session'] =  sub['session'].astype(str) + \"_\" + TARGET[3:]\n",
    "    sub.columns = [\"session_type\", \"labels\"]\n",
    "\n",
    "    sub.to_csv(log_folder + f'sub_{TARGET}.csv', index=False)\n",
    "    print(f\"\\n-> Saved sub to {log_folder + f'sub_{TARGET}.csv'}\")\n",
    "\n",
    "    sub.to_csv(log_folder_2 + f'sub_{TARGET}.csv', index=False)\n",
    "    print(f\"-> Saved sub to {log_folder_2 + f'sub_{TARGET}.csv'}\\n\")\n",
    "\n",
    "    display(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all([os.path.exists(log_folder_2 + f'sub_gt_{c}.csv') for c in CLASSES]):\n",
    "#     sub_final = cudf.concat([\n",
    "#         cudf.read_csv(log_folder_2 + f'sub_gt_{c}.csv') for c in CLASSES\n",
    "#     ], ignore_index=True)\n",
    "\n",
    "#     assert len(sub_final) == 5015409\n",
    "#     sub_final.to_csv(log_folder_2 + f\"submission.csv\", index=False)\n",
    "\n",
    "#     print(f\"\\n-> Saved final sub to {log_folder_2 + f'submission.csv'}\\n\")\n",
    "\n",
    "#     display(sub_final.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
