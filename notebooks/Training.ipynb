{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/kaggle/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nvtabular as nvt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tokenizers import update_tokenizers\n",
    "# package_path = \"/opt/conda/lib/python3.8/site-packages/transformers\"\n",
    "# input_dir = \"../input/deberta_fast_tokenizer\"\n",
    "\n",
    "# update_tokenizers(package_path, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import OttoDataset\n",
    "from data.preparation import prepare_data\n",
    "# from training.main import k_fold\n",
    "from models import OttoTransformer\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.logger import prepare_log_folder, save_config, create_logger\n",
    "\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvtabular.ops import *\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin_standard_lib import Schema\n",
    "from transformers4rec import torch as tr\n",
    "from transformers4rec.torch.ranking_metric import RecallAt\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "from trainer import Trainer\n",
    "from transformers4rec.config.trainer import T4RecTrainingArguments\n",
    "from transformers4rec.torch.utils.data_utils import NVTabularDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"../input/parquets/train_0.parquet\")\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/parquets/train_0.parquet\"\n",
    "paths = glob.glob(\"../input/parquets/train_*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.loader.torch import Loader \n",
    "from merlin.io import Dataset\n",
    "\n",
    "# train_ds = Dataset(path)\n",
    "train_ds = Dataset(paths)\n",
    "\n",
    "train_dl_merlin = Loader(\n",
    "    train_ds,\n",
    "    batch_size=2**14,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# batch = next(iter(train_dl_merlin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# processed_batch = process_batch(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    processed_batch = {}\n",
    "\n",
    "    to_split = []\n",
    "    for k in batch.keys():\n",
    "        try:\n",
    "            x, s = batch[k]\n",
    "            to_split.append(x)\n",
    "        except:\n",
    "            processed_batch[k] = batch[k]\n",
    "            continue\n",
    "        \n",
    "    x = torch.stack(to_split, 0)\n",
    "    x = x.tensor_split(s[1:].cpu().long().view(-1), axis=1)\n",
    "\n",
    "    processed_batch[\"aid\"] = [x_[0] for x_ in x]\n",
    "    processed_batch[\"ts\"] = [x_[1] for x_ in x]\n",
    "    processed_batch[\"type\"] = [x_[2] for x_ in x]\n",
    "    \n",
    "    processed_batch[\"labels_clicks\"] = [x_[3] for x_ in x]\n",
    "    processed_batch[\"labels_carts\"] = [x_[4::2] for x_ in x]\n",
    "    processed_batch[\"labels_orders\"] = [x_[5::2] for x_ in x]\n",
    "\n",
    "    processed_batch['len'] = [x.size(0) for x in processed_batch['aid']]\n",
    "    \n",
    "    processed_batch['session'] = processed_batch['session'].cpu().numpy().tolist()\n",
    "        \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [10:08<00:00,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 5s, sys: 58.2 s, total: 14min 3s\n",
      "Wall time: 10min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_batches = {}\n",
    "\n",
    "for batch in tqdm(train_dl_merlin):\n",
    "    processed_batch = process_batch(batch[0])\n",
    "\n",
    "#     for k in processed_batch:\n",
    "#         try:\n",
    "#             processed_batches[k] += processed_batch[k]\n",
    "#         except KeyError:\n",
    "#             processed_batches[k] = processed_batch[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 678/678 [09:31<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 45s, sys: 23.8 s, total: 8min 9s\n",
      "Wall time: 9min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_batches = {}\n",
    "\n",
    "for batch in tqdm(train_dl_merlin):\n",
    "    processed_batch = process_batch(batch[0])\n",
    "\n",
    "#     for k in processed_batch:\n",
    "#         try:\n",
    "#             processed_batches[k] += processed_batch[k]\n",
    "#         except KeyError:\n",
    "#             processed_batches[k] = processed_batch[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508/508 [00:20<00:00, 24.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 s, sys: 1.66 s, total: 14.5 s\n",
      "Wall time: 22.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processed_batches = {}\n",
    "\n",
    "for batch in tqdm(train_dl_merlin):\n",
    "    processed_batch = process_batch(batch[0])\n",
    "\n",
    "    for k in processed_batch:\n",
    "        try:\n",
    "            processed_batches[k] += processed_batch[k]\n",
    "        except KeyError:\n",
    "            processed_batches[k] = processed_batch[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_sampler(lens, batch_size=32, drop_last=False):\n",
    "    batches = []\n",
    "    buckets = [[]] * 1000\n",
    "    yielded = 0\n",
    "\n",
    "    for idx, len_ in enumerate(lens):\n",
    "        count_zeros = int(5 * np.log(len_))\n",
    "        if len(buckets[count_zeros]) == 0:\n",
    "            buckets[count_zeros] = []\n",
    "\n",
    "        buckets[count_zeros].append(idx)\n",
    "\n",
    "        if len(buckets[count_zeros]) == batch_size:\n",
    "            batch = list(buckets[count_zeros])\n",
    "#             yield batch\n",
    "            batches.append(batch)\n",
    "            yielded += 1\n",
    "            buckets[count_zeros] = []\n",
    "\n",
    "    batch = []\n",
    "    leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "    for idx in leftover:\n",
    "        batch.append(idx)\n",
    "        if len(batch) == batch_size:\n",
    "            yielded += 1\n",
    "            batches.append(batch)\n",
    "#             yield batch\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0 and not drop_last:\n",
    "        yielded += 1\n",
    "        batches.append(batch)\n",
    "#         yield batch\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = len_sampler(processed_batches['len'], 32, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 36 is out of bounds for dimension 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     for k in processed_batches:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         print(k)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         x = torch.stack([processed_batches[k][idx][:min_len] for idx in batch])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack([processed_batches[k][idx][:min_len] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack([processed_batches[k][idx][min_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m processed_batches \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [51], line 13\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     for k in processed_batches:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         print(k)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         x = torch.stack([processed_batches[k][idx][:min_len] for idx in batch])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack([processed_batches[k][idx][:min_len] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack([processed_batches[k][idx][min_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m processed_batches \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [51], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     for k in processed_batches:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         print(k)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         x = torch.stack([processed_batches[k][idx][:min_len] for idx in batch])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack([processed_batches[k][idx][:min_len] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mprocessed_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmin_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m processed_batches \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 36 is out of bounds for dimension 0 with size 20"
     ]
    }
   ],
   "source": [
    "min_lens = []\n",
    "for batch in tqdm(batches):\n",
    "    lens = [processed_batches['len'][idx] for idx in batch]\n",
    "\n",
    "    min_len = np.min(lens)\n",
    "    min_lens.append(min_len)\n",
    "\n",
    "#     for k in processed_batches:\n",
    "#         print(k)\n",
    "#         x = torch.stack([processed_batches[k][idx][:min_len] for idx in batch])\n",
    "\n",
    "    x = {k: torch.stack([processed_batches[k][idx][:min_len] for idx in batch]) for k in ['aid', 'ts', 'type']}\n",
    "#     y = {k: torch.stack([processed_batches[k][idx][min_len - 1] for idx in batch]) for k in processed_batches if k not in ['session', 'aid', 'len', 'ts', 'type']}\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# for k in processed_batches.keys():\n",
    "#     sz = convert_size(sys.getsizeof(processed_batches[k]))\n",
    "#     print(k, sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 3311/8125 [03:41<05:21, 14.98it/s] \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for batch in tqdm(train_dl_merlin):\n",
    "    processed_batch = process_batch(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test = prepare_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df[df['fold'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OttoDataset(df_test.head(10000), max_len=410, max_trunc=100, train=False, test=True, pad=False)\n",
    "dataset = OttoDataset(df_val.head(10000), max_len=410, max_trunc=100, train=False, test=False, pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for idx in tqdm(range(10000)):\n",
    "    data = dataset[idx]\n",
    "    lens.append(data['ids'].size(0))\n",
    "#     break\n",
    "\n",
    "if len(lens) > 100:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.countplot(x=np.clip(lens, 0, 70))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = OttoTrainDataset(df_test, max_len=410, train=False)\n",
    "# lens = []\n",
    "\n",
    "# for idx in tqdm(range(10000)):\n",
    "#     data = dataset[idx]\n",
    "#     lens.append(data.shape[0])\n",
    "    \n",
    "# plt.figure(figsize=(15, 5))\n",
    "# sns.countplot(x=np.clip(lens, 0, 70))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [dataset.targets[k] for k in sorted(dataset.targets.keys())]\n",
    "recall(copy.deepcopy(y[:100]), copy.deepcopy(y[:100]), k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NERTransformer(\"microsoft/deberta-v3-base\", num_classes=3)\n",
    "model = OttoTransformer(\"roberta-base\", num_classes=3, n_ids=N_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['ids'].unsqueeze(0)\n",
    "types = data['token_type_ids'].unsqueeze(0).cuda()\n",
    "\n",
    "x = torch.cat([x] * 16, 0)\n",
    "types = torch.cat([types] * 16, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "x = x.cuda()\n",
    "types = types.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = {\n",
    "    \"microsoft/deberta-v3-base\": 32,\n",
    "    \"microsoft/deberta-v3-large\": 32,\n",
    "}\n",
    "\n",
    "LRS = {\n",
    "    \"microsoft/deberta-v3-base\": 3e-5,\n",
    "    \"microsoft/deberta-v3-large\": 3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # General\n",
    "    seed = 2222\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # Splits\n",
    "    k = 4\n",
    "    random_state = 2222\n",
    "    selected_folds = [0, 1, 2, 3]\n",
    "    folds_file = \"/workspace/folds_kgd_4.csv\"\n",
    "\n",
    "    # Architecture\n",
    "    name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "    pretrained_weights = None \n",
    "\n",
    "    no_dropout = False\n",
    "    use_conv = False\n",
    "    use_lstm = False\n",
    "    nb_layers = 1\n",
    "    nb_ft = 128\n",
    "    conv_kernel = 5\n",
    "    drop_p = 0 if no_dropout else 0.1\n",
    "    multi_sample_dropout = False\n",
    "\n",
    "    num_classes = 3\n",
    "    n_ids = N_IDS\n",
    "\n",
    "    # Texts\n",
    "    max_len_train = 410\n",
    "    max_len = 410\n",
    "\n",
    "#     extra_data_path = OUT_PATH + \"pl_case5/\"\n",
    "    extra_data_path = None  # OUT_PATH + \"pl_6/df_pl.csv\"\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"name\": \"bce\",  # ce, bce\n",
    "        \"smoothing\": 0,  # 0.01\n",
    "        \"activation\": \"sigmoid\",  # \"sigmoid\", \"softmax\"\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"batch_size\": BATCH_SIZES[name],\n",
    "        \"val_bs\": BATCH_SIZES[name] * 2,\n",
    "        \"use_len_sampler\": True,\n",
    "        \"pad_token\": 1 if \"roberta\" in name else 0,\n",
    "    }\n",
    "\n",
    "    optimizer_config = {\n",
    "        \"name\": \"AdamW\",\n",
    "        \"lr\": 5e-5,\n",
    "        \"lr_transfo\": LRS[name],\n",
    "        \"lr_decay\": 0.99,\n",
    "        \"warmup_prop\": 0.1,\n",
    "        \"weight_decay\": 1,\n",
    "        \"betas\": (0.5, 0.99),\n",
    "        \"max_grad_norm\": 1.,\n",
    "        # AWP\n",
    "        \"use_awp\": False,\n",
    "        \"awp_start_step\": 1000,\n",
    "        \"awp_lr\": 1,\n",
    "        \"awp_eps\": 5e-5 if \"xlarge\" in name else 1e-3,\n",
    "        \"awp_period\": 3,\n",
    "        # SWA\n",
    "        \"use_swa\": False,\n",
    "        \"swa_start\": 9400,\n",
    "        \"swa_freq\": 500,\n",
    "    }\n",
    "\n",
    "    gradient_checkpointing = False\n",
    "    acc_steps = 1\n",
    "    epochs = 1\n",
    "\n",
    "    use_fp16 = True\n",
    "\n",
    "    verbose = 1\n",
    "    verbose_eval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "log_folder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f\"Logging results to {log_folder}\")\n",
    "    save_config(Config, log_folder + \"config.json\")\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "pred_val, pred_test = k_fold(\n",
    "    Config,\n",
    "    df,\n",
    "    df_test=df_test,\n",
    "    log_folder=log_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
