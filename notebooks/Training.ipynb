{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/kaggle/kaggle_otto_rs/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nvtabular as nvt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.io import Dataset\n",
    "from merlin.loader.torch import Loader \n",
    "\n",
    "from nvtabular.ops import *\n",
    "from transformers4rec.torch.ranking_metric import RecallAt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.dataset import OttoDataset\n",
    "# from data.preparation import prepare_data\n",
    "# from training.main import k_fold\n",
    "# from models import OttoTransformer\n",
    "\n",
    "from utils.metrics import *\n",
    "from utils.logger import prepare_log_folder, save_config, create_logger\n",
    "\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"../input/parquets/train_0.parquet\")\n",
    "# df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Done :\n",
    "- Use Merlin Dataloader with big batches\n",
    "- Use len sampler and create small batches efficiently\n",
    "\n",
    "TODO :\n",
    "- Crop randomly\n",
    "- Ignore length 1 cases\n",
    "- leftover batches may result in too small cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../input/parquets/train_0.parquet\"\n",
    "paths = glob.glob(\"../input/parquets/train_*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = Dataset(path)\n",
    "train_ds = Dataset(paths)\n",
    "\n",
    "train_dl_merlin = Loader(\n",
    "    train_ds,\n",
    "    batch_size=2**14,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# batch = next(iter(train_dl_merlin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# processed_batch = process_batch(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    processed_batch = {}\n",
    "\n",
    "    to_split = []\n",
    "    for k in batch.keys():\n",
    "        try:\n",
    "            x, s = batch[k]\n",
    "            to_split.append(x)\n",
    "        except:\n",
    "            processed_batch[k] = batch[k]\n",
    "            continue\n",
    "        \n",
    "    x = torch.stack(to_split, 0)\n",
    "    x = x.tensor_split(s[1:].cpu().long().view(-1), axis=1)\n",
    "\n",
    "    processed_batch[\"aid\"] = [x_[0] for x_ in x]\n",
    "    processed_batch[\"ts\"] = [x_[1] for x_ in x]\n",
    "    processed_batch[\"type\"] = [x_[2] for x_ in x]\n",
    "    \n",
    "    processed_batch[\"labels_clicks\"] = [x_[3] for x_ in x]\n",
    "    processed_batch[\"labels_carts\"] = [x_[4::2] for x_ in x]\n",
    "    processed_batch[\"labels_orders\"] = [x_[5::2] for x_ in x]\n",
    "\n",
    "    processed_batch['len'] = [x.size(0) for x in processed_batch['aid']]\n",
    "    \n",
    "    processed_batch['session'] = processed_batch['session'].cpu().numpy().tolist()\n",
    "        \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_sampler(lens, batch_size=32, drop_last=False, exclude_too_short=True):\n",
    "    batches = []\n",
    "    buckets = [[]] * 1000\n",
    "    yielded = 0\n",
    "\n",
    "    for idx, len_ in enumerate(lens):\n",
    "        if len_ == 1 and exclude_too_short:\n",
    "            continue\n",
    "\n",
    "        count_zeros = int(5 * np.log(len_))\n",
    "        if len(buckets[count_zeros]) == 0:\n",
    "            buckets[count_zeros] = []\n",
    "\n",
    "        buckets[count_zeros].append(idx)\n",
    "\n",
    "        if len(buckets[count_zeros]) == batch_size:\n",
    "            batch = list(buckets[count_zeros])\n",
    "            batches.append(batch)\n",
    "            buckets[count_zeros] = []\n",
    "\n",
    "    batch = []\n",
    "    leftover = [idx for bucket in buckets for idx in bucket][::-1]\n",
    "\n",
    "    for idx in leftover:\n",
    "        batch.append(idx)\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0 and not drop_last:\n",
    "        yielded += 1\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [14:19<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 32s, sys: 1min 25s, total: 20min 58s\n",
      "Wall time: 14min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "crop_idxs = []\n",
    "\n",
    "for batch in tqdm(train_dl_merlin):\n",
    "    processed_batch = process_batch(batch[0])\n",
    "    \n",
    "    batches = len_sampler(\n",
    "        processed_batch['len'],\n",
    "        batch_size=32,\n",
    "        drop_last=False,\n",
    "        exclude_too_short=True\n",
    "    )\n",
    "\n",
    "    for batch in batches:\n",
    "        lens = [processed_batch['len'][idx] for idx in batch]\n",
    "        \n",
    "        min_len = np.min(lens)\n",
    "        crop_idx = np.random.randint(1, min_len)\n",
    "        min_lens.append(min_len)\n",
    "#         print(min_len, crop_idx)\n",
    "        \n",
    "        x = {k: torch.stack([processed_batch[k][idx][:crop_idx] for idx in batch]) for k in ['aid', 'ts', 'type']}\n",
    "        y = {k: torch.stack([processed_batch[k][idx][..., crop_idx - 1] for idx in batch]) for k in ['labels_clicks', 'labels_carts', 'labels_orders']}\n",
    "\n",
    "        crop_idxs.append(crop_idx)\n",
    "\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NERTransformer(\"microsoft/deberta-v3-base\", num_classes=3)\n",
    "model = OttoTransformer(\"roberta-base\", num_classes=3, n_ids=N_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['ids'].unsqueeze(0)\n",
    "types = data['token_type_ids'].unsqueeze(0).cuda()\n",
    "\n",
    "x = torch.cat([x] * 16, 0)\n",
    "types = torch.cat([types] * 16, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "x = x.cuda()\n",
    "types = types.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x, types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZES = {\n",
    "    \"microsoft/deberta-v3-base\": 32,\n",
    "    \"microsoft/deberta-v3-large\": 32,\n",
    "}\n",
    "\n",
    "LRS = {\n",
    "    \"microsoft/deberta-v3-base\": 3e-5,\n",
    "    \"microsoft/deberta-v3-large\": 3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # General\n",
    "    seed = 2222\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    # Splits\n",
    "    k = 4\n",
    "    random_state = 2222\n",
    "    selected_folds = [0, 1, 2, 3]\n",
    "    folds_file = \"/workspace/folds_kgd_4.csv\"\n",
    "\n",
    "    # Architecture\n",
    "    name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "    pretrained_weights = None \n",
    "\n",
    "    no_dropout = False\n",
    "    use_conv = False\n",
    "    use_lstm = False\n",
    "    nb_layers = 1\n",
    "    nb_ft = 128\n",
    "    conv_kernel = 5\n",
    "    drop_p = 0 if no_dropout else 0.1\n",
    "    multi_sample_dropout = False\n",
    "\n",
    "    num_classes = 3\n",
    "    n_ids = N_IDS\n",
    "\n",
    "    # Texts\n",
    "    max_len_train = 410\n",
    "    max_len = 410\n",
    "\n",
    "#     extra_data_path = OUT_PATH + \"pl_case5/\"\n",
    "    extra_data_path = None  # OUT_PATH + \"pl_6/df_pl.csv\"\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"name\": \"bce\",  # ce, bce\n",
    "        \"smoothing\": 0,  # 0.01\n",
    "        \"activation\": \"sigmoid\",  # \"sigmoid\", \"softmax\"\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"batch_size\": BATCH_SIZES[name],\n",
    "        \"val_bs\": BATCH_SIZES[name] * 2,\n",
    "        \"use_len_sampler\": True,\n",
    "        \"pad_token\": 1 if \"roberta\" in name else 0,\n",
    "    }\n",
    "\n",
    "    optimizer_config = {\n",
    "        \"name\": \"AdamW\",\n",
    "        \"lr\": 5e-5,\n",
    "        \"lr_transfo\": LRS[name],\n",
    "        \"lr_decay\": 0.99,\n",
    "        \"warmup_prop\": 0.1,\n",
    "        \"weight_decay\": 1,\n",
    "        \"betas\": (0.5, 0.99),\n",
    "        \"max_grad_norm\": 1.,\n",
    "        # AWP\n",
    "        \"use_awp\": False,\n",
    "        \"awp_start_step\": 1000,\n",
    "        \"awp_lr\": 1,\n",
    "        \"awp_eps\": 5e-5 if \"xlarge\" in name else 1e-3,\n",
    "        \"awp_period\": 3,\n",
    "        # SWA\n",
    "        \"use_swa\": False,\n",
    "        \"swa_start\": 9400,\n",
    "        \"swa_freq\": 500,\n",
    "    }\n",
    "\n",
    "    gradient_checkpointing = False\n",
    "    acc_steps = 1\n",
    "    epochs = 1\n",
    "\n",
    "    use_fp16 = True\n",
    "\n",
    "    verbose = 1\n",
    "    verbose_eval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "log_folder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f\"Logging results to {log_folder}\")\n",
    "    save_config(Config, log_folder + \"config.json\")\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "pred_val, pred_test = k_fold(\n",
    "    Config,\n",
    "    df,\n",
    "    df_test=df_test,\n",
    "    log_folder=log_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
